{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar las bibliotecas necesarias\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.exceptions import NotFittedError\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Configuracion de seaborn\n",
    "sns.set_theme(style='whitegrid', context='paper', palette='muted')\n",
    "\n",
    "# Agregar el directorio de scripts al path\n",
    "sys.path.append(os.path.join(os.path.dirname(os.getcwd()), 'scripts'))\n",
    "\n",
    "# Importar el logger personalizado\n",
    "from logger import CustomLogger\n",
    "\n",
    "# Crear logger\n",
    "logger = CustomLogger(developer='David')\n",
    "app_logger = logger.get_logger('app')\n",
    "errors_logger = logger.get_logger('errors')\n",
    "visualizations_logger = logger.get_logger('visualizations')\n",
    "optimization_logger = logger.get_logger('optimization')\n",
    "results_logger = logger.get_logger('results')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
       "1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
       "2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
       "3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
       "4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
       "\n",
       "  LandContour Utilities  ... PoolArea PoolQC Fence MiscFeature MiscVal MoSold  \\\n",
       "0         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
       "1         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      5   \n",
       "2         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      9   \n",
       "3         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
       "4         Lvl    AllPub  ...        0    NaN   NaN         NaN       0     12   \n",
       "\n",
       "  YrSold  SaleType  SaleCondition  SalePrice  \n",
       "0   2008        WD         Normal     208500  \n",
       "1   2007        WD         Normal     181500  \n",
       "2   2008        WD         Normal     223500  \n",
       "3   2006        WD        Abnorml     140000  \n",
       "4   2008        WD         Normal     250000  \n",
       "\n",
       "[5 rows x 81 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    # Cargar los datos de entrenamiento\n",
    "    train_data = pd.read_csv('../data/train.csv')\n",
    "    app_logger.info(\"Conjunto de datos de entrenamiento cargado exitosamente.\")\n",
    "except FileNotFoundError:\n",
    "    errors_logger.error(\"No se pudo encontrar el archivo de datos de entrenamiento.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    errors_logger.error(f\"Error al cargar los datos de entrenamiento: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "# Cargamos test.csv\n",
    "try:\n",
    "    test_data = pd.read_csv('../data/test.csv')\n",
    "    app_logger.info(\"Conjunto de datos de test cargado exitosamente.\")\n",
    "except FileNotFoundError:\n",
    "    errors_logger.error(\"No se pudo encontrar el archivo de datos de test.\")\n",
    "    raise\n",
    "\n",
    "# Mostrar las primeras filas del conjunto de datos\n",
    "train_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores faltantes por columna:\n",
      "              Total  Porcentaje\n",
      "PoolQC         1453   99.520548\n",
      "MiscFeature    1406   96.301370\n",
      "Alley          1369   93.767123\n",
      "Fence          1179   80.753425\n",
      "MasVnrType      872   59.726027\n",
      "FireplaceQu     690   47.260274\n",
      "LotFrontage     259   17.739726\n",
      "GarageType       81    5.547945\n",
      "GarageYrBlt      81    5.547945\n",
      "GarageFinish     81    5.547945\n",
      "GarageQual       81    5.547945\n",
      "GarageCond       81    5.547945\n",
      "BsmtFinType2     38    2.602740\n",
      "BsmtExposure     38    2.602740\n",
      "BsmtFinType1     37    2.534247\n",
      "BsmtCond         37    2.534247\n",
      "BsmtQual         37    2.534247\n",
      "MasVnrArea        8    0.547945\n",
      "Electrical        1    0.068493\n",
      "\n",
      "Número de filas duplicadas: 0\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Valores faltantes\n",
    "    print(\"Valores faltantes por columna:\")\n",
    "    valores_faltantes = train_data.isnull().sum()\n",
    "    porcentaje_faltantes = 100 * train_data.isnull().sum() / len(train_data)\n",
    "    tabla_faltantes = pd.concat([valores_faltantes, porcentaje_faltantes], axis=1, keys=['Total', 'Porcentaje'])\n",
    "    print(tabla_faltantes[tabla_faltantes['Total'] > 0].sort_values('Total', ascending=False))\n",
    "\n",
    "    # Filas duplicadas\n",
    "    filas_duplicadas = train_data.duplicated().sum()\n",
    "    print(f\"\\nNúmero de filas duplicadas: {filas_duplicadas}\")\n",
    "\n",
    "    # Registrar en el log\n",
    "    if valores_faltantes.sum() > 0:\n",
    "        app_logger.info(f\"Se encontraron {valores_faltantes.sum()} valores faltantes en total.\")\n",
    "    else:\n",
    "        app_logger.info(\"No se encontraron valores faltantes en el conjunto de datos.\")\n",
    "\n",
    "    if filas_duplicadas > 0:\n",
    "        app_logger.warning(f\"Se encontraron {filas_duplicadas} filas duplicadas en el conjunto de datos.\")\n",
    "    else:\n",
    "        app_logger.info(\"No se encontraron filas duplicadas en el conjunto de datos.\")\n",
    "\n",
    "except Exception as e:\n",
    "    errors_logger.exception(\"Error al analizar valores faltantes y filas duplicadas:\")\n",
    "    app_logger.error(\"Se produjo un error al analizar los datos. Consulte el registro de errores para más detalles.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Columns on Train\n",
    "- Pool Quality Col has 99,52% missing values, but we still have Pool Area wich i think is more important than PoolQc\n",
    "- MiscFeature Col has 96.30% missing values. We also have MiscVal that represents the value of Misc Feature.\n",
    "    - Each Misc Feature has a unique value or has multiple values?? If each MiscFeature has only one unique value (univoque relationship) we can drop MiscFeature and leave Misc Val because we wont lose info, otherwise, drop both. Even if MiscVal has no NaNs, using it on its own may not be the best for the model to gather relationships.\n",
    "- Alley has 93% missing values, no other col is related to this one so we can directly drop it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verificar si la relación es unívoca entre MiscFeature y MiscVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relación entre MiscFeature y MiscVal:\n",
      "  MiscFeature  Valores_Unicos_MiscVal\n",
      "0        Gar2                       2\n",
      "1        Othr                       2\n",
      "2        Shed                      18\n",
      "3        TenC                       1\n",
      "\n",
      "La relación entre MiscFeature y MiscVal no es unívoca.\n",
      "\n",
      "Ejemplos de MiscFeature con múltiples valores de MiscVal:\n",
      "MiscFeature: Gar2\n",
      "Valores de MiscVal: [15500  8300]\n",
      "\n",
      "MiscFeature: Othr\n",
      "Valores de MiscVal: [3500    0]\n",
      "\n",
      "MiscFeature: Shed\n",
      "Valores de MiscVal: [ 700  350  500  400  480  450 1200  800 2000  600 1300   54  620  560\n",
      " 1400    0 1150 2500]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Crear un DataFrame con MiscFeature y MiscVal\n",
    "    misc_df = train_data[['MiscFeature', 'MiscVal']]\n",
    "    app_logger.info(\"DataFrame misc_df creado exitosamente\")\n",
    "except KeyError as e:\n",
    "    errors_logger.exception(\"Error al crear misc_df: Columna no encontrada\")\n",
    "    print(f\"Error: No se encontró la columna {str(e)} en el conjunto de datos\")\n",
    "    raise\n",
    "\n",
    "try:\n",
    "    # Agrupar por MiscFeature y contar los valores únicos de MiscVal\n",
    "    relacion_misc = misc_df.groupby('MiscFeature')['MiscVal'].nunique().reset_index()\n",
    "    relacion_misc.columns = ['MiscFeature', 'Valores_Unicos_MiscVal']\n",
    "    app_logger.info(\"Relación entre MiscFeature y MiscVal generada correctamente\")\n",
    "except Exception as e:\n",
    "    errors_logger.exception(\"Error al generar la relación entre MiscFeature y MiscVal\")\n",
    "    print(\"Se produjo un error al analizar la relación. Consulte el registro de errores para más detalles.\")\n",
    "    raise\n",
    "\n",
    "print(\"Relación entre MiscFeature y MiscVal:\")\n",
    "print(relacion_misc)\n",
    "\n",
    "try:\n",
    "    # Verificar si cada MiscFeature tiene un único valor de MiscVal\n",
    "    es_univoca = (relacion_misc['Valores_Unicos_MiscVal'] == 1).all()\n",
    "    \n",
    "    if es_univoca:\n",
    "        print(\"\\nLa relación entre MiscFeature y MiscVal es unívoca.\")\n",
    "        app_logger.info(\"La relación entre MiscFeature y MiscVal es unívoca. Se puede considerar dejar una columna.\")\n",
    "    else:\n",
    "        print(\"\\nLa relación entre MiscFeature y MiscVal no es unívoca.\")\n",
    "        app_logger.info(\"La relación entre MiscFeature y MiscVal no es unívoca. Se recomienda eliminar ambas columnas.\")\n",
    "except Exception as e:\n",
    "    errors_logger.exception(\"Error al verificar la univocidad de la relación\")\n",
    "    print(\"Se produjo un error al verificar la relación. Consulte el registro de errores para más detalles.\")\n",
    "\n",
    "if not es_univoca:\n",
    "    try:\n",
    "        print(\"\\nEjemplos de MiscFeature con múltiples valores de MiscVal:\")\n",
    "        ejemplos_multiples = relacion_misc[relacion_misc['Valores_Unicos_MiscVal'] > 1]\n",
    "        for _, row in ejemplos_multiples.iterrows():\n",
    "            feature = row['MiscFeature']\n",
    "            valores = misc_df[misc_df['MiscFeature'] == feature]['MiscVal'].unique()\n",
    "            print(f\"MiscFeature: {feature}\")\n",
    "            print(f\"Valores de MiscVal: {valores}\\n\")\n",
    "            app_logger.info(f\"MiscFeature '{feature}' tiene múltiples valores: {valores}\")\n",
    "    except Exception as e:\n",
    "        errors_logger.exception(\"Error al mostrar ejemplos de MiscFeature con múltiples valores\")\n",
    "        print(\"Se produjo un error al mostrar los ejemplos. Consulte el registro de errores para más detalles.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dado que MiscVal y MiscFeature no son univocamente relacionados, se eliminan ambas columnas puesto que estan relacionadas y no se tienen datos de una y la otra son casi todo 0\n",
    "train_data = train_data.drop(columns=['MiscFeature', 'MiscVal'])\n",
    "app_logger.info(\"Se eliminaron las columnas MiscFeature y MiscVal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas eliminadas: Alley, PoolQC\n",
      "\n",
      "Forma del conjunto de datos original: (1460, 79)\n",
      "Forma del conjunto de datos filtrado: (1460, 77)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Filtrar columnas con más del 90% de valores faltantes\n",
    "    umbral_faltantes = 0.9\n",
    "    columnas_a_eliminar = tabla_faltantes[tabla_faltantes['Porcentaje'] > 90].index\n",
    "    train_data_filtrado = train_data.drop(columns=columnas_a_eliminar.drop(['MiscFeature']))\n",
    "\n",
    "    # Registrar en el log\n",
    "    if len(columnas_a_eliminar) > 0:\n",
    "        app_logger.info(f\"Se eliminaron {len(columnas_a_eliminar)} columnas con más del 90% de valores faltantes: {', '.join(columnas_a_eliminar)}\")\n",
    "        print(f\"Columnas eliminadas: {', '.join(columnas_a_eliminar.drop(['MiscFeature']))}\")\n",
    "    else:\n",
    "        app_logger.info(\"No se encontraron columnas con más del 90% de valores faltantes.\")\n",
    "        print(\"No se encontraron columnas con más del 90% de valores faltantes.\")\n",
    "\n",
    "    # Mostrar la forma del nuevo conjunto de datos\n",
    "    print(f\"\\nForma del conjunto de datos original: {train_data.shape}\")\n",
    "    print(f\"Forma del conjunto de datos filtrado: {train_data_filtrado.shape}\")\n",
    "\n",
    "except KeyError as e:\n",
    "    errors_logger.exception(f\"Error al acceder a una columna: {str(e)}\")\n",
    "    app_logger.error(\"Se produjo un error al filtrar las columnas. Verifique los nombres de las columnas.\")\n",
    "except ValueError as e:\n",
    "    errors_logger.exception(f\"Error en el cálculo de porcentajes: {str(e)}\")\n",
    "    app_logger.error(\"Se produjo un error al calcular los porcentajes de valores faltantes.\")\n",
    "except Exception as e:\n",
    "    errors_logger.exception(f\"Error inesperado: {str(e)}\")\n",
    "    app_logger.error(\"Se produjo un error inesperado durante el procesamiento de los datos.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se ha generado un análisis interactivo con dtale.\n",
      "Por favor, acceda a la siguiente URL para explorar los datos: http://PortatilDavid:40000\n"
     ]
    }
   ],
   "source": [
    "# Importar dtale\n",
    "import dtale\n",
    "\n",
    "# Crear una instancia de dtale con los datos filtrados\n",
    "d = dtale.show(train_data_filtrado)\n",
    "\n",
    "# Mostrar el enlace para acceder a la interfaz de dtale\n",
    "print(\"Se ha generado un análisis interactivo con dtale.\")\n",
    "print(f\"Por favor, acceda a la siguiente URL para explorar los datos: {d._url}\")\n",
    "\n",
    "# Registrar en el log\n",
    "app_logger.info(\"Se ha generado un análisis interactivo utilizando dtale.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis D-Tale\n",
    "- We found important predictive power with the following features ``FullBath_quantile, FullBath, GrLiveArea, GrLiveArea_power, GrLiveArea_quantile,GrLiveArea_robust, GarageCars, OverallQual``\n",
    "- Maybe add a feature that stratifies if the house has garage or not (Never Mind ``GarageCond`` and ``GarageQual`` already do).\n",
    "- 81 houses have 0 car capacity in garage and 0 sqft but not NA specified on ``GarageCond``, ``GarageQual``, ``GarageType`` and ``GarageFinish``, drop rows with 0 or impute them. If a house has 0 for ``GarageCars`` and ``GarageArea``, the ``GarageType``, ``GarageCond``, ``GarageQual``, and ``GarageFinish`` should be NA?.\n",
    "- ``GarageQual`` and ``GarageCond`` are duplicates.\n",
    "- The features ``Fireplaces`` and ``FireplaceQu`` are both NaN or 0 for the same row, so imputation should be done simultaneosly maintaining their possible relationship, imputing the mean and most frecuent values independently could potentially introduce noise as the pairing could possibly be random.\n",
    "\n",
    "- Most of the numerical variables where Non-normal, with log relationship with sales price (target).\n",
    "\n",
    "    ### Low Variance Features\n",
    "    - Dtale showed the following features having low variance:\n",
    "        - ``BsmtFinSF2`` : 88.56% are 0s, related to this feature we have ``BsmtFinType2`` with 88.33% of rows with value 'Unf' which means 'unfinished '. This could mean that as it is unfinshed the sqft on ``BsmtFinSF2`` are not taken into account so the value will be 0?. Coincidence percentaje between ``BsmtFinType2 == 'Unf' and BsmtFinSF2 == 0``: 97.14%\n",
    "        - ``LowQualFinSF``: 98.22% are 0s. This feature represents 'Low quality finished square feet (all floors)', i dont really know if we could impute this values as the meaning of the variable is not deterministic or easy to define because it represent the sqft of LowQuality finished sqft of all floors, but there isnt a feature defining wich houses are LowQuality or finished. \n",
    "            - Maybe we could create a feature called ``Total_sqft`` and/or Mean between all floors per house, etc... \n",
    "        - ``KitchenAbvGr``: From possible values {0,1,2,3} 95.34% are 1s. These feature represents the number of kitchens above grade. There is also the ``TotRmsAbvGrd`` feature measuring the total rooms above grade and the feature ``KitchenQual`` wich indicates the quality of the kitchens. Due to this i think we could drop this feature and leave the other two, as the model will probably discover the relationships.\n",
    "        - ``PoolArea``: 99.52% values are 0s. The related feature was ``PoolQC`` representing the pool quality but it had 99.79% of NaNs values so i think we can drop both.\n",
    "        - ``EnclosedPorch``: 85.75% of 0s.\n",
    "        - ``3SsnPorch``: 98.36% of 0s.\n",
    "        - ``ScreenPorch``: 92.05% of 0s.\n",
    "        - Out of this 3 features realted with sqft of different types of Porch the one that has the most sens on maintaining on the dataset is ``EnclosedPorch`` that is de opposite to ``OpenPorch`` (that measures the open porch sqft) and its not that much full of 0s as the other two. This could be okay with te model as it can learn that if we have some value on ``EnclosedPorch``, the house has an enclosed porch, and the same for ``OpenPorch``. The other 2 features are practically all 0s and not much value can be obtained. I think the 2 important features are the ones i mentioned, we could do some new features to encode this info of having 1 porch or another or both.\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Low_Variance_Features = [\n",
    "    \"BsmtFinSF2\",\n",
    "    \"LowQualFinSF\",\n",
    "    \"KitchenAbvGr\",\n",
    "    \"PoolArea\",\n",
    "    \"EnclosedPorch\",\n",
    "    \"3SsnPorch\",\n",
    "    \"ScreenPorch\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de casas remodeladas: 696\n",
      "Número de casas no remodeladas: 764\n",
      "Porcentaje de casas remodeladas: 47.67%\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Comparar YearRemodAdd con YearBuilt para identificar casas remodeladas\n",
    "    casas_remodeladas = train_data_filtrado[train_data_filtrado['YearRemodAdd'] != train_data_filtrado['YearBuilt']]\n",
    "    casas_no_remodeladas = train_data_filtrado[train_data_filtrado['YearRemodAdd'] == train_data_filtrado['YearBuilt']]\n",
    "\n",
    "    # Calcular el número de casas remodeladas y no remodeladas\n",
    "    num_remodeladas = len(casas_remodeladas)\n",
    "    num_no_remodeladas = len(casas_no_remodeladas)\n",
    "\n",
    "    # Imprimir los resultados\n",
    "    print(f\"Número de casas remodeladas: {num_remodeladas}\")\n",
    "    print(f\"Número de casas no remodeladas: {num_no_remodeladas}\")\n",
    "    print(f\"Porcentaje de casas remodeladas: {(num_remodeladas / len(train_data_filtrado)) * 100:.2f}%\")\n",
    "\n",
    "    # Registrar en el log\n",
    "    app_logger.info(f\"Se identificaron {num_remodeladas} casas remodeladas y {num_no_remodeladas} casas no remodeladas.\")\n",
    "\n",
    "except KeyError as e:\n",
    "    errors_logger.exception(f\"Error al acceder a las columnas: {str(e)}\")\n",
    "    app_logger.error(\"No se pudo completar el análisis de casas remodeladas debido a un error en las columnas.\")\n",
    "except ZeroDivisionError:\n",
    "    errors_logger.exception(\"Error al calcular el porcentaje: División por cero.\")\n",
    "    app_logger.error(\"No se pudo calcular el porcentaje de casas remodeladas debido a un error en los datos.\")\n",
    "except Exception as e:\n",
    "    errors_logger.exception(f\"Error inesperado durante el análisis de casas remodeladas: {str(e)}\")\n",
    "    app_logger.error(\"Se produjo un error inesperado durante el análisis de casas remodeladas.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_porcentajes(df):\n",
    "    '''\n",
    "    Calcula el porcentaje de casas que coinciden en el año de construcción con el año de construcción del garaje y el año de remodelación.\n",
    "    '''\n",
    "    try:\n",
    "        coinciden = df['YearBuilt'] == df['GarageYrBlt']\n",
    "        diferentes = ~coinciden\n",
    "        porcentaje_coinciden = (coinciden.sum() / len(df)) * 100\n",
    "        porcentaje_diferentes = (diferentes.sum() / len(df)) * 100\n",
    "        return porcentaje_coinciden, porcentaje_diferentes\n",
    "    except KeyError as e:\n",
    "        app_logger.error(f\"Error al acceder a las columnas: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        app_logger.error(f\"Error inesperado al calcular porcentajes: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analizar_coincidencias_remod(df, diferentes):\n",
    "    '''\n",
    "    Analiza las casas que coinciden en el año de construcción con el año de construcción del garaje y el año de remodelación.\n",
    "    '''\n",
    "    try:\n",
    "        coinciden_con_remod = df[diferentes]['GarageYrBlt'] == df[diferentes]['YearRemodAdd']\n",
    "        porcentaje_coinciden_remod = (coinciden_con_remod.sum() / diferentes.sum()) * 100\n",
    "        return porcentaje_coinciden_remod\n",
    "    except KeyError as e:\n",
    "        app_logger.error(f\"Error al acceder a las columnas para análisis de remodelación: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        app_logger.error(f\"Error inesperado en análisis de remodelación: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_diferencias_tiempo(df):\n",
    "    '''\n",
    "    Calcula las diferencias de tiempo entre el año de construcción del garaje y el año de construcción y el año de remodelación.\n",
    "    '''\n",
    "    try:\n",
    "        df['diff_with_yearbuilt'] = abs(df['GarageYrBlt'] - df['YearBuilt'])\n",
    "        df['diff_with_yearremodadd'] = abs(df['GarageYrBlt'] - df['YearRemodAdd'])\n",
    "        return df\n",
    "    except KeyError as e:\n",
    "        app_logger.error(f\"Error al calcular diferencias de tiempo: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        app_logger.error(f\"Error inesperado al calcular diferencias de tiempo: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analizar_proximidad(df, umbral=7):\n",
    "    '''\n",
    "    Analiza las casas que están dentro del umbral de proximidad con el año de construcción del garaje y el año de construcción y el año de remodelación.\n",
    "    '''\n",
    "    try:\n",
    "        proxima_a_yearbuilt = df[df['diff_with_yearbuilt'] <= umbral]\n",
    "        proxima_a_yearremodadd = df[df['diff_with_yearremodadd'] <= umbral]\n",
    "        mas_de_umbral = df[(df['diff_with_yearbuilt'] > umbral) & (df['diff_with_yearremodadd'] > umbral)]\n",
    "        return proxima_a_yearbuilt, proxima_a_yearremodadd, mas_de_umbral\n",
    "    except KeyError as e:\n",
    "        app_logger.error(f\"Error al analizar proximidad: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        app_logger.error(f\"Error inesperado al analizar proximidad: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_tiempos_medios_extremos(df):\n",
    "    '''\n",
    "    Calcula los tiempos medios y extremos de la construcción y la remodelación.\n",
    "    '''\n",
    "    try:\n",
    "        tiempo_construccion = df['GarageYrBlt'] - df['YearBuilt']\n",
    "        tiempo_remodelacion = df['YearRemodAdd'] - df['GarageYrBlt']\n",
    "        tiempo_hasta_remodelacion = df['YearRemodAdd'] - df['YearBuilt']\n",
    "        \n",
    "        return {\n",
    "            'construccion': (tiempo_construccion.mean(), tiempo_construccion.min(), tiempo_construccion.max()),\n",
    "            'remodelacion': (tiempo_remodelacion.mean(), tiempo_remodelacion.min(), tiempo_remodelacion.max()),\n",
    "            'hasta_remodelacion': (tiempo_hasta_remodelacion.mean(), tiempo_hasta_remodelacion.min(), tiempo_hasta_remodelacion.max())\n",
    "        }\n",
    "    except KeyError as e:\n",
    "        app_logger.error(f\"Error al calcular tiempos medios y extremos: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        app_logger.error(f\"Error inesperado al calcular tiempos medios y extremos: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analizar_datos(train_data_filtrado):\n",
    "    '''\n",
    "    Analiza los datos de construcción y remodelación.\n",
    "    '''\n",
    "    try:\n",
    "        porcentaje_coinciden, porcentaje_diferentes = calcular_porcentajes(train_data_filtrado)\n",
    "        diferentes = train_data_filtrado['YearBuilt'] != train_data_filtrado['GarageYrBlt']\n",
    "        porcentaje_coinciden_remod = analizar_coincidencias_remod(train_data_filtrado, diferentes)\n",
    "        \n",
    "        no_coincide_ninguno = (~(train_data_filtrado['YearBuilt'] == train_data_filtrado['GarageYrBlt'])) & \\\n",
    "                              (~(train_data_filtrado['GarageYrBlt'] == train_data_filtrado['YearRemodAdd']))\n",
    "        porcentaje_no_coincide_ninguno = (no_coincide_ninguno.sum() / len(train_data_filtrado)) * 100\n",
    "        \n",
    "        df_no_coincide = calcular_diferencias_tiempo(train_data_filtrado[no_coincide_ninguno])\n",
    "        proxima_a_yearbuilt, proxima_a_yearremodadd, mas_de_7_anos = analizar_proximidad(df_no_coincide)\n",
    "        \n",
    "        porcentaje_proxima_a_yearbuilt = (len(proxima_a_yearbuilt) / len(df_no_coincide)) * 100\n",
    "        porcentaje_proxima_a_yearremodadd = (len(proxima_a_yearremodadd) / len(df_no_coincide)) * 100\n",
    "        porcentaje_mas_de_7_anos = (len(mas_de_7_anos) / len(df_no_coincide)) * 100\n",
    "        porcentaje_mas_7_anos_total = (len(mas_de_7_anos) / len(train_data_filtrado)) * 100\n",
    "        \n",
    "        tiempos = calcular_tiempos_medios_extremos(train_data_filtrado)\n",
    "        \n",
    "        # Imprimir resultados\n",
    "        print(f\"Porcentaje de casas donde YearBuilt coincide con GarageYrBlt: {porcentaje_coinciden:.2f}%\")\n",
    "        print(f\"Del {porcentaje_diferentes:.2f}% que no coincide con YearBuilt, {porcentaje_coinciden_remod:.2f}% coincide con YearRemodAdd\")\n",
    "        print(f\"Porcentaje de casas donde GarageYrBlt no coincide ni con YearBuilt ni con YearRemodAdd: {porcentaje_no_coincide_ninguno:.2f}%\")\n",
    "        print(f\"Porcentaje de casas donde GarageYrBlt está a más de 7 años de YearBuilt y YearRemodAdd respecto al total: {porcentaje_mas_7_anos_total:.2f}%\")\n",
    "        print(f\"Porcentaje de casas donde GarageYrBlt no coincide con YearBuilt ni con YearRemodAdd y está a menos de 7 años de YearBuilt: {porcentaje_proxima_a_yearbuilt:.2f}%\")\n",
    "        print(f\"Porcentaje de casas donde GarageYrBlt no coincide con YearBuilt ni con YearRemodAdd y está a menos de 7 años de YearRemodAdd: {porcentaje_proxima_a_yearremodadd:.2f}%\")\n",
    "        print(f\"Porcentaje de casas donde GarageYrBlt no coincide con YearBuilt ni con YearRemodAdd y está a más de 7 años de ambas fechas: {porcentaje_mas_de_7_anos:.2f}%\")\n",
    "        \n",
    "        for key, (media, minimo, maximo) in tiempos.items():\n",
    "            print(f\"Tiempo {key} - Media: {media:.2f}, Mínimo: {minimo:.2f}, Máximo: {maximo:.2f} años\")\n",
    "        \n",
    "        # Registrar en el log\n",
    "        app_logger.info(f\"Análisis de YearBuilt vs GarageYrBlt completado. Coinciden: {porcentaje_coinciden:.2f}%, Diferentes: {porcentaje_diferentes:.2f}%\")\n",
    "        app_logger.info(f\"El {porcentaje_no_coincide_ninguno:.2f}% de las casas tienen GarageYrBlt que no coincide ni con YearBuilt ni con YearRemodAdd.\")\n",
    "        app_logger.info(f\"El {porcentaje_mas_7_anos_total:.2f}% del total de casas tienen GarageYrBlt a más de 7 años de YearBuilt y YearRemodAdd.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        app_logger.error(f\"Error en el análisis de datos: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porcentaje de casas donde YearBuilt coincide con GarageYrBlt: 74.59%\n",
      "Del 25.41% que no coincide con YearBuilt, 14.56% coincide con YearRemodAdd\n",
      "Porcentaje de casas donde GarageYrBlt no coincide ni con YearBuilt ni con YearRemodAdd: 21.71%\n",
      "Porcentaje de casas donde GarageYrBlt está a más de 7 años de YearBuilt y YearRemodAdd respecto al total: 9.45%\n",
      "Porcentaje de casas donde GarageYrBlt no coincide con YearBuilt ni con YearRemodAdd y está a menos de 7 años de YearBuilt: 22.08%\n",
      "Porcentaje de casas donde GarageYrBlt no coincide con YearBuilt ni con YearRemodAdd y está a menos de 7 años de YearRemodAdd: 23.03%\n",
      "Porcentaje de casas donde GarageYrBlt no coincide con YearBuilt ni con YearRemodAdd y está a más de 7 años de ambas fechas: 43.53%\n",
      "Tiempo construccion - Media: 5.55, Mínimo: -10.00, Máximo: 123.00 años\n",
      "Tiempo remodelacion - Media: 6.93, Mínimo: -53.00, Máximo: 98.00 años\n",
      "Tiempo hasta_remodelacion - Media: 13.60, Mínimo: 0.00, Máximo: 123.00 años\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_11380\\2723570695.py:6: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_11380\\2723570695.py:7: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ejecutar el análisis\n",
    "try:\n",
    "    analizar_datos(train_data_filtrado)\n",
    "except Exception as e:\n",
    "    app_logger.error(f\"Error general en la ejecución del análisis: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_porcentaje(numerador, denominador):\n",
    "    try:\n",
    "        return (numerador / denominador) * 100\n",
    "    except ZeroDivisionError:\n",
    "        app_logger.warning(\"División por cero al calcular porcentaje\")\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analizar_bsmt_fin_type2(df):\n",
    "    try:\n",
    "        coincidencias = ((df['BsmtFinType2'] == 'Unf') & (df['BsmtFinSF2'] == 0)).sum()\n",
    "        total_casos_0 = (df['BsmtFinSF2'] == 0).sum()\n",
    "        porcentaje_coincidencia = calcular_porcentaje(coincidencias, total_casos_0)\n",
    "\n",
    "        app_logger.info(f\"El {porcentaje_coincidencia:.2f}% de los casos tienen BsmtFinType2 == 'Unf' y BsmtFinSF2 == 0 simultáneamente.\")\n",
    "        print(f\"Porcentaje de coincidencia entre BsmtFinType2 == 'Unf' y BsmtFinSF2 == 0 del total de casas donde BsmtFinSF2 == 0: {porcentaje_coincidencia:.2f}%\")\n",
    "\n",
    "        no_coincidencias = ((df['BsmtFinType2'] == 'Unf') & (df['BsmtFinSF2'] != 0)).sum()\n",
    "        total_casos_no_0 = (df['BsmtFinSF2'] != 0).sum()\n",
    "        porcentaje_no_coincidencia = calcular_porcentaje(no_coincidencias, total_casos_no_0)\n",
    "\n",
    "        app_logger.info(f\"El {porcentaje_no_coincidencia:.2f}% de los casos tienen BsmtFinType2 == 'Unf' pero BsmtFinSF2 != 0. Con un total de {no_coincidencias} casos.\")\n",
    "        print(f\"Porcentaje de casos donde BsmtFinType2 == 'Unf' pero BsmtFinSF2 != 0 respecto al total de casas donde BsmtFinSF2 != 0: {porcentaje_no_coincidencia:.2f}%\")\n",
    "\n",
    "        combinaciones_cero = df[df['BsmtFinSF2'] == 0]['BsmtFinType2'].value_counts()\n",
    "        porcentaje_combinaciones = calcular_porcentaje(combinaciones_cero, len(df))\n",
    "\n",
    "        print(\"Combinaciones con BsmtFinSF2 == 0:\")\n",
    "        for tipo, conteo in combinaciones_cero.items():\n",
    "            porcentaje = porcentaje_combinaciones[tipo]\n",
    "            print(f\"BsmtFinType2 = '{tipo}': {conteo} casos ({porcentaje:.2f}%)\")\n",
    "            app_logger.info(f\"BsmtFinType2 = '{tipo}': {porcentaje:.2f}% de los casos\")\n",
    "\n",
    "        porcentaje_total_cero = (df['BsmtFinSF2'] == 0).mean() * 100\n",
    "        print(f\"\\nPorcentaje total de casos con BsmtFinSF2 == 0: {porcentaje_total_cero:.2f}%\")\n",
    "        app_logger.info(f\"El {porcentaje_total_cero:.2f}% de los casos tienen BsmtFinSF2 == 0\")\n",
    "\n",
    "    except KeyError as e:\n",
    "        app_logger.error(f\"Error al acceder a las columnas del DataFrame: {e}\")\n",
    "    except Exception as e:\n",
    "        app_logger.error(f\"Error inesperado durante el análisis: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porcentaje de coincidencia entre BsmtFinType2 == 'Unf' y BsmtFinSF2 == 0 del total de casas donde BsmtFinSF2 == 0: 97.14%\n",
      "Porcentaje de casos donde BsmtFinType2 == 'Unf' pero BsmtFinSF2 != 0 respecto al total de casas donde BsmtFinSF2 != 0: 0.00%\n",
      "Combinaciones con BsmtFinSF2 == 0:\n",
      "BsmtFinType2 = 'Unf': 1256 casos (86.03%)\n",
      "\n",
      "Porcentaje total de casos con BsmtFinSF2 == 0: 88.56%\n"
     ]
    }
   ],
   "source": [
    "# Ejecutar el análisis\n",
    "try:\n",
    "    analizar_bsmt_fin_type2(train_data_filtrado)\n",
    "except Exception as e:\n",
    "    app_logger.error(f\"Error general en la ejecución del análisis: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XiCorr Coeficiente\n",
    "El coeficiente Xicorr (Xi-correlation coefficient) es una medida de asociación entre dos variables que puede ser utilizada como alternativa al coeficiente de correlación de Pearson, especialmente cuando las variables no tienen una relación lineal.\n",
    "\n",
    "### Definición del Coeficiente Xicorr\n",
    "\n",
    "El coeficiente Xicorr se basa en la idea de las \"permutaciones locales\". Se enfoca en la desviación de las permutaciones locales de una relación ideal, y puede ser más robusto frente a distribuciones no normales y relaciones no lineales.\n",
    "\n",
    "#### 1. Supongamos que tenemos dos variables $X$ e $Y$ con $n$ observaciones cada una.\n",
    "\n",
    "$$ X = (X_1, X_2, \\ldots, X_n) $$\n",
    "$$ Y = (Y_1, Y_2, \\ldots, Y_n) $$\n",
    "\n",
    "\n",
    "#### 2. Ordenamos ambas variables en orden ascendente.\n",
    "\n",
    "$$ X_{(1)}, X_{(2)}, \\ldots, X_{(n)} $$\n",
    "$$ Y_{(1)}, Y_{(2)}, \\ldots, Y_{(n)} $$\n",
    "\n",
    "#### 3. Calculamos las posiciones de los valores originales en los vectores ordenados. Denotamos las posiciones de $X_i$ en $X_{(i)}$ como $P_X(i)$ y de $Y_i$ en $Y_{(i)}$ como $P_Y(i)$.\n",
    "\n",
    "$$ P_X(i) = \\text{posición de } X_i \\text{ en } X_{(i)} $$\n",
    "$$ P_Y(i) = \\text{posición de } Y_i \\text{ en } Y_{(i)} $$\n",
    "\n",
    "#### 4. Definimos las permutaciones locales $ \\pi_X $ y $ \\pi_Y $ de $X$ e $Y$ respectivamente, que representan cómo se permutan los valores cuando se ordenan.\n",
    "\n",
    "#### 5. Calculamos la desviación de estas permutaciones locales de una relación ideal. Esta desviación se mide a través de una función de distancia. Una forma común de definir esta distancia es usar la distancia de Kendall ($ \\tau $).\n",
    "\n",
    "$$ \\tau(\\pi_X, \\pi_Y) = \\text{Número de discordancias entre } \\pi_X \\text{ y } \\pi_Y $$\n",
    "\n",
    "#### 6. Normalizamos esta distancia para obtener el coeficiente Xicorr. La normalización se hace para que el coeficiente esté en el rango [-1, 1], similar a los coeficientes de correlación tradicionales.\n",
    "\n",
    "$$ \\text{Xicorr}(X, Y) = 1 - \\frac{2 \\tau(\\pi_X, \\pi_Y)}{n(n-1)/2} $$\n",
    "\n",
    "Aquí, $ n(n-1)/2 $ es el número total de pares posibles en el conjunto de datos.\n",
    "\n",
    "### Interpretación del Coeficiente Xicorr\n",
    "\n",
    "- **Xicorr = 1**: Indica una correlación perfecta y positiva entre las variables $X$ e $Y$.\n",
    "- **Xicorr = -1**: Indica una correlación perfecta y negativa entre las variables $X$ e $Y$.\n",
    "- **Xicorr = 0**: Indica que no hay correlación entre las variables $X$ e $Y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pasos Intuitivos del Cálculo del Coeficiente Xicorr\n",
    "\n",
    "1. **Ordenación de las Variables**:\n",
    "   - Imagina que tienes dos listas de números, una para cada variable, $X$ e $Y$. Primero, ordenas cada lista de menor a mayor.\n",
    "   - Por ejemplo, si $X = [4, 1, 3]$, lo ordenas como $X_{ordenado} = [1, 3, 4]$. Lo mismo haces con $Y$.\n",
    "\n",
    "2. **Rastreo de las Posiciones Originales**:\n",
    "   - Luego, haces un seguimiento de las posiciones originales de los elementos en las listas ordenadas. Es decir, determinas dónde estaba originalmente cada valor en la lista desordenada.\n",
    "   - Por ejemplo, si $X = [4, 1, 3]$ y $X_{ordenado} = [1, 3, 4]$, el valor 1 estaba en la posición 2 originalmente, el 3 en la posición 3 y el 4 en la posición 1.\n",
    "\n",
    "3. **Comparación de las Permutaciones**:\n",
    "   - Ahora, haces lo mismo para la variable $Y$. Una vez que tienes las posiciones originales de ambos conjuntos de datos ordenados, puedes compararlas.\n",
    "   - Imagina que tienes las posiciones originales de $X$ como $\\pi_X$ y las posiciones originales de $Y$ como $\\pi_Y$.\n",
    "\n",
    "4. **Medición de la Discordancia**:\n",
    "   - Comparas estas permutaciones ($\\pi_X$ y $\\pi_Y$) para ver cuán diferentes son. La discordancia se mide contando cuántas veces un par de elementos está en un orden diferente en $X$ en comparación con $Y$.\n",
    "   - Por ejemplo, si en $X$ el segundo elemento viene antes que el primero pero en $Y$ el primero viene antes que el segundo, eso es una discordancia.\n",
    "\n",
    "5. **Calculo de la Distancia de Discordancia**:\n",
    "   - La distancia de Kendall ($\\tau$) es una forma común de medir esta discordancia, contando cuántas discordancias hay entre las posiciones de $X$ e $Y$.\n",
    "   - Si hay muchas discordancias, significa que las listas están muy desalineadas.\n",
    "\n",
    "6. **Normalización de la Distancia**:\n",
    "   - Para obtener el coeficiente Xicorr, normalizas la cantidad de discordancias para que el valor resultante esté en un rango estándar (generalmente de -1 a 1).\n",
    "   - La fórmula de normalización convierte la cantidad de discordancias en un valor que puede interpretarse como un coeficiente de correlación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection Statiscal Methods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_11380\\3479987822.py:20: FutureWarning:\n",
      "\n",
      "Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 correlaciones más fuertes con SalePrice:\n",
      "Id              0.228061\n",
      "PoolArea        0.216643\n",
      "LowQualFinSF    0.216589\n",
      "3SsnPorch       0.213059\n",
      "BsmtFinSF2      0.212266\n",
      "ScreenPorch     0.207370\n",
      "BsmtHalfBath    0.204921\n",
      "KitchenAbvGr    0.193598\n",
      "LotArea         0.154691\n",
      "FullBath        0.116227\n",
      "Name: SalePrice, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\david\\anaconda3\\envs\\david_pcc\\lib\\site-packages\\dcor\\_fast_dcov_avl.py:554: UserWarning:\n",
      "\n",
      "Falling back to uncompiled AVL fast distance covariance terms because of TypeError exception raised: No matching definition for argument type(s) array(int64, 1d, C), array(int64, 1d, C), bool. Rembember: only floating point values can be used in the compiled implementations.\n",
      "\n",
      "c:\\Users\\david\\anaconda3\\envs\\david_pcc\\lib\\site-packages\\dcor\\_dcor_internals.py:188: RuntimeWarning:\n",
      "\n",
      "overflow encountered in scalar multiply\n",
      "\n",
      "c:\\Users\\david\\anaconda3\\envs\\david_pcc\\lib\\site-packages\\dcor\\_fast_dcov_avl.py:554: UserWarning:\n",
      "\n",
      "Falling back to uncompiled AVL fast distance covariance terms because of TypeError exception raised: No matching definition for argument type(s) array(float64, 1d, C), array(int64, 1d, C), bool. Rembember: only floating point values can be used in the compiled implementations.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features en la unión de los top 10 de HSIC, MIC y DCOR:\n",
      "{'BsmtHalfBath', 'TotRmsAbvGrd', 'OverallCond', 'KitchenAbvGr', 'GarageYrBlt', 'YearBuilt', 'BedroomAbvGr', 'OverallQual', 'GrLivArea', '1stFlrSF', 'GarageCars', 'MoSold', 'YrSold', 'FullBath', 'MasVnrArea', 'TotalBsmtSF', 'YearRemodAdd', 'BsmtFullBath', 'GarageArea', 'Fireplaces', 'LotFrontage', 'HalfBath'}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from minepy import MINE\n",
    "import dcor\n",
    "from scipy.stats import gamma\n",
    "from scipy import stats\n",
    "import xicorpy\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "def calcular_xi_correlacion(x, y):\n",
    "    resultado = xicorpy.compute_xi_correlation(x, y, get_modified_xi=False, m_nearest_neighbours=3, get_p_values=False)\n",
    "    return resultado.iloc[0] if isinstance(resultado, (pd.Series, pd.DataFrame)) else resultado\n",
    "\n",
    "def calcular_correlacion(x, y):\n",
    "    # Comprobamos si las variables son numéricas\n",
    "    if not (np.issubdtype(x.dtype, np.number) and np.issubdtype(y.dtype, np.number)):\n",
    "        raise ValueError(\"Ambas variables deben ser numéricas\")\n",
    "    \n",
    "    # Comprobamos la normalidad de las variables\n",
    "    _, p_value_x = stats.normaltest(x)\n",
    "    _, p_value_y = stats.normaltest(y)\n",
    "    \n",
    "    # Si ambas variables son normales, usamos Pearson\n",
    "    if p_value_x > 0.05 and p_value_y > 0.05:\n",
    "        return stats.pearsonr(x, y)[0]\n",
    "    \n",
    "    # Comprobamos la monotonicidad de la relación\n",
    "    is_monotonic = np.all(np.diff(y[np.argsort(x)]) >= 0) or np.all(np.diff(y[np.argsort(x)]) <= 0)\n",
    "    \n",
    "    # Si la relación es monótona, usamos Spearman, si no, Kendall\n",
    "    if is_monotonic:\n",
    "        return stats.spearmanr(x, y)[0]\n",
    "    else:\n",
    "        return stats.kendalltau(x, y)[0]\n",
    "\n",
    "def calcular_matriz_xi(df):\n",
    "    columnas = df.columns\n",
    "    n = len(columnas)\n",
    "    matriz_xi = np.eye(n)\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            xi_valor = calcular_xi_correlacion(df.iloc[:, i], df.iloc[:, j])\n",
    "            matriz_xi[i, j] = matriz_xi[j, i] = xi_valor\n",
    "    return pd.DataFrame(matriz_xi, index=columnas, columns=columnas)\n",
    "\n",
    "def hsic_gam(X, Y, alph=0.05):\n",
    "    n = X.shape[0]\n",
    "    H = np.eye(n) - np.ones((n, n)) / n\n",
    "    Kc = H @ X @ X.T @ H\n",
    "    Lc = H @ Y @ Y.T @ H\n",
    "    testStat = np.trace(Kc @ Lc)\n",
    "    varHSIC = (Kc * Lc).sum() / (n * (n - 1))\n",
    "    K = varHSIC * (n - 1) * (n - 2) * (n - 3)\n",
    "    c = np.sqrt(K) / n\n",
    "    v = varHSIC * (2 * (n - 2)) / (n * (n - 1))\n",
    "    alph = 1 - c / (n * (n - 1))\n",
    "    thresh = gamma.ppf(1 - alph, v / 2, scale=2 / v)\n",
    "    return testStat, thresh\n",
    "\n",
    "def calcular_mic(x, y):\n",
    "    mine = MINE()\n",
    "    mine.compute_score(x, y)\n",
    "    return mine.mic()\n",
    "\n",
    "def analizar_correlaciones(train_data_filtrado):\n",
    "    app_logger.info(\"Calculando la matriz de correlación Xi...\")\n",
    "    matriz_xi = calcular_matriz_xi(train_data_filtrado.select_dtypes(include=[np.number]))\n",
    "    correlaciones_saleprice = matriz_xi['SalePrice'].sort_values(ascending=False).drop('SalePrice')\n",
    "    top_correlaciones = correlaciones_saleprice.head(10)\n",
    "    print(\"Top 10 correlaciones más fuertes con SalePrice:\")\n",
    "    print(top_correlaciones)\n",
    "    app_logger.info(\"Se han identificado las 10 correlaciones más fuertes con SalePrice utilizando el método Xi.\")\n",
    "\n",
    "def realizar_prueba_hsic(train_data_filtrado):\n",
    "    app_logger.info(\"Realizando prueba de la función HSIC...\")\n",
    "    caracteristicas_numericas = train_data_filtrado.select_dtypes(include=[np.number]).drop(columns=['SalePrice'])\n",
    "    Y = train_data_filtrado['SalePrice'].values.reshape(-1, 1)\n",
    "    resultados = {}\n",
    "    for caracteristica in caracteristicas_numericas:\n",
    "        X = train_data_filtrado[caracteristica].values.reshape(-1, 1)\n",
    "        estadistico_prueba, umbral = hsic_gam(X, Y)\n",
    "        resultados[caracteristica] = {\n",
    "            'estadistico': estadistico_prueba,\n",
    "            'umbral': umbral,\n",
    "            'dependiente': estadistico_prueba > umbral\n",
    "        }\n",
    "        if resultados[caracteristica]['dependiente']:\n",
    "            app_logger.info(f\"La variable {caracteristica} y el precio de venta son dependientes según la prueba HSIC.\")\n",
    "        else:\n",
    "            app_logger.info(f\"No se puede rechazar la hipótesis de independencia entre {caracteristica} y el precio de venta.\")\n",
    "    \n",
    "    caracteristicas_dependientes = {k: v for k, v in resultados.items() if v['dependiente']}\n",
    "    app_logger.info(f\"Se encontraron {len(caracteristicas_dependientes)} características dependientes del precio de venta según la prueba HSIC.\")\n",
    "    return resultados, caracteristicas_dependientes\n",
    "\n",
    "def analizar_mic_y_correlacion_distancia(train_data_filtrado):\n",
    "    app_logger.info(\"Calculando los puntajes MIC y correlación de distancia...\")\n",
    "    columnas_numericas = train_data_filtrado.select_dtypes(include=[np.number]).drop(columns=['SalePrice']).columns\n",
    "    puntajes_mic = {columna: calcular_mic(train_data_filtrado[columna], train_data_filtrado['SalePrice']) for columna in columnas_numericas}\n",
    "    puntajes_corr_dist = {columna: dcor.distance_correlation(train_data_filtrado[columna], train_data_filtrado['SalePrice']) for columna in columnas_numericas}\n",
    "    \n",
    "    puntajes_mic = sorted(puntajes_mic.items(), key=lambda item: item[1], reverse=True)\n",
    "    puntajes_corr_dist = sorted(puntajes_corr_dist.items(), key=lambda item: item[1], reverse=True)\n",
    "    \n",
    "    app_logger.info(\"Análisis de MIC y correlación de distancia completado.\")\n",
    "    return puntajes_mic, puntajes_corr_dist\n",
    "\n",
    "def analizar_correlacion(train_data_filtrado):\n",
    "    app_logger.info(\"Analizando la correlación entre las características...\")\n",
    "    \n",
    "    # Seleccionar solo las columnas numéricas\n",
    "    datos_numericos = train_data_filtrado.select_dtypes(include=[np.number])\n",
    "    \n",
    "    # Calcular las correlaciones con 'SalePrice' usando la función personalizada\n",
    "    correlaciones_saleprice = {}\n",
    "    for columna in datos_numericos.columns:\n",
    "        if columna != 'SalePrice':\n",
    "            try:\n",
    "                correlacion = calcular_correlacion(datos_numericos[columna], datos_numericos['SalePrice'])\n",
    "                correlaciones_saleprice[columna] = correlacion\n",
    "            except ValueError as e:\n",
    "                app_logger.warning(f\"No se pudo calcular la correlación para {columna}: {str(e)}\")\n",
    "    \n",
    "    # Ordenar las correlaciones\n",
    "    correlaciones_saleprice = dict(sorted(correlaciones_saleprice.items(), key=lambda item: abs(item[1]), reverse=True))\n",
    "    \n",
    "    # Obtener las 10 características más correlacionadas con 'SalePrice'\n",
    "    top_10_correlaciones = dict(list(correlaciones_saleprice.items())[:10])\n",
    "    \n",
    "    print(\"Las 10 características más correlacionadas con SalePrice son:\")\n",
    "    for caracteristica, correlacion in top_10_correlaciones.items():\n",
    "        print(f\"{caracteristica}: {correlacion}\")\n",
    "    \n",
    "    app_logger.info(\"Análisis de correlación completado.\")\n",
    "    \n",
    "    return top_10_correlaciones\n",
    "\n",
    "# Ejecutar la función de análisis de correlación\n",
    "top_10_correlaciones = analizar_correlacion(train_data_filtrado)\n",
    "\n",
    "# Ejecutar las funciones\n",
    "analizar_correlaciones(train_data_filtrado)\n",
    "resultados_hsic, caracteristicas_dependientes = realizar_prueba_hsic(train_data_filtrado)\n",
    "puntajes_mic, puntajes_corr_dist = analizar_mic_y_correlacion_distancia(train_data_filtrado)\n",
    "\n",
    "# Unir los resultados\n",
    "union_features = set(caracteristicas_dependientes.keys()) | set(dict(puntajes_mic[:10]).keys()) | set(dict(puntajes_corr_dist[:10]).keys())\n",
    "\n",
    "app_logger.info(\"Features en la unión de los top 10 de HSIC, MIC y DCOR:\")\n",
    "app_logger.info(union_features)\n",
    "\n",
    "print(\"Features en la unión de los top 10 de HSIC, MIC y DCOR:\")\n",
    "print(union_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar las bibliotecas necesarias\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar los datos\n",
    "try:\n",
    "    # Seleccionar las características de union_features\n",
    "    X = train_data_filtrado[list(union_features)]\n",
    "    y = train_data_filtrado['SalePrice']\n",
    "except Exception as e:\n",
    "    errors_logger.error(f\"Error al seleccionar las características de union_features: {e}\")\n",
    "    raise\n",
    "\n",
    "app_logger.info(\"Características seleccionadas correctamente.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Características de baja varianza en union_features:\n",
      "['KitchenAbvGr']\n"
     ]
    }
   ],
   "source": [
    "# Identificar características de baja varianza que están en union_features\n",
    "caracteristicas_baja_varianza = [feature for feature in Low_Variance_Features if feature in union_features]\n",
    "\n",
    "if len(caracteristicas_baja_varianza) > 0:\n",
    "    app_logger.info(f\"Se encontraron {len(caracteristicas_baja_varianza)} características de baja varianza en union_features:\")\n",
    "    for caracteristica in caracteristicas_baja_varianza:\n",
    "        app_logger.info(f\"  - {caracteristica}\")\n",
    "else:\n",
    "    app_logger.info(\"No se encontraron características de baja varianza de Low_variance_features en union_features.\")\n",
    "\n",
    "# Imprimir resultados\n",
    "print(\"Características de baja varianza en union_features:\")\n",
    "print(caracteristicas_baja_varianza)\n",
    "\n",
    "print(\"Características de baja varianza totales:\")\n",
    "print(Low_Variance_Features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminamos esta feature por que no aporta información relevante ya que el 95,31% estan a 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Características restantes en X después de eliminar 'KitchenAbvGr':\n",
      "['OverallQual', 'GarageArea', 'TotalBsmtSF', 'LotFrontage', 'BedroomAbvGr', 'YearBuilt', 'YearRemodAdd', 'OverallCond', 'MasVnrArea', 'BsmtHalfBath', 'TotRmsAbvGrd', 'GarageCars', 'HalfBath', 'GarageYrBlt', '1stFlrSF', 'Fireplaces', 'YrSold', 'FullBath', 'MoSold', 'BsmtFullBath', 'GrLivArea']\n"
     ]
    }
   ],
   "source": [
    "# Eliminar la característica 'KitchenAbvGr' de X\n",
    "app_logger.info(\"Eliminando la característica 'KitchenAbvGr' de X...\")\n",
    "try:\n",
    "    X = X.drop('KitchenAbvGr', axis=1)\n",
    "    app_logger.info(\"La característica 'KitchenAbvGr' ha sido eliminada de X.\")\n",
    "except KeyError:\n",
    "    app_logger.info(\"La característica 'KitchenAbvGr' no se encuentra en X.\")\n",
    "except Exception as e:\n",
    "    errors_logger.error(f\"Error al eliminar la característica 'KitchenAbvGr' de X: {e}\")\n",
    "    raise\n",
    "\n",
    "# Imprimir las características restantes\n",
    "print(\"Características restantes en X después de eliminar 'KitchenAbvGr':\")\n",
    "print(X.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables con valores faltantes:\n",
      "             Valores Faltantes  Porcentaje\n",
      "LotFrontage                259   17.739726\n",
      "GarageYrBlt                 81    5.547945\n",
      "MasVnrArea                   8    0.547945\n"
     ]
    }
   ],
   "source": [
    "# Análisis de valores faltantes\n",
    "app_logger.info(\"Iniciando análisis de valores faltantes...\")\n",
    "\n",
    "try:\n",
    "    # Calcular el número de valores faltantes por columna\n",
    "    missing_values = X.isnull().sum()\n",
    "    \n",
    "    # Calcular el porcentaje de valores faltantes\n",
    "    missing_percentage = 100 * missing_values / len(X)\n",
    "    \n",
    "    # Crear un DataFrame con los resultados\n",
    "    missing_table = pd.concat([missing_values, missing_percentage], axis=1, keys=['Valores Faltantes', 'Porcentaje'])\n",
    "    \n",
    "    # Ordenar el DataFrame por el número de valores faltantes en orden descendente\n",
    "    missing_table = missing_table[missing_table['Valores Faltantes'] > 0].sort_values('Valores Faltantes', ascending=False)\n",
    "    \n",
    "    if not missing_table.empty:\n",
    "        app_logger.info(\"Se encontraron las siguientes variables con valores faltantes:\")\n",
    "        for index, row in missing_table.iterrows():\n",
    "            app_logger.info(f\"  - {index}: {row['Valores Faltantes']} valores faltantes ({row['Porcentaje']:.2f}%)\")\n",
    "        \n",
    "        print(\"Variables con valores faltantes:\")\n",
    "        print(missing_table)\n",
    "    else:\n",
    "        app_logger.info(\"No se encontraron variables con valores faltantes.\")\n",
    "        print(\"No hay variables con valores faltantes.\")\n",
    "\n",
    "except Exception as e:\n",
    "    errors_logger.error(f\"Error durante el análisis de valores faltantes: {e}\")\n",
    "    raise\n",
    "\n",
    "app_logger.info(\"Análisis de valores faltantes completado.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tratamiento de nulos de ``GarageYrBlt``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocesar_GarageYrBlt(X):\n",
    "    try:\n",
    "        # Rellenar valores nulos de GarageYrBlt con YearBuilt\n",
    "        X.loc[:, 'GarageYrBlt'] = X.loc[:, 'GarageYrBlt'].fillna(X['YearBuilt'])\n",
    "        \n",
    "        # Usar .loc para ajustar GarageYrBlt\n",
    "        X.loc[:, 'GarageYrBlt'] = X.apply(lambda row: max(row['GarageYrBlt'], row['YearBuilt']), axis=1)\n",
    "        \n",
    "        app_logger.info(\"Se ha utilizado .loc para evitar SettingWithCopyWarning en el preprocesamiento de GarageYrBlt\")\n",
    "        return X\n",
    "    except KeyError as e:\n",
    "        app_logger.error(f\"Error: Columna no encontrada - {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        app_logger.error(f\"Error inesperado durante el preprocesamiento de GarageYrBlt: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    Nans_previous = X['GarageYrBlt'].isna().sum()\n",
    "    X = preprocesar_GarageYrBlt(X)\n",
    "    Nans_after = X['GarageYrBlt'].isna().sum()\n",
    "    app_logger.info(f\"Preprocesamiento de GarageYrBlt completado. Nans anteriores: {Nans_previous}, Nans después: {Nans_after}\")\n",
    "except Exception as e:\n",
    "    app_logger.error(f\"Error durante el preprocesamiento de GarageYrBlt: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tratamiento de NaNs de ``MasVnrArea``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determinar_tipo_y_tratar_nans(X, columna):\n",
    "    try:\n",
    "        tipo = X[columna].dtype\n",
    "        \n",
    "        if np.issubdtype(tipo, np.number):\n",
    "            if X[columna].nunique() < 20:\n",
    "                app_logger.info(f\"{columna} se considerará categórica debido a que tiene menos de 20 valores únicos.\")\n",
    "                es_categorica = True\n",
    "            else:\n",
    "                app_logger.info(f\"{columna} se considerará numérica.\")\n",
    "                es_categorica = False\n",
    "        else:\n",
    "            app_logger.info(f\"{columna} es categórica por su tipo de dato.\")\n",
    "            es_categorica = True\n",
    "        \n",
    "        if es_categorica:\n",
    "            moda = X[columna].mode()[0]\n",
    "            X[columna].fillna(moda, inplace=True)\n",
    "            app_logger.info(f\"Se han rellenado los NaNs de {columna} con la moda: {moda}\")\n",
    "        else:\n",
    "            mediana = X[columna].median()\n",
    "            X[columna].fillna(mediana, inplace=True)\n",
    "            app_logger.info(f\"Se han rellenado los NaNs de {columna} con la mediana: {mediana}\")\n",
    "        \n",
    "        nans_restantes = X[columna].isna().sum()\n",
    "        app_logger.info(f\"NaNs restantes en {columna} después del tratamiento: {nans_restantes}\")\n",
    "        \n",
    "    except KeyError:\n",
    "        app_logger.error(f\"La columna {columna} no existe en el DataFrame.\")\n",
    "    except Exception as e:\n",
    "        app_logger.error(f\"Error inesperado al procesar la columna {columna}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uso de la función\n",
    "try:\n",
    "    determinar_tipo_y_tratar_nans(X, 'MasVnrArea')\n",
    "except Exception as e:\n",
    "    app_logger.error(f\"Error general en el procesamiento: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tratamiento de NaNs de ``LotFrontage``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tratar_nans_lotfrontage(X):\n",
    "    try:\n",
    "        mediana_LotFrontage = X['LotFrontage'].median()\n",
    "        X['LotFrontage'].fillna(mediana_LotFrontage, inplace=True)\n",
    "        app_logger.info(f\"Se han rellenado los NaNs de LotFrontage con la mediana: {mediana_LotFrontage}\")\n",
    "        \n",
    "        nans_restantes_LotFrontage = X['LotFrontage'].isna().sum()\n",
    "        app_logger.info(f\"NaNs restantes en LotFrontage después del tratamiento: {nans_restantes_LotFrontage}\")\n",
    "        \n",
    "        return X\n",
    "    except KeyError:\n",
    "        app_logger.error(\"La columna 'LotFrontage' no existe en el DataFrame.\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        app_logger.error(f\"Error inesperado al tratar los NaNs de LotFrontage: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uso de la función\n",
    "try:\n",
    "    X = tratar_nans_lotfrontage(X)\n",
    "except Exception as e:\n",
    "    app_logger.error(f\"No se pudo completar el tratamiento de NaNs en LotFrontage: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def procesar_variables_categoricas(X, variables):\n",
    "    \"\"\"Convierte las variables especificadas a tipo categórico.\"\"\"\n",
    "    try:\n",
    "        for var in variables:\n",
    "            X[var] = X[var].astype('category')\n",
    "            app_logger.info(f\"Se ha convertido {var} a tipo categórico\")\n",
    "        return X\n",
    "    except KeyError as e:\n",
    "        app_logger.error(f\"Error al procesar variable categórica: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        app_logger.error(f\"Error inesperado al procesar variables categóricas: {e}\")\n",
    "        raise\n",
    "\n",
    "def crear_nuevas_features(X):\n",
    "    \"\"\"Crea nuevas features basadas en años.\"\"\"\n",
    "    try:\n",
    "        X['BuiltUntilSold'] = X['YrSold'] - X['YearBuilt']\n",
    "        X['BuiltUntilRemod'] = X['YearRemodAdd'] - X['YearBuilt']\n",
    "        X['HasBeenRemodeled'] = (X['YearRemodAdd'] != X['YearBuilt']).astype(bool)\n",
    "        app_logger.info(\"Se han creado las nuevas features: BuiltUntilSold, BuiltUntilRemod y HasBeenRemodeled\")\n",
    "        return X\n",
    "    except KeyError as e:\n",
    "        app_logger.error(f\"Error al crear nuevas features: Columna no encontrada - {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        app_logger.error(f\"Error inesperado al crear nuevas features: {e}\")\n",
    "        raise\n",
    "\n",
    "def clasificar_variables(X):\n",
    "    \"\"\"Clasifica las variables en numéricas y categóricas.\"\"\"\n",
    "    variables_numericas = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    variables_categoricas = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    \n",
    "    variables_a_excluir = ['YearRemodAdd', 'YearBuilt', 'YrSold'] + variables_cardinales\n",
    "    variables_numericas = [var for var in variables_numericas if var not in variables_a_excluir]\n",
    "    \n",
    "    app_logger.info(f\"Variables numéricas: {variables_numericas}\")\n",
    "    app_logger.info(f\"Variables categóricas: {variables_categoricas}\")\n",
    "    return variables_numericas, variables_categoricas\n",
    "\n",
    "def imprimir_estadisticas(X):\n",
    "    \"\"\"Imprime estadísticas de las nuevas features.\"\"\"\n",
    "    app_logger.info(f\"Estadísticas de BuiltUntilSold: \\n{X['BuiltUntilSold'].describe()}\")\n",
    "    app_logger.info(f\"Estadísticas de BuiltUntilRemod: \\n{X['BuiltUntilRemod'].describe()}\")\n",
    "    app_logger.info(f\"Proporción de casas remodeladas: {X['HasBeenRemodeled'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables globales\n",
    "variables_cardinales = ['HalfBath', 'Fireplaces', 'FullBath', 'BedroomAbvGr', 'TotRmsAbvGrd', 'GarageCars', 'BsmtHalfBath', 'BsmtFullBath']\n",
    "variables_ordinales = ['OverallQual', 'OverallCond']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procesamiento principal\n",
    "try:\n",
    "    # Asumimos que X es un DataFrame de pandas ya cargado\n",
    "    X = procesar_variables_categoricas(X, variables_cardinales + variables_ordinales)\n",
    "    X = crear_nuevas_features(X)\n",
    "    variables_numericas, variables_categoricas = clasificar_variables(X)\n",
    "    imprimir_estadisticas(X)\n",
    "except Exception as e:\n",
    "    errors_logger.error(f\"Error en el procesamiento principal: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar las bibliotecas necesarias\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "import pandas as pd\n",
    "\n",
    "transformed_data = X.copy()\n",
    "\n",
    "# Función para aplicar Label Encoding y convertir a tipo category\n",
    "def aplicar_label_encoding(X, columna):\n",
    "    try:\n",
    "        le = LabelEncoder()\n",
    "        X[columna] = le.fit_transform(X[columna])\n",
    "        X[columna] = X[columna].astype('category')\n",
    "        app_logger.info(f\"Se aplicó Label Encoding a la columna {columna} y se convirtió a tipo category\")\n",
    "    except Exception as e:\n",
    "        errors_logger.error(f\"Error al aplicar Label Encoding a la columna {columna}: {str(e)}\")\n",
    "    return X\n",
    "\n",
    "# Función para aplicar One-Hot Encoding\n",
    "def aplicar_one_hot_encoding(X, columna):\n",
    "    try:\n",
    "        ohe = OneHotEncoder(drop='first', sparse_output=False)\n",
    "        encoded = ohe.fit_transform(X[[columna]])\n",
    "        encoded_df = pd.DataFrame(encoded, columns=[f\"{columna}_{cat}\" for cat in ohe.categories_[0][1:]], dtype=bool)\n",
    "        X = pd.concat([X.drop(columna, axis=1), encoded_df], axis=1)\n",
    "        app_logger.info(f\"Se aplicó One-Hot Encoding a la columna {columna}\")\n",
    "    except Exception as e:\n",
    "        errors_logger.error(f\"Error al aplicar One-Hot Encoding a la columna {columna}: {str(e)}\")\n",
    "    return X\n",
    "\n",
    "# Iterar sobre las columnas categóricas\n",
    "for columna in transformed_data.select_dtypes(include=['object', 'category']).columns:\n",
    "    num_categorias = transformed_data[columna].nunique()\n",
    "    \n",
    "    if num_categorias > 6:\n",
    "        transformed_data = aplicar_label_encoding(transformed_data, columna)\n",
    "    else:\n",
    "        transformed_data = aplicar_one_hot_encoding(transformed_data, columna)\n",
    "\n",
    "app_logger.info(\"Se ha completado el procesamiento de variables categóricas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La columna GarageArea no sigue una distribución normal (p-valor: 0.0000)\n",
      "La columna TotalBsmtSF no sigue una distribución normal (p-valor: 0.0000)\n",
      "La columna LotFrontage no sigue una distribución normal (p-valor: 0.0000)\n",
      "La columna YearBuilt no sigue una distribución normal (p-valor: 0.0000)\n",
      "La columna YearRemodAdd no sigue una distribución normal (p-valor: 0.0000)\n",
      "La columna MasVnrArea no sigue una distribución normal (p-valor: 0.0000)\n",
      "La columna GarageYrBlt no sigue una distribución normal (p-valor: 0.0000)\n",
      "La columna 1stFlrSF no sigue una distribución normal (p-valor: 0.0009)\n",
      "La columna YrSold no sigue una distribución normal (p-valor: 0.0000)\n",
      "La columna MoSold no sigue una distribución normal (p-valor: 0.0000)\n",
      "La columna GrLivArea no sigue una distribución normal (p-valor: 0.0009)\n",
      "La columna BuiltUntilSold no sigue una distribución normal (p-valor: 0.0000)\n",
      "La columna BuiltUntilRemod no sigue una distribución normal (p-valor: 0.0000)\n",
      "La columna GarageArea_power no sigue una distribución normal (p-valor: 0.0000)\n",
      "La columna TotalBsmtSF_quantile no sigue una distribución normal (p-valor: 0.0000)\n",
      "La columna LotFrontage_quantile no sigue una distribución normal (p-valor: 0.0000)\n",
      "La columna YearBuilt_robustscaler no sigue una distribución normal (p-valor: 0.0000)\n",
      "La columna YearRemodAdd_robustscaler no sigue una distribución normal (p-valor: 0.0000)\n",
      "La columna MasVnrArea_quantile no sigue una distribución normal (p-valor: 0.0000)\n",
      "La columna GarageYrBlt_robustscaler no sigue una distribución normal (p-valor: 0.0000)\n",
      "La columna 1stFlrSF_quantile no sigue una distribución normal (p-valor: 0.0017)\n",
      "La columna MoSold_quantile no sigue una distribución normal (p-valor: 0.0000)\n",
      "La columna GrLivArea_quantile no sigue una distribución normal (p-valor: 0.0018)\n",
      "La columna BuiltUntilSold_robustscaler no sigue una distribución normal (p-valor: 0.0000)\n",
      "La columna BuiltUntilRemod_robustscaler no sigue una distribución normal (p-valor: 0.0000)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, PowerTransformer, QuantileTransformer\n",
    "from scipy.stats import shapiro\n",
    "\n",
    "def select_transformation(data: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Selecciona la transformación apropiada para cada columna numérica del DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): DataFrame de entrada.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Diccionario con las transformaciones seleccionadas para cada columna.\n",
    "    \"\"\"\n",
    "    transformations = {}\n",
    "    numeric_columns = data.select_dtypes(include=['int64', 'float64']).columns\n",
    "    \n",
    "    for column in numeric_columns:\n",
    "        try:\n",
    "            column_data = data[column].dropna()\n",
    "            \n",
    "            if len(column_data) < 3:\n",
    "                transformations[column] = None\n",
    "                app_logger.info(f\"La columna {column} tiene menos de 3 valores no nulos. No se aplicará transformación.\")\n",
    "                continue\n",
    "            \n",
    "            _, p_value = shapiro(column_data)\n",
    "            skewness = column_data.skew()\n",
    "            unique_count = column_data.nunique()\n",
    "            \n",
    "            if p_value > 0.05:\n",
    "                transformations[column] = StandardScaler()\n",
    "                app_logger.info(f\"Se seleccionó StandardScaler para la columna {column}\")\n",
    "            elif abs(skewness) > 1:\n",
    "                transformations[column] = PowerTransformer(method='yeo-johnson')\n",
    "                app_logger.info(f\"Se seleccionó PowerTransformer para la columna {column}\")\n",
    "            elif unique_count < 10:\n",
    "                transformations[column] = None\n",
    "                app_logger.info(f\"La columna {column} tiene menos de 10 valores únicos. No se aplicará transformación.\")\n",
    "            elif abs(skewness) > 0.5:\n",
    "                transformations[column] = RobustScaler()\n",
    "                app_logger.info(f\"Se seleccionó RobustScaler para la columna {column}\")\n",
    "            else:\n",
    "                transformations[column] = QuantileTransformer(output_distribution='normal')\n",
    "                app_logger.info(f\"Se seleccionó QuantileTransformer para la columna {column}\")\n",
    "        except Exception as e:\n",
    "            errors_logger.error(f\"Error al procesar la columna {column}: {str(e)}\")\n",
    "            transformations[column] = None\n",
    "    \n",
    "    return transformations\n",
    "\n",
    "def apply_transformations(data: pd.DataFrame, transformations: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aplica las transformaciones seleccionadas a las columnas del DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): DataFrame de entrada.\n",
    "        transformations (dict): Diccionario con las transformaciones a aplicar.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame con las transformaciones aplicadas.\n",
    "    \"\"\"\n",
    "    transformed_data = data.copy()\n",
    "    \n",
    "    for column, transformer in transformations.items():\n",
    "        try:\n",
    "            if transformer is not None:\n",
    "                transformed_data[column] = transformer.fit_transform(data[[column]])\n",
    "                app_logger.info(f\"Se aplicó la transformación {type(transformer).__name__} a la columna {column}\")\n",
    "            else:\n",
    "                app_logger.info(f\"No se aplicó transformación a la columna {column}\")\n",
    "        except Exception as e:\n",
    "            errors_logger.error(f\"Error al aplicar la transformación a la columna {column}: {str(e)}\")\n",
    "            transformed_data[column] = data[column]\n",
    "    \n",
    "    return transformed_data\n",
    "\n",
    "try:\n",
    "    transformations = select_transformation(transformed_data)\n",
    "    transformed_data = apply_transformations(transformed_data, transformations)\n",
    "    app_logger.info(\"Se completó la selección y aplicación de transformaciones\")\n",
    "except Exception as e:\n",
    "    errors_logger.error(f\"Error general en el proceso de transformación: {str(e)}\")\n",
    "\n",
    "def select_transformation(data: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Selecciona la transformación apropiada para cada columna numérica del DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): DataFrame de entrada.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Diccionario con las transformaciones seleccionadas para cada columna.\n",
    "    \"\"\"\n",
    "    transformations = {}\n",
    "    numeric_columns = data.select_dtypes(include=['int64', 'float64']).columns\n",
    "    \n",
    "    for column in numeric_columns:\n",
    "        column_data = data[column].dropna()\n",
    "        \n",
    "        if len(column_data) < 3:\n",
    "            transformations[column] = None\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            _, p_value = shapiro(column_data)\n",
    "            skewness = column_data.skew()\n",
    "            unique_count = column_data.nunique()\n",
    "            \n",
    "            if p_value > 0.05:\n",
    "                transformations[column] = StandardScaler()\n",
    "            elif abs(skewness) > 1:\n",
    "                transformations[column] = PowerTransformer(method='yeo-johnson')\n",
    "            elif unique_count < 10:\n",
    "                transformations[column] = None\n",
    "            elif abs(skewness) > 0.5:\n",
    "                transformations[column] = RobustScaler()\n",
    "            else:\n",
    "                transformations[column] = QuantileTransformer(output_distribution='normal')\n",
    "        except Exception as e:\n",
    "            print(f\"Error al procesar la columna {column}: {str(e)}\")\n",
    "            transformations[column] = None\n",
    "    \n",
    "    return transformations\n",
    "\n",
    "def apply_transformations(data: pd.DataFrame, transformations: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aplica las transformaciones seleccionadas a las columnas del DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): DataFrame de entrada.\n",
    "        transformations (dict): Diccionario con las transformaciones a aplicar.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame con las transformaciones aplicadas.\n",
    "    \"\"\"\n",
    "    transformed_data = data.copy()\n",
    "    \n",
    "    for column, transformer in transformations.items():\n",
    "        if transformer is not None:\n",
    "            try:\n",
    "                transformed_column = transformer.fit_transform(data[[column]])\n",
    "                suffix = '_' + transformer.__class__.__name__.lower().replace('transformer', '')\n",
    "                new_column_name = f\"{column}{suffix}\"\n",
    "                transformed_data[new_column_name] = transformed_column\n",
    "            except Exception as e:\n",
    "                print(f\"Error al transformar la columna {column}: {str(e)}\")\n",
    "        else:\n",
    "            transformed_data[column] = data[column]\n",
    "    \n",
    "    return transformed_data\n",
    "\n",
    "def check_normality(data: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Comprueba la normalidad de las columnas numéricas después de las transformaciones.\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): DataFrame transformado.\n",
    "    \"\"\"\n",
    "    numeric_columns = data.select_dtypes(include=['int64', 'float64']).columns\n",
    "    \n",
    "    for column in numeric_columns:\n",
    "        column_data = data[column].dropna()\n",
    "        \n",
    "        if len(column_data) < 3:\n",
    "            print(f\"La columna {column} tiene menos de 3 valores no nulos.\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            _, p_value = shapiro(column_data)\n",
    "            status = \"sigue\" if p_value > 0.05 else \"no sigue\"\n",
    "            print(f\"La columna {column} {status} una distribución normal (p-valor: {p_value:.4f})\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error al comprobar la normalidad de la columna {column}: {str(e)}\")\n",
    "\n",
    "# Uso del código mejorado\n",
    "try:\n",
    "    transformations = select_transformation(transformed_data)\n",
    "    transformed_data = apply_transformations(transformed_data, transformations)\n",
    "    check_normality(transformed_data)\n",
    "except Exception as e:\n",
    "    print(f\"Error general: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import cross_validate, train_test_split, cross_val_predict\n",
    "from sklearn.metrics import make_scorer, mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "import time\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "scoring = {\n",
    "    'rmse': make_scorer(rmse, greater_is_better=False),\n",
    "    'mse': 'neg_mean_squared_error',\n",
    "    'mae': 'neg_mean_absolute_error',\n",
    "    'r2': 'r2'\n",
    "}\n",
    "\n",
    "def plot_residuals(y_true, y_pred, title, version):\n",
    "    app_logger.info(f'Creando plot de residuos - {title}')\n",
    "    print('Creando plot de residuos')\n",
    "    # Gráfico de residuos vs valores predichos\n",
    "    residuals = y_true - y_pred\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(x=y_pred, y=residuals)\n",
    "    plt.axhline(y=0, color='r', linestyle='--')\n",
    "    plt.xlabel('Valores predichos')\n",
    "    plt.ylabel('Residuos')\n",
    "    plt.title(f'Gráfico de Residuos - {title}')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Crear subdirectorio para la versión\n",
    "    version_dir = f'../plots/v{version}'\n",
    "    os.makedirs(version_dir, exist_ok=True)\n",
    "    \n",
    "    path = f'{version_dir}/residuos_{title.replace(\" \", \"_\")}.png'\n",
    "    plt.savefig(path)\n",
    "    plt.close()\n",
    "    logger.log_visualization(f'Gráfico de Residuos - {title}', path)\n",
    "\n",
    "    # Distribución de los residuos\n",
    "    app_logger.info(f'Creando distribución de residuos - {title}')\n",
    "    print('Creando distribución de residuos')\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(residuals, kde=True)\n",
    "    plt.xlabel('Residuos')\n",
    "    plt.ylabel('Frecuencia')\n",
    "    plt.title(f'Distribución de Residuos - {title}')\n",
    "    plt.tight_layout()\n",
    "    path = f'{version_dir}/distribucion_residuos_{title.replace(\" \", \"_\")}.png'\n",
    "    plt.savefig(path)\n",
    "    plt.close()\n",
    "    logger.log_visualization(f'Distribución de Residuos - {title}', path)\n",
    "\n",
    "\n",
    "def plot_regression(y_true, y_pred, title, version):\n",
    "    app_logger.info(f'Creando plot de regresión - {title}')\n",
    "    print('Creando plot de regresión')\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(x=y_true, y=y_pred)\n",
    "    plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--', lw=2)\n",
    "    plt.xlabel('True values')\n",
    "    plt.ylabel('Predicted values')\n",
    "    plt.title(f'Regression Plot - {title}')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Crear subdirectorio para la versión\n",
    "    version_dir = f'../plots/v{version}'\n",
    "    os.makedirs(version_dir, exist_ok=True)\n",
    "    \n",
    "    path = f'{version_dir}/regression_{title.replace(\" \", \"_\")}.png'\n",
    "    plt.savefig(path)\n",
    "    plt.close()\n",
    "    logger.log_visualization(f'Regression Plot - {title}', path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluar_modelo(nombre, modelo, X, y, transformado=False, version=\"1.0\"):\n",
    "    '''\n",
    "    \"\"\"\n",
    "    Evalúa un modelo de regresión utilizando validación cruzada y métricas de rendimiento.\n",
    "\n",
    "    Esta función realiza las siguientes tareas:\n",
    "    1. Ejecuta una validación cruzada de 5 folds en el modelo proporcionado.\n",
    "    2. Entrena el modelo en todo el conjunto de datos.\n",
    "    3. Calcula métricas de rendimiento tanto para la validación cruzada como para el conjunto de entrenamiento.\n",
    "    4. Registra los resultados y genera visualizaciones.\n",
    "\n",
    "    Parámetros:\n",
    "    -----------\n",
    "    ``nombre`` : str\n",
    "        Nombre del modelo a evaluar.\n",
    "    ``modelo`` : objeto estimador de scikit-learn\n",
    "        Modelo de regresión a evaluar.\n",
    "    ``X`` : array-like o DataFrame\n",
    "        Características de entrada.\n",
    "    ``y`` : array-like o Series\n",
    "        Variable objetivo.\n",
    "    ``transformado`` : bool, opcional (default=False)\n",
    "        Indica si los datos han sido transformados previamente.\n",
    "\n",
    "    Retorna:\n",
    "    --------\n",
    "    dict\n",
    "        Un diccionario con las métricas de rendimiento calculadas.\n",
    "\n",
    "    Efectos secundarios:\n",
    "    --------------------\n",
    "    - Registra los resultados utilizando el logger.\n",
    "    - Genera y guarda gráficos de residuos y regresión.\n",
    "\n",
    "    Notas:\n",
    "    ------\n",
    "    Las métricas calculadas incluyen RMSE, MSE, MAE y R2, tanto para validación cruzada como para el conjunto de entrenamiento completo.\n",
    "    \"\"\"\n",
    "    '''\n",
    "    inicio = time.time()\n",
    "    print(f'Evaluando modelo: {nombre}')\n",
    "    app_logger.info(f'Evaluando modelo: {nombre} en la version - {version}')\n",
    "    try:\n",
    "        \n",
    "        # Validación cruzada\n",
    "        resultados_cv = cross_validate(modelo, X, y, cv=5, scoring=scoring, n_jobs=-1, return_train_score=True)\n",
    "        \n",
    "        tiempo_ejecucion = time.time() - inicio\n",
    "        \n",
    "        # Métricas de validación cruzada\n",
    "        rmse_cv = -np.mean(resultados_cv['test_rmse'])\n",
    "        mse_cv = -np.mean(resultados_cv['test_mse'])\n",
    "        mae_cv = -np.mean(resultados_cv['test_mae'])\n",
    "        r2_cv = np.mean(resultados_cv['test_r2'])\n",
    "        \n",
    "        # Métricas en conjunto de entrenamiento\n",
    "        rmse_train = -np.mean(resultados_cv['train_rmse'])\n",
    "        mse_train = -np.mean(resultados_cv['train_mse'])\n",
    "        mae_train = -np.mean(resultados_cv['train_mae'])\n",
    "        r2_train = np.mean(resultados_cv['train_r2'])\n",
    "        \n",
    "        sufijo = \"transformado\" if transformado else \"sin transformar\"\n",
    "        estudio = f\"Regresión simple {nombre} - {sufijo}\"\n",
    "        hiperparams = str(modelo.get_params())\n",
    "        \n",
    "        # Registrar resultados\n",
    "        logger.log_results(estudio, version, nombre, rmse_cv, mse_cv, mae_cv, r2_cv, hiperparams, tiempo_ejecucion)\n",
    "\n",
    "        # Registrar train\n",
    "        logger.log_results(estudio + \" - Train\", version, nombre, rmse_train, mse_train, mae_train, r2_train, hiperparams, tiempo_ejecucion)\n",
    "\n",
    "        #Realizar inferencia (predicciones) con cross_val_predict\n",
    "        \n",
    "        y_pred = cross_val_predict(modelo, X, y, cv=5, n_jobs=-1)\n",
    "        \n",
    "        \n",
    "        # Generar visualizaciones\n",
    "        plot_residuals(y, y_pred, f'{nombre} - {sufijo}', version)\n",
    "        plot_regression(y, y_pred, f'{nombre} - {sufijo}', version)\n",
    "        \n",
    "        return {\n",
    "            'nombre': nombre,\n",
    "            'rmse_cv': rmse_cv,\n",
    "            'mse_cv': mse_cv,\n",
    "            'mae_cv': mae_cv,\n",
    "            'r2_cv': r2_cv,\n",
    "            'rmse_train': rmse_train,\n",
    "            'mse_train': mse_train,\n",
    "            'mae_train': mae_train,\n",
    "            'r2_train': r2_train,\n",
    "            'tiempo': tiempo_ejecucion,\n",
    "            'hiperparams': hiperparams\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        errors_logger.error(f\"Error al evaluar el modelo {nombre}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def evaluar_modelos(X, y, X_transformado, y_transformado, version=\"1.0\"):\n",
    "    modelos = {\n",
    "        'LinearRegression': LinearRegression(n_jobs=-1),\n",
    "        'Ridge': Ridge(random_state=42),\n",
    "        'Lasso': Lasso(random_state=42),\n",
    "        'ElasticNet': ElasticNet(random_state=42)\n",
    "    }\n",
    "    \n",
    "    resultados = []\n",
    "    \n",
    "    for nombre, modelo in modelos.items():\n",
    "        resultado = evaluar_modelo(nombre, modelo, X, y, version=version)\n",
    "        if resultado:\n",
    "            resultados.append(resultado)\n",
    "        \n",
    "        resultado_transformado = evaluar_modelo(nombre, modelo, X_transformado, y_transformado, transformado=True, version=version)\n",
    "        if resultado_transformado:\n",
    "            resultados.append(resultado_transformado)\n",
    "    \n",
    "    return resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados = evaluar_modelos(X, y, transformed_data, y, version=\"1.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1460 entries, 0 to 1459\n",
      "Data columns (total 47 columns):\n",
      " #   Column                        Non-Null Count  Dtype   \n",
      "---  ------                        --------------  -----   \n",
      " 0   OverallQual                   1460 non-null   category\n",
      " 1   GarageArea                    1460 non-null   float64 \n",
      " 2   TotalBsmtSF                   1460 non-null   float64 \n",
      " 3   LotFrontage                   1460 non-null   float64 \n",
      " 4   BedroomAbvGr                  1460 non-null   category\n",
      " 5   YearBuilt                     1460 non-null   float64 \n",
      " 6   YearRemodAdd                  1460 non-null   float64 \n",
      " 7   OverallCond                   1460 non-null   category\n",
      " 8   MasVnrArea                    1460 non-null   float64 \n",
      " 9   TotRmsAbvGrd                  1460 non-null   category\n",
      " 10  GarageYrBlt                   1460 non-null   float64 \n",
      " 11  1stFlrSF                      1460 non-null   float64 \n",
      " 12  YrSold                        1460 non-null   int64   \n",
      " 13  MoSold                        1460 non-null   float64 \n",
      " 14  GrLivArea                     1460 non-null   float64 \n",
      " 15  BuiltUntilSold                1460 non-null   float64 \n",
      " 16  BuiltUntilRemod               1460 non-null   float64 \n",
      " 17  HasBeenRemodeled              1460 non-null   bool    \n",
      " 18  BsmtHalfBath_1                1460 non-null   bool    \n",
      " 19  BsmtHalfBath_2                1460 non-null   bool    \n",
      " 20  GarageCars_1                  1460 non-null   bool    \n",
      " 21  GarageCars_2                  1460 non-null   bool    \n",
      " 22  GarageCars_3                  1460 non-null   bool    \n",
      " 23  GarageCars_4                  1460 non-null   bool    \n",
      " 24  HalfBath_1                    1460 non-null   bool    \n",
      " 25  HalfBath_2                    1460 non-null   bool    \n",
      " 26  Fireplaces_1                  1460 non-null   bool    \n",
      " 27  Fireplaces_2                  1460 non-null   bool    \n",
      " 28  Fireplaces_3                  1460 non-null   bool    \n",
      " 29  FullBath_1                    1460 non-null   bool    \n",
      " 30  FullBath_2                    1460 non-null   bool    \n",
      " 31  FullBath_3                    1460 non-null   bool    \n",
      " 32  BsmtFullBath_1                1460 non-null   bool    \n",
      " 33  BsmtFullBath_2                1460 non-null   bool    \n",
      " 34  BsmtFullBath_3                1460 non-null   bool    \n",
      " 35  GarageArea_power              1460 non-null   float64 \n",
      " 36  TotalBsmtSF_quantile          1460 non-null   float64 \n",
      " 37  LotFrontage_quantile          1460 non-null   float64 \n",
      " 38  YearBuilt_robustscaler        1460 non-null   float64 \n",
      " 39  YearRemodAdd_robustscaler     1460 non-null   float64 \n",
      " 40  MasVnrArea_quantile           1460 non-null   float64 \n",
      " 41  GarageYrBlt_robustscaler      1460 non-null   float64 \n",
      " 42  1stFlrSF_quantile             1460 non-null   float64 \n",
      " 43  MoSold_quantile               1460 non-null   float64 \n",
      " 44  GrLivArea_quantile            1460 non-null   float64 \n",
      " 45  BuiltUntilSold_robustscaler   1460 non-null   float64 \n",
      " 46  BuiltUntilRemod_robustscaler  1460 non-null   float64 \n",
      "dtypes: bool(18), category(4), float64(24), int64(1)\n",
      "memory usage: 318.1 KB\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "transformed_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function flush_figures at 0x000001D75B465790> (for post_execute):\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\envs\\david_pcc\\lib\\site-packages\\matplotlib_inline\\backend_inline.py:126\u001b[0m, in \u001b[0;36mflush_figures\u001b[1;34m()\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m InlineBackend\u001b[38;5;241m.\u001b[39minstance()\u001b[38;5;241m.\u001b[39mclose_figures:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;66;03m# ignore the tracking, just draw and close all figures\u001b[39;00m\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 126\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    128\u001b[0m         \u001b[38;5;66;03m# safely show traceback if in IPython, else raise\u001b[39;00m\n\u001b[0;32m    129\u001b[0m         ip \u001b[38;5;241m=\u001b[39m get_ipython()\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\envs\\david_pcc\\lib\\site-packages\\matplotlib_inline\\backend_inline.py:90\u001b[0m, in \u001b[0;36mshow\u001b[1;34m(close, block)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m figure_manager \u001b[38;5;129;01min\u001b[39;00m Gcf\u001b[38;5;241m.\u001b[39mget_all_fig_managers():\n\u001b[1;32m---> 90\u001b[0m         \u001b[43mdisplay\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfigure_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanvas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fetch_figure_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfigure_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanvas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     95\u001b[0m     show\u001b[38;5;241m.\u001b[39m_to_draw \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\envs\\david_pcc\\lib\\site-packages\\IPython\\core\\display_functions.py:298\u001b[0m, in \u001b[0;36mdisplay\u001b[1;34m(include, exclude, metadata, transient, display_id, raw, clear, *objs, **kwargs)\u001b[0m\n\u001b[0;32m    296\u001b[0m     publish_display_data(data\u001b[38;5;241m=\u001b[39mobj, metadata\u001b[38;5;241m=\u001b[39mmetadata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 298\u001b[0m     format_dict, md_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m format_dict:\n\u001b[0;32m    300\u001b[0m         \u001b[38;5;66;03m# nothing to display (e.g. _ipython_display_ took over)\u001b[39;00m\n\u001b[0;32m    301\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\envs\\david_pcc\\lib\\site-packages\\IPython\\core\\formatters.py:179\u001b[0m, in \u001b[0;36mDisplayFormatter.format\u001b[1;34m(self, obj, include, exclude)\u001b[0m\n\u001b[0;32m    177\u001b[0m md \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mformatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m    181\u001b[0m     \u001b[38;5;66;03m# FIXME: log the exception\u001b[39;00m\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\envs\\david_pcc\\lib\\site-packages\\decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[0;32m    231\u001b[0m     args, kw \u001b[38;5;241m=\u001b[39m fix(args, kw, sig)\n\u001b[1;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcaller\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mextras\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\envs\\david_pcc\\lib\\site-packages\\IPython\\core\\formatters.py:223\u001b[0m, in \u001b[0;36mcatch_format_error\u001b[1;34m(method, self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"show traceback on failed format call\"\"\"\u001b[39;00m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 223\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:\n\u001b[0;32m    225\u001b[0m     \u001b[38;5;66;03m# don't warn on NotImplementedErrors\u001b[39;00m\n\u001b[0;32m    226\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_return(\u001b[38;5;28;01mNone\u001b[39;00m, args[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\envs\\david_pcc\\lib\\site-packages\\IPython\\core\\formatters.py:340\u001b[0m, in \u001b[0;36mBaseFormatter.__call__\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    338\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 340\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprinter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;66;03m# Finally look for special method names\u001b[39;00m\n\u001b[0;32m    342\u001b[0m method \u001b[38;5;241m=\u001b[39m get_real_method(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_method)\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\envs\\david_pcc\\lib\\site-packages\\IPython\\core\\pylabtools.py:152\u001b[0m, in \u001b[0;36mprint_figure\u001b[1;34m(fig, fmt, bbox_inches, base64, **kwargs)\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend_bases\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FigureCanvasBase\n\u001b[0;32m    150\u001b[0m     FigureCanvasBase(fig)\n\u001b[1;32m--> 152\u001b[0m \u001b[43mfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanvas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprint_figure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbytes_io\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    153\u001b[0m data \u001b[38;5;241m=\u001b[39m bytes_io\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fmt \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msvg\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\envs\\david_pcc\\lib\\site-packages\\matplotlib\\backend_bases.py:2342\u001b[0m, in \u001b[0;36mFigureCanvasBase.print_figure\u001b[1;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[0;32m   2336\u001b[0m     renderer \u001b[38;5;241m=\u001b[39m _get_renderer(\n\u001b[0;32m   2337\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure,\n\u001b[0;32m   2338\u001b[0m         functools\u001b[38;5;241m.\u001b[39mpartial(\n\u001b[0;32m   2339\u001b[0m             print_method, orientation\u001b[38;5;241m=\u001b[39morientation)\n\u001b[0;32m   2340\u001b[0m     )\n\u001b[0;32m   2341\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(renderer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_draw_disabled\u001b[39m\u001b[38;5;124m\"\u001b[39m, nullcontext)():\n\u001b[1;32m-> 2342\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2344\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bbox_inches:\n\u001b[0;32m   2345\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m bbox_inches \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtight\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\envs\\david_pcc\\lib\\site-packages\\matplotlib\\artist.py:95\u001b[0m, in \u001b[0;36m_finalize_rasterization.<locals>.draw_wrapper\u001b[1;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(draw)\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdraw_wrapper\u001b[39m(artist, renderer, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 95\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m renderer\u001b[38;5;241m.\u001b[39m_rasterizing:\n\u001b[0;32m     97\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstop_rasterizing()\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\envs\\david_pcc\\lib\\site-packages\\matplotlib\\artist.py:72\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[1;34m(artist, renderer)\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     70\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstart_filter()\n\u001b[1;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\envs\\david_pcc\\lib\\site-packages\\matplotlib\\figure.py:3175\u001b[0m, in \u001b[0;36mFigure.draw\u001b[1;34m(self, renderer)\u001b[0m\n\u001b[0;32m   3172\u001b[0m         \u001b[38;5;66;03m# ValueError can occur when resizing a window.\u001b[39;00m\n\u001b[0;32m   3174\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch\u001b[38;5;241m.\u001b[39mdraw(renderer)\n\u001b[1;32m-> 3175\u001b[0m \u001b[43mmimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_draw_list_compositing_images\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3176\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43martists\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuppressComposite\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3178\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sfig \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubfigs:\n\u001b[0;32m   3179\u001b[0m     sfig\u001b[38;5;241m.\u001b[39mdraw(renderer)\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\envs\\david_pcc\\lib\\site-packages\\matplotlib\\image.py:131\u001b[0m, in \u001b[0;36m_draw_list_compositing_images\u001b[1;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m not_composite \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_images:\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m artists:\n\u001b[1;32m--> 131\u001b[0m         \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;66;03m# Composite any adjacent images together\u001b[39;00m\n\u001b[0;32m    134\u001b[0m     image_group \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\envs\\david_pcc\\lib\\site-packages\\matplotlib\\artist.py:72\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[1;34m(artist, renderer)\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     70\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstart_filter()\n\u001b[1;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\envs\\david_pcc\\lib\\site-packages\\matplotlib\\axes\\_base.py:3064\u001b[0m, in \u001b[0;36m_AxesBase.draw\u001b[1;34m(self, renderer)\u001b[0m\n\u001b[0;32m   3061\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m artists_rasterized:\n\u001b[0;32m   3062\u001b[0m     _draw_rasterized(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure, artists_rasterized, renderer)\n\u001b[1;32m-> 3064\u001b[0m \u001b[43mmimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_draw_list_compositing_images\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3065\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43martists\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuppressComposite\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3067\u001b[0m renderer\u001b[38;5;241m.\u001b[39mclose_group(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maxes\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   3068\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\envs\\david_pcc\\lib\\site-packages\\matplotlib\\image.py:131\u001b[0m, in \u001b[0;36m_draw_list_compositing_images\u001b[1;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m not_composite \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_images:\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m artists:\n\u001b[1;32m--> 131\u001b[0m         \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;66;03m# Composite any adjacent images together\u001b[39;00m\n\u001b[0;32m    134\u001b[0m     image_group \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\envs\\david_pcc\\lib\\site-packages\\matplotlib\\artist.py:72\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[1;34m(artist, renderer)\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     70\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstart_filter()\n\u001b[1;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\envs\\david_pcc\\lib\\site-packages\\matplotlib\\patches.py:590\u001b[0m, in \u001b[0;36mPatch.draw\u001b[1;34m(self, renderer)\u001b[0m\n\u001b[0;32m    588\u001b[0m transform \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_transform()\n\u001b[0;32m    589\u001b[0m tpath \u001b[38;5;241m=\u001b[39m transform\u001b[38;5;241m.\u001b[39mtransform_path_non_affine(path)\n\u001b[1;32m--> 590\u001b[0m affine \u001b[38;5;241m=\u001b[39m \u001b[43mtransform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_affine\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_draw_paths_with_artist_properties(\n\u001b[0;32m    592\u001b[0m     renderer,\n\u001b[0;32m    593\u001b[0m     [(tpath, affine,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    596\u001b[0m       \u001b[38;5;66;03m# transparent, but do if it is None.\u001b[39;00m\n\u001b[0;32m    597\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_facecolor \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_facecolor[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)])\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\envs\\david_pcc\\lib\\site-packages\\matplotlib\\transforms.py:2450\u001b[0m, in \u001b[0;36mCompositeGenericTransform.get_affine\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2447\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_b\u001b[38;5;241m.\u001b[39mget_affine()\n\u001b[0;32m   2448\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Affine2D(np\u001b[38;5;241m.\u001b[39mdot(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_b\u001b[38;5;241m.\u001b[39mget_affine()\u001b[38;5;241m.\u001b[39mget_matrix(),\n\u001b[1;32m-> 2450\u001b[0m                            \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_a\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_affine\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mget_matrix()))\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\envs\\david_pcc\\lib\\site-packages\\matplotlib\\transforms.py:2449\u001b[0m, in \u001b[0;36mCompositeGenericTransform.get_affine\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2447\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_b\u001b[38;5;241m.\u001b[39mget_affine()\n\u001b[0;32m   2448\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mAffine2D\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_b\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_affine\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2450\u001b[0m \u001b[43m                           \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_a\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_affine\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\envs\\david_pcc\\lib\\site-packages\\matplotlib\\transforms.py:1903\u001b[0m, in \u001b[0;36mAffine2D.__init__\u001b[1;34m(self, matrix, **kwargs)\u001b[0m\n\u001b[0;32m   1900\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m matrix \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1901\u001b[0m     \u001b[38;5;66;03m# A bit faster than np.identity(3).\u001b[39;00m\n\u001b[0;32m   1902\u001b[0m     matrix \u001b[38;5;241m=\u001b[39m IdentityTransform\u001b[38;5;241m.\u001b[39m_mtx\n\u001b[1;32m-> 1903\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mtx \u001b[38;5;241m=\u001b[39m \u001b[43mmatrix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1904\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_invalid \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Generación de nuevas características y evaluación de modelos\n",
    "# Función para añadir nuevas características\n",
    "def añadir_nuevas_caracteristicas(df):\n",
    "    try:\n",
    "        nuevas_columnas = {}\n",
    "        \n",
    "        for col in df.select_dtypes(include=['int64', 'float64']).columns:\n",
    "            nuevas_columnas[f'{col}_squared'] = np.square(df[col])\n",
    "            \n",
    "            if (df[col] >= 0).all():\n",
    "                nuevas_columnas[f'{col}_log'] = np.log1p(df[col])\n",
    "                nuevas_columnas[f'{col}_sqrt'] = np.sqrt(df[col])\n",
    "            \n",
    "            # Añadir el producto de columnas\n",
    "            for col2 in df.select_dtypes(include=['int64', 'float64']).columns:\n",
    "                if col != col2:\n",
    "                    nuevas_columnas[f'{col}_x_{col2}'] = df[col] * df[col2]\n",
    "                    \n",
    "        df_new = pd.concat([df, pd.DataFrame(nuevas_columnas)], axis=1)\n",
    "        app_logger.info(f\"Se han añadido {len(nuevas_columnas)} nuevas características.\")\n",
    "        return df_new\n",
    "    except Exception as e:\n",
    "        app_logger.error(f\"Error al añadir nuevas características: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "try:\n",
    "    # Aplicar las nuevas características a ambos datasets\n",
    "    X_new = añadir_nuevas_caracteristicas(X.copy())\n",
    "    transformed_data_new = añadir_nuevas_caracteristicas(transformed_data.copy())\n",
    "\n",
    "    # Dividir los nuevos datasets\n",
    "    X_train_new, X_test_new, y_train_new, y_test_new = train_test_split(X_new, y, test_size=0.2, random_state=42)\n",
    "    X_train_transformed_new, X_test_transformed_new, y_train_transformed_new, y_test_transformed_new = train_test_split(transformed_data_new, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Evaluar los modelos con los nuevos datasets\n",
    "    resultados_nuevos = evaluar_modelos(X_train_new, y_train_new, X_train_transformed_new, y_train_transformed_new, version=\"2.0 - Polynomial Features\")\n",
    "\n",
    "except Exception as e:\n",
    "    app_logger.error(f\"Error en el proceso principal: {str(e)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Possible Features\n",
    "- Based on ``YearRemodAdd`` and ``YearBuilt`` as the dataset description stated, if they are equal that means that the house is not remodeled and if its different means that it has been remodeled, we can add a binary feature indicating this.\n",
    "- Reduce ``YearBuilt`` and ``YrSold`` to ``TimeToSell``.\n",
    "- Convert ``YearRemodAdd`` to ``TimeUntilRemod`` that means the time since it was built until it was remod, and ``RemodUntilSale`` that is the time since it was remod until it was sold.\n",
    "- Porch and Deck Areas: Create a total porch area feature and a binary indicator for houses with porches.\n",
    "- Proximity and Neighborhood Effects: Group neighborhoods into clusters based on median house prices to capture locality effects.\n",
    "- GeoCode neighborhoods.\n",
    "- Total Square Footage: Combine all square footage features (``1stFlrSF``, ``2ndFlrSF``, ``TotalBsmtSF``, etc.) into a single feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inconsistencies\n",
    "- In some cases ``GarageYrBlt`` (year that the garage was built) was previous to ``YearBuilt`` which is not logical. We can modify this cases and transform those values to the year that the house was built assumming this criterion. This variable is related mostly with the built year and the Remodelation year that we can discard it as it only adds complexity with no info to the dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finrl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
