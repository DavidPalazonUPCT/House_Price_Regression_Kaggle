{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar las bibliotecas necesarias\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.exceptions import NotFittedError\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Configuracion de seaborn\n",
    "sns.set_theme(style='whitegrid', context='paper', palette='muted')\n",
    "\n",
    "# Agregar el directorio de scripts al path\n",
    "sys.path.append(os.path.join(os.path.dirname(os.getcwd()), 'scripts'))\n",
    "\n",
    "# Importar el logger personalizado\n",
    "from logger import CustomLogger\n",
    "\n",
    "# Crear logger\n",
    "logger = CustomLogger(developer='David')\n",
    "app_logger = logger.get_logger('app')\n",
    "errors_logger = logger.get_logger('errors')\n",
    "visualizations_logger = logger.get_logger('visualizations')\n",
    "optimization_logger = logger.get_logger('optimization')\n",
    "results_logger = logger.get_logger('results')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
       "1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
       "2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
       "3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
       "4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
       "\n",
       "  LandContour Utilities  ... PoolArea PoolQC Fence MiscFeature MiscVal MoSold  \\\n",
       "0         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
       "1         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      5   \n",
       "2         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      9   \n",
       "3         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
       "4         Lvl    AllPub  ...        0    NaN   NaN         NaN       0     12   \n",
       "\n",
       "  YrSold  SaleType  SaleCondition  SalePrice  \n",
       "0   2008        WD         Normal     208500  \n",
       "1   2007        WD         Normal     181500  \n",
       "2   2008        WD         Normal     223500  \n",
       "3   2006        WD        Abnorml     140000  \n",
       "4   2008        WD         Normal     250000  \n",
       "\n",
       "[5 rows x 81 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    # Cargar los datos de entrenamiento\n",
    "    train_data = pd.read_csv('../data/train.csv')\n",
    "    app_logger.info(\"Conjunto de datos de entrenamiento cargado exitosamente.\")\n",
    "except FileNotFoundError:\n",
    "    errors_logger.error(\"No se pudo encontrar el archivo de datos de entrenamiento.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    errors_logger.error(f\"Error al cargar los datos de entrenamiento: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "# Cargamos test.csv\n",
    "try:\n",
    "    test_data = pd.read_csv('../data/test.csv')\n",
    "    app_logger.info(\"Conjunto de datos de test cargado exitosamente.\")\n",
    "except FileNotFoundError:\n",
    "    errors_logger.error(\"No se pudo encontrar el archivo de datos de test.\")\n",
    "    raise\n",
    "\n",
    "# Mostrar las primeras filas del conjunto de datos\n",
    "train_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores faltantes por columna:\n",
      "              Total  Porcentaje\n",
      "PoolQC         1453   99.520548\n",
      "MiscFeature    1406   96.301370\n",
      "Alley          1369   93.767123\n",
      "Fence          1179   80.753425\n",
      "MasVnrType      872   59.726027\n",
      "FireplaceQu     690   47.260274\n",
      "LotFrontage     259   17.739726\n",
      "GarageType       81    5.547945\n",
      "GarageYrBlt      81    5.547945\n",
      "GarageFinish     81    5.547945\n",
      "GarageQual       81    5.547945\n",
      "GarageCond       81    5.547945\n",
      "BsmtFinType2     38    2.602740\n",
      "BsmtExposure     38    2.602740\n",
      "BsmtFinType1     37    2.534247\n",
      "BsmtCond         37    2.534247\n",
      "BsmtQual         37    2.534247\n",
      "MasVnrArea        8    0.547945\n",
      "Electrical        1    0.068493\n",
      "\n",
      "NÃºmero de filas duplicadas: 0\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Valores faltantes\n",
    "    print(\"Valores faltantes por columna:\")\n",
    "    valores_faltantes = train_data.isnull().sum()\n",
    "    porcentaje_faltantes = 100 * train_data.isnull().sum() / len(train_data)\n",
    "    tabla_faltantes = pd.concat([valores_faltantes, porcentaje_faltantes], axis=1, keys=['Total', 'Porcentaje'])\n",
    "    print(tabla_faltantes[tabla_faltantes['Total'] > 0].sort_values('Total', ascending=False))\n",
    "\n",
    "    # Filas duplicadas\n",
    "    filas_duplicadas = train_data.duplicated().sum()\n",
    "    print(f\"\\nNÃºmero de filas duplicadas: {filas_duplicadas}\")\n",
    "\n",
    "    # Registrar en el log\n",
    "    if valores_faltantes.sum() > 0:\n",
    "        app_logger.info(f\"Se encontraron {valores_faltantes.sum()} valores faltantes en total.\")\n",
    "    else:\n",
    "        app_logger.info(\"No se encontraron valores faltantes en el conjunto de datos.\")\n",
    "\n",
    "    if filas_duplicadas > 0:\n",
    "        app_logger.warning(f\"Se encontraron {filas_duplicadas} filas duplicadas en el conjunto de datos.\")\n",
    "    else:\n",
    "        app_logger.info(\"No se encontraron filas duplicadas en el conjunto de datos.\")\n",
    "\n",
    "except Exception as e:\n",
    "    errors_logger.exception(\"Error al analizar valores faltantes y filas duplicadas:\")\n",
    "    app_logger.error(\"Se produjo un error al analizar los datos. Consulte el registro de errores para mÃ¡s detalles.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Columns on Train\n",
    "- Pool Quality Col has 99,52% missing values, but we still have Pool Area wich i think is more important than PoolQc\n",
    "- MiscFeature Col has 96.30% missing values. We also have MiscVal that represents the value of Misc Feature.\n",
    "    - Each Misc Feature has a unique value or has multiple values?? If each MiscFeature has only one unique value (univoque relationship) we can drop MiscFeature and leave Misc Val because we wont lose info, otherwise, drop both. Even if MiscVal has no NaNs, using it on its own may not be the best for the model to gather relationships.\n",
    "- Alley has 93% missing values, no other col is related to this one so we can directly drop it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verificar si la relaciÃ³n es unÃ­voca entre MiscFeature y MiscVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RelaciÃ³n entre MiscFeature y MiscVal:\n",
      "  MiscFeature  Valores_Unicos_MiscVal\n",
      "0        Gar2                       2\n",
      "1        Othr                       2\n",
      "2        Shed                      18\n",
      "3        TenC                       1\n",
      "\n",
      "La relaciÃ³n entre MiscFeature y MiscVal no es unÃ­voca.\n",
      "\n",
      "Ejemplos de MiscFeature con mÃºltiples valores de MiscVal:\n",
      "MiscFeature: Gar2\n",
      "Valores de MiscVal: [15500  8300]\n",
      "\n",
      "MiscFeature: Othr\n",
      "Valores de MiscVal: [3500    0]\n",
      "\n",
      "MiscFeature: Shed\n",
      "Valores de MiscVal: [ 700  350  500  400  480  450 1200  800 2000  600 1300   54  620  560\n",
      " 1400    0 1150 2500]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Crear un DataFrame con MiscFeature y MiscVal\n",
    "    misc_df = train_data[['MiscFeature', 'MiscVal']]\n",
    "    app_logger.info(\"DataFrame misc_df creado exitosamente\")\n",
    "except KeyError as e:\n",
    "    errors_logger.exception(\"Error al crear misc_df: Columna no encontrada\")\n",
    "    print(f\"Error: No se encontrÃ³ la columna {str(e)} en el conjunto de datos\")\n",
    "    raise\n",
    "\n",
    "try:\n",
    "    # Agrupar por MiscFeature y contar los valores Ãºnicos de MiscVal\n",
    "    relacion_misc = misc_df.groupby('MiscFeature')['MiscVal'].nunique().reset_index()\n",
    "    relacion_misc.columns = ['MiscFeature', 'Valores_Unicos_MiscVal']\n",
    "    app_logger.info(\"RelaciÃ³n entre MiscFeature y MiscVal generada correctamente\")\n",
    "except Exception as e:\n",
    "    errors_logger.exception(\"Error al generar la relaciÃ³n entre MiscFeature y MiscVal\")\n",
    "    print(\"Se produjo un error al analizar la relaciÃ³n. Consulte el registro de errores para mÃ¡s detalles.\")\n",
    "    raise\n",
    "\n",
    "print(\"RelaciÃ³n entre MiscFeature y MiscVal:\")\n",
    "print(relacion_misc)\n",
    "\n",
    "try:\n",
    "    # Verificar si cada MiscFeature tiene un Ãºnico valor de MiscVal\n",
    "    es_univoca = (relacion_misc['Valores_Unicos_MiscVal'] == 1).all()\n",
    "    \n",
    "    if es_univoca:\n",
    "        print(\"\\nLa relaciÃ³n entre MiscFeature y MiscVal es unÃ­voca.\")\n",
    "        app_logger.info(\"La relaciÃ³n entre MiscFeature y MiscVal es unÃ­voca. Se puede considerar dejar una columna.\")\n",
    "    else:\n",
    "        print(\"\\nLa relaciÃ³n entre MiscFeature y MiscVal no es unÃ­voca.\")\n",
    "        app_logger.info(\"La relaciÃ³n entre MiscFeature y MiscVal no es unÃ­voca. Se recomienda eliminar ambas columnas.\")\n",
    "except Exception as e:\n",
    "    errors_logger.exception(\"Error al verificar la univocidad de la relaciÃ³n\")\n",
    "    print(\"Se produjo un error al verificar la relaciÃ³n. Consulte el registro de errores para mÃ¡s detalles.\")\n",
    "\n",
    "if not es_univoca:\n",
    "    try:\n",
    "        print(\"\\nEjemplos de MiscFeature con mÃºltiples valores de MiscVal:\")\n",
    "        ejemplos_multiples = relacion_misc[relacion_misc['Valores_Unicos_MiscVal'] > 1]\n",
    "        for _, row in ejemplos_multiples.iterrows():\n",
    "            feature = row['MiscFeature']\n",
    "            valores = misc_df[misc_df['MiscFeature'] == feature]['MiscVal'].unique()\n",
    "            print(f\"MiscFeature: {feature}\")\n",
    "            print(f\"Valores de MiscVal: {valores}\\n\")\n",
    "            app_logger.info(f\"MiscFeature '{feature}' tiene mÃºltiples valores: {valores}\")\n",
    "    except Exception as e:\n",
    "        errors_logger.exception(\"Error al mostrar ejemplos de MiscFeature con mÃºltiples valores\")\n",
    "        print(\"Se produjo un error al mostrar los ejemplos. Consulte el registro de errores para mÃ¡s detalles.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dado que MiscVal y MiscFeature no son univocamente relacionados, se eliminan ambas columnas puesto que estan relacionadas y no se tienen datos de una y la otra son casi todo 0\n",
    "train_data = train_data.drop(columns=['MiscFeature', 'MiscVal'])\n",
    "app_logger.info(\"Se eliminaron las columnas MiscFeature y MiscVal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas eliminadas: Alley, PoolQC\n",
      "\n",
      "Forma del conjunto de datos original: (1460, 79)\n",
      "Forma del conjunto de datos filtrado: (1460, 77)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Filtrar columnas con mÃ¡s del 90% de valores faltantes\n",
    "    umbral_faltantes = 0.9\n",
    "    columnas_a_eliminar = tabla_faltantes[tabla_faltantes['Porcentaje'] > 90].index\n",
    "    train_data_filtrado = train_data.drop(columns=columnas_a_eliminar.drop(['MiscFeature']))\n",
    "\n",
    "    # Registrar en el log\n",
    "    if len(columnas_a_eliminar) > 0:\n",
    "        app_logger.info(f\"Se eliminaron {len(columnas_a_eliminar)} columnas con mÃ¡s del 90% de valores faltantes: {', '.join(columnas_a_eliminar)}\")\n",
    "        print(f\"Columnas eliminadas: {', '.join(columnas_a_eliminar.drop(['MiscFeature']))}\")\n",
    "    else:\n",
    "        app_logger.info(\"No se encontraron columnas con mÃ¡s del 90% de valores faltantes.\")\n",
    "        print(\"No se encontraron columnas con mÃ¡s del 90% de valores faltantes.\")\n",
    "\n",
    "    # Mostrar la forma del nuevo conjunto de datos\n",
    "    print(f\"\\nForma del conjunto de datos original: {train_data.shape}\")\n",
    "    print(f\"Forma del conjunto de datos filtrado: {train_data_filtrado.shape}\")\n",
    "\n",
    "except KeyError as e:\n",
    "    errors_logger.exception(f\"Error al acceder a una columna: {str(e)}\")\n",
    "    app_logger.error(\"Se produjo un error al filtrar las columnas. Verifique los nombres de las columnas.\")\n",
    "except ValueError as e:\n",
    "    errors_logger.exception(f\"Error en el cÃ¡lculo de porcentajes: {str(e)}\")\n",
    "    app_logger.error(\"Se produjo un error al calcular los porcentajes de valores faltantes.\")\n",
    "except Exception as e:\n",
    "    errors_logger.exception(f\"Error inesperado: {str(e)}\")\n",
    "    app_logger.error(\"Se produjo un error inesperado durante el procesamiento de los datos.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se ha generado un anÃ¡lisis interactivo con dtale.\n",
      "Por favor, acceda a la siguiente URL para explorar los datos: http://PortatilDavid:40000\n"
     ]
    }
   ],
   "source": [
    "# Importar dtale\n",
    "import dtale\n",
    "\n",
    "# Crear una instancia de dtale con los datos filtrados\n",
    "d = dtale.show(train_data_filtrado)\n",
    "\n",
    "# Mostrar el enlace para acceder a la interfaz de dtale\n",
    "print(\"Se ha generado un anÃ¡lisis interactivo con dtale.\")\n",
    "print(f\"Por favor, acceda a la siguiente URL para explorar los datos: {d._url}\")\n",
    "\n",
    "# Registrar en el log\n",
    "app_logger.info(\"Se ha generado un anÃ¡lisis interactivo utilizando dtale.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis D-Tale\n",
    "- We found important predictive power with the following features ``FullBath_quantile, FullBath, GrLiveArea, GrLiveArea_power, GrLiveArea_quantile,GrLiveArea_robust, GarageCars, OverallQual``\n",
    "- Maybe add a feature that stratifies if the house has garage or not (Never Mind ``GarageCond`` and ``GarageQual`` already do).\n",
    "- 81 houses have 0 car capacity in garage and 0 sqft but not NA specified on ``GarageCond``, ``GarageQual``, ``GarageType`` and ``GarageFinish``, drop rows with 0 or impute them. If a house has 0 for ``GarageCars`` and ``GarageArea``, the ``GarageType``, ``GarageCond``, ``GarageQual``, and ``GarageFinish`` should be NA?.\n",
    "- ``GarageQual`` and ``GarageCond`` are duplicates.\n",
    "- The features ``Fireplaces`` and ``FireplaceQu`` are both NaN or 0 for the same row, so imputation should be done simultaneosly maintaining their possible relationship, imputing the mean and most frecuent values independently could potentially introduce noise as the pairing could possibly be random.\n",
    "\n",
    "- Most of the numerical variables where Non-normal, with log relationship with sales price (target).\n",
    "\n",
    "    ### Low Variance Features\n",
    "    - Dtale showed the following features having low variance:\n",
    "        - ``BsmtFinSF2`` : 88.56% are 0s, related to this feature we have ``BsmtFinType2`` with 88.33% of rows with value 'Unf' which means 'unfinished '. This could mean that as it is unfinshed the sqft on ``BsmtFinSF2`` are not taken into account so the value will be 0?. Coincidence percentaje between ``BsmtFinType2 == 'Unf' and BsmtFinSF2 == 0``: 97.14%\n",
    "        - ``LowQualFinSF``: 98.22% are 0s. This feature represents 'Low quality finished square feet (all floors)', i dont really know if we could impute this values as the meaning of the variable is not deterministic or easy to define because it represent the sqft of LowQuality finished sqft of all floors, but there isnt a feature defining wich houses are LowQuality or finished. \n",
    "            - Maybe we could create a feature called ``Total_sqft`` and/or Mean between all floors per house, etc... \n",
    "        - ``KitchenAbvGr``: From possible values {0,1,2,3} 95.34% are 1s. These feature represents the number of kitchens above grade. There is also the ``TotRmsAbvGrd`` feature measuring the total rooms above grade and the feature ``KitchenQual`` wich indicates the quality of the kitchens. Due to this i think we could drop this feature and leave the other two, as the model will probably discover the relationships.\n",
    "        - ``PoolArea``: 99.52% values are 0s. The related feature was ``PoolQC`` representing the pool quality but it had 99.79% of NaNs values so i think we can drop both.\n",
    "        - ``EnclosedPorch``: 85.75% of 0s.\n",
    "        - ``3SsnPorch``: 98.36% of 0s.\n",
    "        - ``ScreenPorch``: 92.05% of 0s.\n",
    "        - Out of this 3 features realted with sqft of different types of Porch the one that has the most sens on maintaining on the dataset is ``EnclosedPorch`` that is de opposite to ``OpenPorch`` (that measures the open porch sqft) and its not that much full of 0s as the other two. This could be okay with te model as it can learn that if we have some value on ``EnclosedPorch``, the house has an enclosed porch, and the same for ``OpenPorch``. The other 2 features are practically all 0s and not much value can be obtained. I think the 2 important features are the ones i mentioned, we could do some new features to encode this info of having 1 porch or another or both.\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Low_Variance_Features = [\n",
    "    \"BsmtFinSF2\",\n",
    "    \"LowQualFinSF\",\n",
    "    \"KitchenAbvGr\",\n",
    "    \"PoolArea\",\n",
    "    \"EnclosedPorch\",\n",
    "    \"3SsnPorch\",\n",
    "    \"ScreenPorch\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NÃºmero de casas remodeladas: 696\n",
      "NÃºmero de casas no remodeladas: 764\n",
      "Porcentaje de casas remodeladas: 47.67%\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Comparar YearRemodAdd con YearBuilt para identificar casas remodeladas\n",
    "    casas_remodeladas = train_data_filtrado[train_data_filtrado['YearRemodAdd'] != train_data_filtrado['YearBuilt']]\n",
    "    casas_no_remodeladas = train_data_filtrado[train_data_filtrado['YearRemodAdd'] == train_data_filtrado['YearBuilt']]\n",
    "\n",
    "    # Calcular el nÃºmero de casas remodeladas y no remodeladas\n",
    "    num_remodeladas = len(casas_remodeladas)\n",
    "    num_no_remodeladas = len(casas_no_remodeladas)\n",
    "\n",
    "    # Imprimir los resultados\n",
    "    print(f\"NÃºmero de casas remodeladas: {num_remodeladas}\")\n",
    "    print(f\"NÃºmero de casas no remodeladas: {num_no_remodeladas}\")\n",
    "    print(f\"Porcentaje de casas remodeladas: {(num_remodeladas / len(train_data_filtrado)) * 100:.2f}%\")\n",
    "\n",
    "    # Registrar en el log\n",
    "    app_logger.info(f\"Se identificaron {num_remodeladas} casas remodeladas y {num_no_remodeladas} casas no remodeladas.\")\n",
    "\n",
    "except KeyError as e:\n",
    "    errors_logger.exception(f\"Error al acceder a las columnas: {str(e)}\")\n",
    "    app_logger.error(\"No se pudo completar el anÃ¡lisis de casas remodeladas debido a un error en las columnas.\")\n",
    "except ZeroDivisionError:\n",
    "    errors_logger.exception(\"Error al calcular el porcentaje: DivisiÃ³n por cero.\")\n",
    "    app_logger.error(\"No se pudo calcular el porcentaje de casas remodeladas debido a un error en los datos.\")\n",
    "except Exception as e:\n",
    "    errors_logger.exception(f\"Error inesperado durante el anÃ¡lisis de casas remodeladas: {str(e)}\")\n",
    "    app_logger.error(\"Se produjo un error inesperado durante el anÃ¡lisis de casas remodeladas.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_porcentajes(df):\n",
    "    '''\n",
    "    Calcula el porcentaje de casas que coinciden en el aÃ±o de construcciÃ³n con el aÃ±o de construcciÃ³n del garaje y el aÃ±o de remodelaciÃ³n.\n",
    "    '''\n",
    "    try:\n",
    "        coinciden = df['YearBuilt'] == df['GarageYrBlt']\n",
    "        diferentes = ~coinciden\n",
    "        porcentaje_coinciden = (coinciden.sum() / len(df)) * 100\n",
    "        porcentaje_diferentes = (diferentes.sum() / len(df)) * 100\n",
    "        return porcentaje_coinciden, porcentaje_diferentes\n",
    "    except KeyError as e:\n",
    "        app_logger.error(f\"Error al acceder a las columnas: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        app_logger.error(f\"Error inesperado al calcular porcentajes: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analizar_coincidencias_remod(df, diferentes):\n",
    "    '''\n",
    "    Analiza las casas que coinciden en el aÃ±o de construcciÃ³n con el aÃ±o de construcciÃ³n del garaje y el aÃ±o de remodelaciÃ³n.\n",
    "    '''\n",
    "    try:\n",
    "        coinciden_con_remod = df[diferentes]['GarageYrBlt'] == df[diferentes]['YearRemodAdd']\n",
    "        porcentaje_coinciden_remod = (coinciden_con_remod.sum() / diferentes.sum()) * 100\n",
    "        return porcentaje_coinciden_remod\n",
    "    except KeyError as e:\n",
    "        app_logger.error(f\"Error al acceder a las columnas para anÃ¡lisis de remodelaciÃ³n: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        app_logger.error(f\"Error inesperado en anÃ¡lisis de remodelaciÃ³n: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_diferencias_tiempo(df):\n",
    "    '''\n",
    "    Calcula las diferencias de tiempo entre el aÃ±o de construcciÃ³n del garaje y el aÃ±o de construcciÃ³n y el aÃ±o de remodelaciÃ³n.\n",
    "    '''\n",
    "    try:\n",
    "        df['diff_with_yearbuilt'] = abs(df['GarageYrBlt'] - df['YearBuilt'])\n",
    "        df['diff_with_yearremodadd'] = abs(df['GarageYrBlt'] - df['YearRemodAdd'])\n",
    "        return df\n",
    "    except KeyError as e:\n",
    "        app_logger.error(f\"Error al calcular diferencias de tiempo: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        app_logger.error(f\"Error inesperado al calcular diferencias de tiempo: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analizar_proximidad(df, umbral=7):\n",
    "    '''\n",
    "    Analiza las casas que estÃ¡n dentro del umbral de proximidad con el aÃ±o de construcciÃ³n del garaje y el aÃ±o de construcciÃ³n y el aÃ±o de remodelaciÃ³n.\n",
    "    '''\n",
    "    try:\n",
    "        proxima_a_yearbuilt = df[df['diff_with_yearbuilt'] <= umbral]\n",
    "        proxima_a_yearremodadd = df[df['diff_with_yearremodadd'] <= umbral]\n",
    "        mas_de_umbral = df[(df['diff_with_yearbuilt'] > umbral) & (df['diff_with_yearremodadd'] > umbral)]\n",
    "        return proxima_a_yearbuilt, proxima_a_yearremodadd, mas_de_umbral\n",
    "    except KeyError as e:\n",
    "        app_logger.error(f\"Error al analizar proximidad: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        app_logger.error(f\"Error inesperado al analizar proximidad: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_tiempos_medios_extremos(df):\n",
    "    '''\n",
    "    Calcula los tiempos medios y extremos de la construcciÃ³n y la remodelaciÃ³n.\n",
    "    '''\n",
    "    try:\n",
    "        tiempo_construccion = df['GarageYrBlt'] - df['YearBuilt']\n",
    "        tiempo_remodelacion = df['YearRemodAdd'] - df['GarageYrBlt']\n",
    "        tiempo_hasta_remodelacion = df['YearRemodAdd'] - df['YearBuilt']\n",
    "        \n",
    "        return {\n",
    "            'construccion': (tiempo_construccion.mean(), tiempo_construccion.min(), tiempo_construccion.max()),\n",
    "            'remodelacion': (tiempo_remodelacion.mean(), tiempo_remodelacion.min(), tiempo_remodelacion.max()),\n",
    "            'hasta_remodelacion': (tiempo_hasta_remodelacion.mean(), tiempo_hasta_remodelacion.min(), tiempo_hasta_remodelacion.max())\n",
    "        }\n",
    "    except KeyError as e:\n",
    "        app_logger.error(f\"Error al calcular tiempos medios y extremos: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        app_logger.error(f\"Error inesperado al calcular tiempos medios y extremos: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analizar_datos(train_data_filtrado):\n",
    "    '''\n",
    "    Analiza los datos de construcciÃ³n y remodelaciÃ³n.\n",
    "    '''\n",
    "    try:\n",
    "        porcentaje_coinciden, porcentaje_diferentes = calcular_porcentajes(train_data_filtrado)\n",
    "        diferentes = train_data_filtrado['YearBuilt'] != train_data_filtrado['GarageYrBlt']\n",
    "        porcentaje_coinciden_remod = analizar_coincidencias_remod(train_data_filtrado, diferentes)\n",
    "        \n",
    "        no_coincide_ninguno = (~(train_data_filtrado['YearBuilt'] == train_data_filtrado['GarageYrBlt'])) & \\\n",
    "                              (~(train_data_filtrado['GarageYrBlt'] == train_data_filtrado['YearRemodAdd']))\n",
    "        porcentaje_no_coincide_ninguno = (no_coincide_ninguno.sum() / len(train_data_filtrado)) * 100\n",
    "        \n",
    "        df_no_coincide = calcular_diferencias_tiempo(train_data_filtrado[no_coincide_ninguno])\n",
    "        proxima_a_yearbuilt, proxima_a_yearremodadd, mas_de_7_anos = analizar_proximidad(df_no_coincide)\n",
    "        \n",
    "        porcentaje_proxima_a_yearbuilt = (len(proxima_a_yearbuilt) / len(df_no_coincide)) * 100\n",
    "        porcentaje_proxima_a_yearremodadd = (len(proxima_a_yearremodadd) / len(df_no_coincide)) * 100\n",
    "        porcentaje_mas_de_7_anos = (len(mas_de_7_anos) / len(df_no_coincide)) * 100\n",
    "        porcentaje_mas_7_anos_total = (len(mas_de_7_anos) / len(train_data_filtrado)) * 100\n",
    "        \n",
    "        tiempos = calcular_tiempos_medios_extremos(train_data_filtrado)\n",
    "        \n",
    "        # Imprimir resultados\n",
    "        print(f\"Porcentaje de casas donde YearBuilt coincide con GarageYrBlt: {porcentaje_coinciden:.2f}%\")\n",
    "        print(f\"Del {porcentaje_diferentes:.2f}% que no coincide con YearBuilt, {porcentaje_coinciden_remod:.2f}% coincide con YearRemodAdd\")\n",
    "        print(f\"Porcentaje de casas donde GarageYrBlt no coincide ni con YearBuilt ni con YearRemodAdd: {porcentaje_no_coincide_ninguno:.2f}%\")\n",
    "        print(f\"Porcentaje de casas donde GarageYrBlt estÃ¡ a mÃ¡s de 7 aÃ±os de YearBuilt y YearRemodAdd respecto al total: {porcentaje_mas_7_anos_total:.2f}%\")\n",
    "        print(f\"Porcentaje de casas donde GarageYrBlt no coincide con YearBuilt ni con YearRemodAdd y estÃ¡ a menos de 7 aÃ±os de YearBuilt: {porcentaje_proxima_a_yearbuilt:.2f}%\")\n",
    "        print(f\"Porcentaje de casas donde GarageYrBlt no coincide con YearBuilt ni con YearRemodAdd y estÃ¡ a menos de 7 aÃ±os de YearRemodAdd: {porcentaje_proxima_a_yearremodadd:.2f}%\")\n",
    "        print(f\"Porcentaje de casas donde GarageYrBlt no coincide con YearBuilt ni con YearRemodAdd y estÃ¡ a mÃ¡s de 7 aÃ±os de ambas fechas: {porcentaje_mas_de_7_anos:.2f}%\")\n",
    "        \n",
    "        for key, (media, minimo, maximo) in tiempos.items():\n",
    "            print(f\"Tiempo {key} - Media: {media:.2f}, MÃ­nimo: {minimo:.2f}, MÃ¡ximo: {maximo:.2f} aÃ±os\")\n",
    "        \n",
    "        # Registrar en el log\n",
    "        app_logger.info(f\"AnÃ¡lisis de YearBuilt vs GarageYrBlt completado. Coinciden: {porcentaje_coinciden:.2f}%, Diferentes: {porcentaje_diferentes:.2f}%\")\n",
    "        app_logger.info(f\"El {porcentaje_no_coincide_ninguno:.2f}% de las casas tienen GarageYrBlt que no coincide ni con YearBuilt ni con YearRemodAdd.\")\n",
    "        app_logger.info(f\"El {porcentaje_mas_7_anos_total:.2f}% del total de casas tienen GarageYrBlt a mÃ¡s de 7 aÃ±os de YearBuilt y YearRemodAdd.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        app_logger.error(f\"Error en el anÃ¡lisis de datos: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porcentaje de casas donde YearBuilt coincide con GarageYrBlt: 74.59%\n",
      "Del 25.41% que no coincide con YearBuilt, 14.56% coincide con YearRemodAdd\n",
      "Porcentaje de casas donde GarageYrBlt no coincide ni con YearBuilt ni con YearRemodAdd: 21.71%\n",
      "Porcentaje de casas donde GarageYrBlt estÃ¡ a mÃ¡s de 7 aÃ±os de YearBuilt y YearRemodAdd respecto al total: 9.45%\n",
      "Porcentaje de casas donde GarageYrBlt no coincide con YearBuilt ni con YearRemodAdd y estÃ¡ a menos de 7 aÃ±os de YearBuilt: 22.08%\n",
      "Porcentaje de casas donde GarageYrBlt no coincide con YearBuilt ni con YearRemodAdd y estÃ¡ a menos de 7 aÃ±os de YearRemodAdd: 23.03%\n",
      "Porcentaje de casas donde GarageYrBlt no coincide con YearBuilt ni con YearRemodAdd y estÃ¡ a mÃ¡s de 7 aÃ±os de ambas fechas: 43.53%\n",
      "Tiempo construccion - Media: 5.55, MÃ­nimo: -10.00, MÃ¡ximo: 123.00 aÃ±os\n",
      "Tiempo remodelacion - Media: 6.93, MÃ­nimo: -53.00, MÃ¡ximo: 98.00 aÃ±os\n",
      "Tiempo hasta_remodelacion - Media: 13.60, MÃ­nimo: 0.00, MÃ¡ximo: 123.00 aÃ±os\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_11380\\2723570695.py:6: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_11380\\2723570695.py:7: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ejecutar el anÃ¡lisis\n",
    "try:\n",
    "    analizar_datos(train_data_filtrado)\n",
    "except Exception as e:\n",
    "    app_logger.error(f\"Error general en la ejecuciÃ³n del anÃ¡lisis: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_porcentaje(numerador, denominador):\n",
    "    try:\n",
    "        return (numerador / denominador) * 100\n",
    "    except ZeroDivisionError:\n",
    "        app_logger.warning(\"DivisiÃ³n por cero al calcular porcentaje\")\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analizar_bsmt_fin_type2(df):\n",
    "    try:\n",
    "        coincidencias = ((df['BsmtFinType2'] == 'Unf') & (df['BsmtFinSF2'] == 0)).sum()\n",
    "        total_casos_0 = (df['BsmtFinSF2'] == 0).sum()\n",
    "        porcentaje_coincidencia = calcular_porcentaje(coincidencias, total_casos_0)\n",
    "\n",
    "        app_logger.info(f\"El {porcentaje_coincidencia:.2f}% de los casos tienen BsmtFinType2 == 'Unf' y BsmtFinSF2 == 0 simultÃ¡neamente.\")\n",
    "        print(f\"Porcentaje de coincidencia entre BsmtFinType2 == 'Unf' y BsmtFinSF2 == 0 del total de casas donde BsmtFinSF2 == 0: {porcentaje_coincidencia:.2f}%\")\n",
    "\n",
    "        no_coincidencias = ((df['BsmtFinType2'] == 'Unf') & (df['BsmtFinSF2'] != 0)).sum()\n",
    "        total_casos_no_0 = (df['BsmtFinSF2'] != 0).sum()\n",
    "        porcentaje_no_coincidencia = calcular_porcentaje(no_coincidencias, total_casos_no_0)\n",
    "\n",
    "        app_logger.info(f\"El {porcentaje_no_coincidencia:.2f}% de los casos tienen BsmtFinType2 == 'Unf' pero BsmtFinSF2 != 0. Con un total de {no_coincidencias} casos.\")\n",
    "        print(f\"Porcentaje de casos donde BsmtFinType2 == 'Unf' pero BsmtFinSF2 != 0 respecto al total de casas donde BsmtFinSF2 != 0: {porcentaje_no_coincidencia:.2f}%\")\n",
    "\n",
    "        combinaciones_cero = df[df['BsmtFinSF2'] == 0]['BsmtFinType2'].value_counts()\n",
    "        porcentaje_combinaciones = calcular_porcentaje(combinaciones_cero, len(df))\n",
    "\n",
    "        print(\"Combinaciones con BsmtFinSF2 == 0:\")\n",
    "        for tipo, conteo in combinaciones_cero.items():\n",
    "            porcentaje = porcentaje_combinaciones[tipo]\n",
    "            print(f\"BsmtFinType2 = '{tipo}': {conteo} casos ({porcentaje:.2f}%)\")\n",
    "            app_logger.info(f\"BsmtFinType2 = '{tipo}': {porcentaje:.2f}% de los casos\")\n",
    "\n",
    "        porcentaje_total_cero = (df['BsmtFinSF2'] == 0).mean() * 100\n",
    "        print(f\"\\nPorcentaje total de casos con BsmtFinSF2 == 0: {porcentaje_total_cero:.2f}%\")\n",
    "        app_logger.info(f\"El {porcentaje_total_cero:.2f}% de los casos tienen BsmtFinSF2 == 0\")\n",
    "\n",
    "    except KeyError as e:\n",
    "        app_logger.error(f\"Error al acceder a las columnas del DataFrame: {e}\")\n",
    "    except Exception as e:\n",
    "        app_logger.error(f\"Error inesperado durante el anÃ¡lisis: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porcentaje de coincidencia entre BsmtFinType2 == 'Unf' y BsmtFinSF2 == 0 del total de casas donde BsmtFinSF2 == 0: 97.14%\n",
      "Porcentaje de casos donde BsmtFinType2 == 'Unf' pero BsmtFinSF2 != 0 respecto al total de casas donde BsmtFinSF2 != 0: 0.00%\n",
      "Combinaciones con BsmtFinSF2 == 0:\n",
      "BsmtFinType2 = 'Unf': 1256 casos (86.03%)\n",
      "\n",
      "Porcentaje total de casos con BsmtFinSF2 == 0: 88.56%\n"
     ]
    }
   ],
   "source": [
    "# Ejecutar el anÃ¡lisis\n",
    "try:\n",
    "    analizar_bsmt_fin_type2(train_data_filtrado)\n",
    "except Exception as e:\n",
    "    app_logger.error(f\"Error general en la ejecuciÃ³n del anÃ¡lisis: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XiCorr Coeficiente\n",
    "El coeficiente Xicorr (Xi-correlation coefficient) es una medida de asociaciÃ³n entre dos variables que puede ser utilizada como alternativa al coeficiente de correlaciÃ³n de Pearson, especialmente cuando las variables no tienen una relaciÃ³n lineal.\n",
    "\n",
    "### DefiniciÃ³n del Coeficiente Xicorr\n",
    "\n",
    "El coeficiente Xicorr se basa en la idea de las \"permutaciones locales\". Se enfoca en la desviaciÃ³n de las permutaciones locales de una relaciÃ³n ideal, y puede ser mÃ¡s robusto frente a distribuciones no normales y relaciones no lineales.\n",
    "\n",
    "#### 1. Supongamos que tenemos dos variables $X$ e $Y$ con $n$ observaciones cada una.\n",
    "\n",
    "$$ X = (X_1, X_2, \\ldots, X_n) $$\n",
    "$$ Y = (Y_1, Y_2, \\ldots, Y_n) $$\n",
    "\n",
    "\n",
    "#### 2. Ordenamos ambas variables en orden ascendente.\n",
    "\n",
    "$$ X_{(1)}, X_{(2)}, \\ldots, X_{(n)} $$\n",
    "$$ Y_{(1)}, Y_{(2)}, \\ldots, Y_{(n)} $$\n",
    "\n",
    "#### 3. Calculamos las posiciones de los valores originales en los vectores ordenados. Denotamos las posiciones de $X_i$ en $X_{(i)}$ como $P_X(i)$ y de $Y_i$ en $Y_{(i)}$ como $P_Y(i)$.\n",
    "\n",
    "$$ P_X(i) = \\text{posiciÃ³n de } X_i \\text{ en } X_{(i)} $$\n",
    "$$ P_Y(i) = \\text{posiciÃ³n de } Y_i \\text{ en } Y_{(i)} $$\n",
    "\n",
    "#### 4. Definimos las permutaciones locales $ \\pi_X $ y $ \\pi_Y $ de $X$ e $Y$ respectivamente, que representan cÃ³mo se permutan los valores cuando se ordenan.\n",
    "\n",
    "#### 5. Calculamos la desviaciÃ³n de estas permutaciones locales de una relaciÃ³n ideal. Esta desviaciÃ³n se mide a travÃ©s de una funciÃ³n de distancia. Una forma comÃºn de definir esta distancia es usar la distancia de Kendall ($ \\tau $).\n",
    "\n",
    "$$ \\tau(\\pi_X, \\pi_Y) = \\text{NÃºmero de discordancias entre } \\pi_X \\text{ y } \\pi_Y $$\n",
    "\n",
    "#### 6. Normalizamos esta distancia para obtener el coeficiente Xicorr. La normalizaciÃ³n se hace para que el coeficiente estÃ© en el rango [-1, 1], similar a los coeficientes de correlaciÃ³n tradicionales.\n",
    "\n",
    "$$ \\text{Xicorr}(X, Y) = 1 - \\frac{2 \\tau(\\pi_X, \\pi_Y)}{n(n-1)/2} $$\n",
    "\n",
    "AquÃ­, $ n(n-1)/2 $ es el nÃºmero total de pares posibles en el conjunto de datos.\n",
    "\n",
    "### InterpretaciÃ³n del Coeficiente Xicorr\n",
    "\n",
    "- **Xicorr = 1**: Indica una correlaciÃ³n perfecta y positiva entre las variables $X$ e $Y$.\n",
    "- **Xicorr = -1**: Indica una correlaciÃ³n perfecta y negativa entre las variables $X$ e $Y$.\n",
    "- **Xicorr = 0**: Indica que no hay correlaciÃ³n entre las variables $X$ e $Y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pasos Intuitivos del CÃ¡lculo del Coeficiente Xicorr\n",
    "\n",
    "1. **OrdenaciÃ³n de las Variables**:\n",
    "   - Imagina que tienes dos listas de nÃºmeros, una para cada variable, $X$ e $Y$. Primero, ordenas cada lista de menor a mayor.\n",
    "   - Por ejemplo, si $X = [4, 1, 3]$, lo ordenas como $X_{ordenado} = [1, 3, 4]$. Lo mismo haces con $Y$.\n",
    "\n",
    "2. **Rastreo de las Posiciones Originales**:\n",
    "   - Luego, haces un seguimiento de las posiciones originales de los elementos en las listas ordenadas. Es decir, determinas dÃ³nde estaba originalmente cada valor en la lista desordenada.\n",
    "   - Por ejemplo, si $X = [4, 1, 3]$ y $X_{ordenado} = [1, 3, 4]$, el valor 1 estaba en la posiciÃ³n 2 originalmente, el 3 en la posiciÃ³n 3 y el 4 en la posiciÃ³n 1.\n",
    "\n",
    "3. **ComparaciÃ³n de las Permutaciones**:\n",
    "   - Ahora, haces lo mismo para la variable $Y$. Una vez que tienes las posiciones originales de ambos conjuntos de datos ordenados, puedes compararlas.\n",
    "   - Imagina que tienes las posiciones originales de $X$ como $\\pi_X$ y las posiciones originales de $Y$ como $\\pi_Y$.\n",
    "\n",
    "4. **MediciÃ³n de la Discordancia**:\n",
    "   - Comparas estas permutaciones ($\\pi_X$ y $\\pi_Y$) para ver cuÃ¡n diferentes son. La discordancia se mide contando cuÃ¡ntas veces un par de elementos estÃ¡ en un orden diferente en $X$ en comparaciÃ³n con $Y$.\n",
    "   - Por ejemplo, si en $X$ el segundo elemento viene antes que el primero pero en $Y$ el primero viene antes que el segundo, eso es una discordancia.\n",
    "\n",
    "5. **Calculo de la Distancia de Discordancia**:\n",
    "   - La distancia de Kendall ($\\tau$) es una forma comÃºn de medir esta discordancia, contando cuÃ¡ntas discordancias hay entre las posiciones de $X$ e $Y$.\n",
    "   - Si hay muchas discordancias, significa que las listas estÃ¡n muy desalineadas.\n",
    "\n",
    "6. **NormalizaciÃ³n de la Distancia**:\n",
    "   - Para obtener el coeficiente Xicorr, normalizas la cantidad de discordancias para que el valor resultante estÃ© en un rango estÃ¡ndar (generalmente de -1 a 1).\n",
    "   - La fÃ³rmula de normalizaciÃ³n convierte la cantidad de discordancias en un valor que puede interpretarse como un coeficiente de correlaciÃ³n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection Statiscal Methods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_11380\\3479987822.py:20: FutureWarning:\n",
      "\n",
      "Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 correlaciones mÃ¡s fuertes con SalePrice:\n",
      "Id              0.228061\n",
      "PoolArea        0.216643\n",
      "LowQualFinSF    0.216589\n",
      "3SsnPorch       0.213059\n",
      "BsmtFinSF2      0.212266\n",
      "ScreenPorch     0.207370\n",
      "BsmtHalfBath    0.204921\n",
      "KitchenAbvGr    0.193598\n",
      "LotArea         0.154691\n",
      "FullBath        0.116227\n",
      "Name: SalePrice, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\david\\anaconda3\\envs\\david_pcc\\lib\\site-packages\\dcor\\_fast_dcov_avl.py:554: UserWarning:\n",
      "\n",
      "Falling back to uncompiled AVL fast distance covariance terms because of TypeError exception raised: No matching definition for argument type(s) array(int64, 1d, C), array(int64, 1d, C), bool. Rembember: only floating point values can be used in the compiled implementations.\n",
      "\n",
      "c:\\Users\\david\\anaconda3\\envs\\david_pcc\\lib\\site-packages\\dcor\\_dcor_internals.py:188: RuntimeWarning:\n",
      "\n",
      "overflow encountered in scalar multiply\n",
      "\n",
      "c:\\Users\\david\\anaconda3\\envs\\david_pcc\\lib\\site-packages\\dcor\\_fast_dcov_avl.py:554: UserWarning:\n",
      "\n",
      "Falling back to uncompiled AVL fast distance covariance terms because of TypeError exception raised: No matching definition for argument type(s) array(float64, 1d, C), array(int64, 1d, C), bool. Rembember: only floating point values can be used in the compiled implementations.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features en la uniÃ³n de los top 10 de HSIC, MIC y DCOR:\n",
      "{'BsmtHalfBath', 'TotRmsAbvGrd', 'OverallCond', 'KitchenAbvGr', 'GarageYrBlt', 'YearBuilt', 'BedroomAbvGr', 'OverallQual', 'GrLivArea', '1stFlrSF', 'GarageCars', 'MoSold', 'YrSold', 'FullBath', 'MasVnrArea', 'TotalBsmtSF', 'YearRemodAdd', 'BsmtFullBath', 'GarageArea', 'Fireplaces', 'LotFrontage', 'HalfBath'}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from minepy import MINE\n",
    "import dcor\n",
    "from scipy.stats import gamma\n",
    "from scipy import stats\n",
    "import xicorpy\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "def calcular_xi_correlacion(x, y):\n",
    "    resultado = xicorpy.compute_xi_correlation(x, y, get_modified_xi=False, m_nearest_neighbours=3, get_p_values=False)\n",
    "    return resultado.iloc[0] if isinstance(resultado, (pd.Series, pd.DataFrame)) else resultado\n",
    "\n",
    "def calcular_correlacion(x, y):\n",
    "    # Comprobamos si las variables son numÃ©ricas\n",
    "    if not (np.issubdtype(x.dtype, np.number) and np.issubdtype(y.dtype, np.number)):\n",
    "        raise ValueError(\"Ambas variables deben ser numÃ©ricas\")\n",
    "    \n",
    "    # Comprobamos la normalidad de las variables\n",
    "    _, p_value_x = stats.normaltest(x)\n",
    "    _, p_value_y = stats.normaltest(y)\n",
    "    \n",
    "    # Si ambas variables son normales, usamos Pearson\n",
    "    if p_value_x > 0.05 and p_value_y > 0.05:\n",
    "        return stats.pearsonr(x, y)[0]\n",
    "    \n",
    "    # Comprobamos la monotonicidad de la relaciÃ³n\n",
    "    is_monotonic = np.all(np.diff(y[np.argsort(x)]) >= 0) or np.all(np.diff(y[np.argsort(x)]) <= 0)\n",
    "    \n",
    "    # Si la relaciÃ³n es monÃ³tona, usamos Spearman, si no, Kendall\n",
    "    if is_monotonic:\n",
    "        return stats.spearmanr(x, y)[0]\n",
    "    else:\n",
    "        return stats.kendalltau(x, y)[0]\n",
    "\n",
    "def calcular_matriz_xi(df):\n",
    "    columnas = df.columns\n",
    "    n = len(columnas)\n",
    "    matriz_xi = np.eye(n)\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            xi_valor = calcular_xi_correlacion(df.iloc[:, i], df.iloc[:, j])\n",
    "            matriz_xi[i, j] = matriz_xi[j, i] = xi_valor\n",
    "    return pd.DataFrame(matriz_xi, index=columnas, columns=columnas)\n",
    "\n",
    "def hsic_gam(X, Y, alph=0.05):\n",
    "    n = X.shape[0]\n",
    "    H = np.eye(n) - np.ones((n, n)) / n\n",
    "    Kc = H @ X @ X.T @ H\n",
    "    Lc = H @ Y @ Y.T @ H\n",
    "    testStat = np.trace(Kc @ Lc)\n",
    "    varHSIC = (Kc * Lc).sum() / (n * (n - 1))\n",
    "    K = varHSIC * (n - 1) * (n - 2) * (n - 3)\n",
    "    c = np.sqrt(K) / n\n",
    "    v = varHSIC * (2 * (n - 2)) / (n * (n - 1))\n",
    "    alph = 1 - c / (n * (n - 1))\n",
    "    thresh = gamma.ppf(1 - alph, v / 2, scale=2 / v)\n",
    "    return testStat, thresh\n",
    "\n",
    "def calcular_mic(x, y):\n",
    "    mine = MINE()\n",
    "    mine.compute_score(x, y)\n",
    "    return mine.mic()\n",
    "\n",
    "def analizar_correlaciones(train_data_filtrado):\n",
    "    app_logger.info(\"Calculando la matriz de correlaciÃ³n Xi...\")\n",
    "    matriz_xi = calcular_matriz_xi(train_data_filtrado.select_dtypes(include=[np.number]))\n",
    "    correlaciones_saleprice = matriz_xi['SalePrice'].sort_values(ascending=False).drop('SalePrice')\n",
    "    top_correlaciones = correlaciones_saleprice.head(10)\n",
    "    print(\"Top 10 correlaciones mÃ¡s fuertes con SalePrice:\")\n",
    "    print(top_correlaciones)\n",
    "    app_logger.info(\"Se han identificado las 10 correlaciones mÃ¡s fuertes con SalePrice utilizando el mÃ©todo Xi.\")\n",
    "\n",
    "def realizar_prueba_hsic(train_data_filtrado):\n",
    "    app_logger.info(\"Realizando prueba de la funciÃ³n HSIC...\")\n",
    "    caracteristicas_numericas = train_data_filtrado.select_dtypes(include=[np.number]).drop(columns=['SalePrice'])\n",
    "    Y = train_data_filtrado['SalePrice'].values.reshape(-1, 1)\n",
    "    resultados = {}\n",
    "    for caracteristica in caracteristicas_numericas:\n",
    "        X = train_data_filtrado[caracteristica].values.reshape(-1, 1)\n",
    "        estadistico_prueba, umbral = hsic_gam(X, Y)\n",
    "        resultados[caracteristica] = {\n",
    "            'estadistico': estadistico_prueba,\n",
    "            'umbral': umbral,\n",
    "            'dependiente': estadistico_prueba > umbral\n",
    "        }\n",
    "        if resultados[caracteristica]['dependiente']:\n",
    "            app_logger.info(f\"La variable {caracteristica} y el precio de venta son dependientes segÃºn la prueba HSIC.\")\n",
    "        else:\n",
    "            app_logger.info(f\"No se puede rechazar la hipÃ³tesis de independencia entre {caracteristica} y el precio de venta.\")\n",
    "    \n",
    "    caracteristicas_dependientes = {k: v for k, v in resultados.items() if v['dependiente']}\n",
    "    app_logger.info(f\"Se encontraron {len(caracteristicas_dependientes)} caracterÃ­sticas dependientes del precio de venta segÃºn la prueba HSIC.\")\n",
    "    return resultados, caracteristicas_dependientes\n",
    "\n",
    "def analizar_mic_y_correlacion_distancia(train_data_filtrado):\n",
    "    app_logger.info(\"Calculando los puntajes MIC y correlaciÃ³n de distancia...\")\n",
    "    columnas_numericas = train_data_filtrado.select_dtypes(include=[np.number]).drop(columns=['SalePrice']).columns\n",
    "    puntajes_mic = {columna: calcular_mic(train_data_filtrado[columna], train_data_filtrado['SalePrice']) for columna in columnas_numericas}\n",
    "    puntajes_corr_dist = {columna: dcor.distance_correlation(train_data_filtrado[columna], train_data_filtrado['SalePrice']) for columna in columnas_numericas}\n",
    "    \n",
    "    puntajes_mic = sorted(puntajes_mic.items(), key=lambda item: item[1], reverse=True)\n",
    "    puntajes_corr_dist = sorted(puntajes_corr_dist.items(), key=lambda item: item[1], reverse=True)\n",
    "    \n",
    "    app_logger.info(\"AnÃ¡lisis de MIC y correlaciÃ³n de distancia completado.\")\n",
    "    return puntajes_mic, puntajes_corr_dist\n",
    "\n",
    "def analizar_correlacion(train_data_filtrado):\n",
    "    app_logger.info(\"Analizando la correlaciÃ³n entre las caracterÃ­sticas...\")\n",
    "    \n",
    "    # Seleccionar solo las columnas numÃ©ricas\n",
    "    datos_numericos = train_data_filtrado.select_dtypes(include=[np.number])\n",
    "    \n",
    "    # Calcular las correlaciones con 'SalePrice' usando la funciÃ³n personalizada\n",
    "    correlaciones_saleprice = {}\n",
    "    for columna in datos_numericos.columns:\n",
    "        if columna != 'SalePrice':\n",
    "            try:\n",
    "                correlacion = calcular_correlacion(datos_numericos[columna], datos_numericos['SalePrice'])\n",
    "                correlaciones_saleprice[columna] = correlacion\n",
    "            except ValueError as e:\n",
    "                app_logger.warning(f\"No se pudo calcular la correlaciÃ³n para {columna}: {str(e)}\")\n",
    "    \n",
    "    # Ordenar las correlaciones\n",
    "    correlaciones_saleprice = dict(sorted(correlaciones_saleprice.items(), key=lambda item: abs(item[1]), reverse=True))\n",
    "    \n",
    "    # Obtener las 10 caracterÃ­sticas mÃ¡s correlacionadas con 'SalePrice'\n",
    "    top_10_correlaciones = dict(list(correlaciones_saleprice.items())[:10])\n",
    "    \n",
    "    print(\"Las 10 caracterÃ­sticas mÃ¡s correlacionadas con SalePrice son:\")\n",
    "    for caracteristica, correlacion in top_10_correlaciones.items():\n",
    "        print(f\"{caracteristica}: {correlacion}\")\n",
    "    \n",
    "    app_logger.info(\"AnÃ¡lisis de correlaciÃ³n completado.\")\n",
    "    \n",
    "    return top_10_correlaciones\n",
    "\n",
    "# Ejecutar la funciÃ³n de anÃ¡lisis de correlaciÃ³n\n",
    "top_10_correlaciones = analizar_correlacion(train_data_filtrado)\n",
    "\n",
    "# Ejecutar las funciones\n",
    "analizar_correlaciones(train_data_filtrado)\n",
    "resultados_hsic, caracteristicas_dependientes = realizar_prueba_hsic(train_data_filtrado)\n",
    "puntajes_mic, puntajes_corr_dist = analizar_mic_y_correlacion_distancia(train_data_filtrado)\n",
    "\n",
    "# Unir los resultados\n",
    "union_features = set(caracteristicas_dependientes.keys()) | set(dict(puntajes_mic[:10]).keys()) | set(dict(puntajes_corr_dist[:10]).keys())\n",
    "\n",
    "app_logger.info(\"Features en la uniÃ³n de los top 10 de HSIC, MIC y DCOR:\")\n",
    "app_logger.info(union_features)\n",
    "\n",
    "print(\"Features en la uniÃ³n de los top 10 de HSIC, MIC y DCOR:\")\n",
    "print(union_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar las bibliotecas necesarias\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar los datos\n",
    "try:\n",
    "    # Seleccionar las caracterÃ­sticas de union_features\n",
    "    X = train_data_filtrado[list(union_features)]\n",
    "    y = train_data_filtrado['SalePrice']\n",
    "except Exception as e:\n",
    "    errors_logger.error(f\"Error al seleccionar las caracterÃ­sticas de union_features: {e}\")\n",
    "    raise\n",
    "\n",
    "app_logger.info(\"CaracterÃ­sticas seleccionadas correctamente.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CaracterÃ­sticas de baja varianza en union_features:\n",
      "['KitchenAbvGr']\n"
     ]
    }
   ],
   "source": [
    "# Identificar caracterÃ­sticas de baja varianza que estÃ¡n en union_features\n",
    "caracteristicas_baja_varianza = [feature for feature in Low_Variance_Features if feature in union_features]\n",
    "\n",
    "if len(caracteristicas_baja_varianza) > 0:\n",
    "    app_logger.info(f\"Se encontraron {len(caracteristicas_baja_varianza)} caracterÃ­sticas de baja varianza en union_features:\")\n",
    "    for caracteristica in caracteristicas_baja_varianza:\n",
    "        app_logger.info(f\"  - {caracteristica}\")\n",
    "else:\n",
    "    app_logger.info(\"No se encontraron caracterÃ­sticas de baja varianza de Low_variance_features en union_features.\")\n",
    "\n",
    "# Imprimir resultados\n",
    "print(\"CaracterÃ­sticas de baja varianza en union_features:\")\n",
    "print(caracteristicas_baja_varianza)\n",
    "\n",
    "print(\"CaracterÃ­sticas de baja varianza totales:\")\n",
    "print(Low_Variance_Features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminamos esta feature por que no aporta informaciÃ³n relevante ya que el 95,31% estan a 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CaracterÃ­sticas restantes en X despuÃ©s de eliminar 'KitchenAbvGr':\n",
      "['OverallQual', 'GarageArea', 'TotalBsmtSF', 'LotFrontage', 'BedroomAbvGr', 'YearBuilt', 'YearRemodAdd', 'OverallCond', 'MasVnrArea', 'BsmtHalfBath', 'TotRmsAbvGrd', 'GarageCars', 'HalfBath', 'GarageYrBlt', '1stFlrSF', 'Fireplaces', 'YrSold', 'FullBath', 'MoSold', 'BsmtFullBath', 'GrLivArea']\n"
     ]
    }
   ],
   "source": [
    "# Eliminar la caracterÃ­stica 'KitchenAbvGr' de X\n",
    "app_logger.info(\"Eliminando la caracterÃ­stica 'KitchenAbvGr' de X...\")\n",
    "try:\n",
    "    X = X.drop('KitchenAbvGr', axis=1)\n",
    "    app_logger.info(\"La caracterÃ­stica 'KitchenAbvGr' ha sido eliminada de X.\")\n",
    "except KeyError:\n",
    "    app_logger.info(\"La caracterÃ­stica 'KitchenAbvGr' no se encuentra en X.\")\n",
    "except Exception as e:\n",
    "    errors_logger.error(f\"Error al eliminar la caracterÃ­stica 'KitchenAbvGr' de X: {e}\")\n",
    "    raise\n",
    "\n",
    "# Imprimir las caracterÃ­sticas restantes\n",
    "print(\"CaracterÃ­sticas restantes en X despuÃ©s de eliminar 'KitchenAbvGr':\")\n",
    "print(X.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables con valores faltantes:\n",
      "             Valores Faltantes  Porcentaje\n",
      "LotFrontage                259   17.739726\n",
      "GarageYrBlt                 81    5.547945\n",
      "MasVnrArea                   8    0.547945\n"
     ]
    }
   ],
   "source": [
    "# AnÃ¡lisis de valores faltantes\n",
    "app_logger.info(\"Iniciando anÃ¡lisis de valores faltantes...\")\n",
    "\n",
    "try:\n",
    "    # Calcular el nÃºmero de valores faltantes por columna\n",
    "    missing_values = X.isnull().sum()\n",
    "    \n",
    "    # Calcular el porcentaje de valores faltantes\n",
    "    missing_percentage = 100 * missing_values / len(X)\n",
    "    \n",
    "    # Crear un DataFrame con los resultados\n",
    "    missing_table = pd.concat([missing_values, missing_percentage], axis=1, keys=['Valores Faltantes', 'Porcentaje'])\n",
    "    \n",
    "    # Ordenar el DataFrame por el nÃºmero de valores faltantes en orden descendente\n",
    "    missing_table = missing_table[missing_table['Valores Faltantes'] > 0].sort_values('Valores Faltantes', ascending=False)\n",
    "    \n",
    "    if not missing_table.empty:\n",
    "        app_logger.info(\"Se encontraron las siguientes variables con valores faltantes:\")\n",
    "        for index, row in missing_table.iterrows():\n",
    "            app_logger.info(f\"  - {index}: {row['Valores Faltantes']} valores faltantes ({row['Porcentaje']:.2f}%)\")\n",
    "        \n",
    "        print(\"Variables con valores faltantes:\")\n",
    "        print(missing_table)\n",
    "    else:\n",
    "        app_logger.info(\"No se encontraron variables con valores faltantes.\")\n",
    "        print(\"No hay variables con valores faltantes.\")\n",
    "\n",
    "except Exception as e:\n",
    "    errors_logger.error(f\"Error durante el anÃ¡lisis de valores faltantes: {e}\")\n",
    "    raise\n",
    "\n",
    "app_logger.info(\"AnÃ¡lisis de valores faltantes completado.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tratamiento de nulos de ``GarageYrBlt``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocesar_GarageYrBlt(X):\n",
    "    try:\n",
    "        # Rellenar valores nulos de GarageYrBlt con YearBuilt\n",
    "        X.loc[:, 'GarageYrBlt'] = X.loc[:, 'GarageYrBlt'].fillna(X['YearBuilt'])\n",
    "        \n",
    "        # Usar .loc para ajustar GarageYrBlt\n",
    "        X.loc[:, 'GarageYrBlt'] = X.apply(lambda row: max(row['GarageYrBlt'], row['YearBuilt']), axis=1)\n",
    "        \n",
    "        app_logger.info(\"Se ha utilizado .loc para evitar SettingWithCopyWarning en el preprocesamiento de GarageYrBlt\")\n",
    "        return X\n",
    "    except KeyError as e:\n",
    "        app_logger.error(f\"Error: Columna no encontrada - {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        app_logger.error(f\"Error inesperado durante el preprocesamiento de GarageYrBlt: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    Nans_previous = X['GarageYrBlt'].isna().sum()\n",
    "    X = preprocesar_GarageYrBlt(X)\n",
    "    Nans_after = X['GarageYrBlt'].isna().sum()\n",
    "    app_logger.info(f\"Preprocesamiento de GarageYrBlt completado. Nans anteriores: {Nans_previous}, Nans despuÃ©s: {Nans_after}\")\n",
    "except Exception as e:\n",
    "    app_logger.error(f\"Error durante el preprocesamiento de GarageYrBlt: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tratamiento de NaNs de ``MasVnrArea``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determinar_tipo_y_tratar_nans(X, columna):\n",
    "    try:\n",
    "        tipo = X[columna].dtype\n",
    "        \n",
    "        if np.issubdtype(tipo, np.number):\n",
    "            if X[columna].nunique() < 20:\n",
    "                app_logger.info(f\"{columna} se considerarÃ¡ categÃ³rica debido a que tiene menos de 20 valores Ãºnicos.\")\n",
    "                es_categorica = True\n",
    "            else:\n",
    "                app_logger.info(f\"{columna} se considerarÃ¡ numÃ©rica.\")\n",
    "                es_categorica = False\n",
    "        else:\n",
    "            app_logger.info(f\"{columna} es categÃ³rica por su tipo de dato.\")\n",
    "            es_categorica = True\n",
    "        \n",
    "        if es_categorica:\n",
    "            moda = X[columna].mode()[0]\n",
    "            X[columna].fillna(moda, inplace=True)\n",
    "            app_logger.info(f\"Se han rellenado los NaNs de {columna} con la moda: {moda}\")\n",
    "        else:\n",
    "            mediana = X[columna].median()\n",
    "            X[columna].fillna(mediana, inplace=True)\n",
    "            app_logger.info(f\"Se han rellenado los NaNs de {columna} con la mediana: {mediana}\")\n",
    "        \n",
    "        nans_restantes = X[columna].isna().sum()\n",
    "        app_logger.info(f\"NaNs restantes en {columna} despuÃ©s del tratamiento: {nans_restantes}\")\n",
    "        \n",
    "    except KeyError:\n",
    "        app_logger.error(f\"La columna {columna} no existe en el DataFrame.\")\n",
    "    except Exception as e:\n",
    "        app_logger.error(f\"Error inesperado al procesar la columna {columna}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uso de la funciÃ³n\n",
    "try:\n",
    "    determinar_tipo_y_tratar_nans(X, 'MasVnrArea')\n",
    "except Exception as e:\n",
    "    app_logger.error(f\"Error general en el procesamiento: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tratamiento de NaNs de ``LotFrontage``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tratar_nans_lotfrontage(X):\n",
    "    try:\n",
    "        mediana_LotFrontage = X['LotFrontage'].median()\n",
    "        X['LotFrontage'].fillna(mediana_LotFrontage, inplace=True)\n",
    "        app_logger.info(f\"Se han rellenado los NaNs de LotFrontage con la mediana: {mediana_LotFrontage}\")\n",
    "        \n",
    "        nans_restantes_LotFrontage = X['LotFrontage'].isna().sum()\n",
    "        app_logger.info(f\"NaNs restantes en LotFrontage despuÃ©s del tratamiento: {nans_restantes_LotFrontage}\")\n",
    "        \n",
    "        return X\n",
    "    except KeyError:\n",
    "        app_logger.error(\"La columna 'LotFrontage' no existe en el DataFrame.\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        app_logger.error(f\"Error inesperado al tratar los NaNs de LotFrontage: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uso de la funciÃ³n\n",
    "try:\n",
    "    X = tratar_nans_lotfrontage(X)\n",
    "except Exception as e:\n",
    "    app_logger.error(f\"No se pudo completar el tratamiento de NaNs en LotFrontage: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def procesar_variables_categoricas(X, variables):\n",
    "    \"\"\"Convierte las variables especificadas a tipo categÃ³rico.\"\"\"\n",
    "    try:\n",
    "        for var in variables:\n",
    "            X[var] = X[var].astype('category')\n",
    "            app_logger.info(f\"Se ha convertido {var} a tipo categÃ³rico\")\n",
    "        return X\n",
    "    except KeyError as e:\n",
    "        app_logger.error(f\"Error al procesar variable categÃ³rica: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        app_logger.error(f\"Error inesperado al procesar variables categÃ³ricas: {e}\")\n",
    "        raise\n",
    "\n",
    "def crear_nuevas_features(X):\n",
    "    \"\"\"Crea nuevas features basadas en aÃ±os.\"\"\"\n",
    "    try:\n",
    "        X['BuiltUntilSold'] = X['YrSold'] - X['YearBuilt']\n",
    "        X['BuiltUntilRemod'] = X['YearRemodAdd'] - X['YearBuilt']\n",
    "        X['HasBeenRemodeled'] = (X['YearRemodAdd'] != X['YearBuilt']).astype(bool)\n",
    "        app_logger.info(\"Se han creado las nuevas features: BuiltUntilSold, BuiltUntilRemod y HasBeenRemodeled\")\n",
    "        return X\n",
    "    except KeyError as e:\n",
    "        app_logger.error(f\"Error al crear nuevas features: Columna no encontrada - {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        app_logger.error(f\"Error inesperado al crear nuevas features: {e}\")\n",
    "        raise\n",
    "\n",
    "def clasificar_variables(X):\n",
    "    \"\"\"Clasifica las variables en numÃ©ricas y categÃ³ricas.\"\"\"\n",
    "    variables_numericas = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    variables_categoricas = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    \n",
    "    variables_a_excluir = ['YearRemodAdd', 'YearBuilt', 'YrSold'] + variables_cardinales\n",
    "    variables_numericas = [var for var in variables_numericas if var not in variables_a_excluir]\n",
    "    \n",
    "    app_logger.info(f\"Variables numÃ©ricas: {variables_numericas}\")\n",
    "    app_logger.info(f\"Variables categÃ³ricas: {variables_categoricas}\")\n",
    "    return variables_numericas, variables_categoricas\n",
    "\n",
    "def imprimir_estadisticas(X):\n",
    "    \"\"\"Imprime estadÃ­sticas de las nuevas features.\"\"\"\n",
    "    app_logger.info(f\"EstadÃ­sticas de BuiltUntilSold: \\n{X['BuiltUntilSold'].describe()}\")\n",
    "    app_logger.info(f\"EstadÃ­sticas de BuiltUntilRemod: \\n{X['BuiltUntilRemod'].describe()}\")\n",
    "    app_logger.info(f\"ProporciÃ³n de casas remodeladas: {X['HasBeenRemodeled'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables globales\n",
    "variables_cardinales = ['HalfBath', 'Fireplaces', 'FullBath', 'BedroomAbvGr', 'TotRmsAbvGrd', 'GarageCars', 'BsmtHalfBath', 'BsmtFullBath']\n",
    "variables_ordinales = ['OverallQual', 'OverallCond']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procesamiento principal\n",
    "try:\n",
    "    # Asumimos que X es un DataFrame de pandas ya cargado\n",
    "    X = procesar_variables_categoricas(X, variables_cardinales + variables_ordinales)\n",
    "    X = crear_nuevas_features(X)\n",
    "    variables_numericas, variables_categoricas = clasificar_variables(X)\n",
    "    imprimir_estadisticas(X)\n",
    "except Exception as e:\n",
    "    errors_logger.error(f\"Error en el procesamiento principal: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar las bibliotecas necesarias\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "import pandas as pd\n",
    "\n",
    "transformed_data = X.copy()\n",
    "\n",
    "# FunciÃ³n para aplicar Label Encoding y convertir a tipo category\n",
    "def aplicar_label_encoding(X, columna):\n",
    "    try:\n",
    "        le = LabelEncoder()\n",
    "        X[columna] = le.fit_transform(X[columna])\n",
    "        X[columna] = X[columna].astype('category')\n",
    "        app_logger.info(f\"Se aplicÃ³ Label Encoding a la columna {columna} y se convirtiÃ³ a tipo category\")\n",
    "    except Exception as e:\n",
    "        errors_logger.error(f\"Error al aplicar Label Encoding a la columna {columna}: {str(e)}\")\n",
    "    return X\n",
    "\n",
    "# FunciÃ³n para aplicar One-Hot Encoding\n",
    "def aplicar_one_hot_encoding(X, columna):\n",
    "    try:\n",
    "        ohe = OneHotEncoder(drop='first', sparse_output=False)\n",
    "        encoded = ohe.fit_transform(X[[columna]])\n",
    "        encoded_df = pd.DataFrame(encoded, columns=[f\"{columna}_{cat}\" for cat in ohe.categories_[0][1:]], dtype=bool)\n",
    "        X = pd.concat([X.drop(columna, axis=1), encoded_df], axis=1)\n",
    "        app_logger.info(f\"Se aplicÃ³ One-Hot Encoding a la columna {columna}\")\n",
    "    except Exception as e:\n",
    "        errors_logger.error(f\"Error al aplicar One-Hot Encoding a la columna {columna}: {str(e)}\")\n",
    "    return X\n",
    "\n",
    "# Iterar sobre las columnas categÃ³ricas\n",
    "for columna in transformed_data.select_dtypes(include=['object', 'category']).columns:\n",
    "    num_categorias = transformed_data[columna].nunique()\n",
    "    \n",
    "    if num_categorias > 6:\n",
    "        transformed_data = aplicar_label_encoding(transformed_data, columna)\n",
    "    else:\n",
    "        transformed_data = aplicar_one_hot_encoding(transformed_data, columna)\n",
    "\n",
    "app_logger.info(\"Se ha completado el procesamiento de variables categÃ³ricas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La columna GarageArea no sigue una distribuciÃ³n normal (p-valor: 0.0000)\n",
      "La columna TotalBsmtSF no sigue una distribuciÃ³n normal (p-valor: 0.0000)\n",
      "La columna LotFrontage no sigue una distribuciÃ³n normal (p-valor: 0.0000)\n",
      "La columna YearBuilt no sigue una distribuciÃ³n normal (p-valor: 0.0000)\n",
      "La columna YearRemodAdd no sigue una distribuciÃ³n normal (p-valor: 0.0000)\n",
      "La columna MasVnrArea no sigue una distribuciÃ³n normal (p-valor: 0.0000)\n",
      "La columna GarageYrBlt no sigue una distribuciÃ³n normal (p-valor: 0.0000)\n",
      "La columna 1stFlrSF no sigue una distribuciÃ³n normal (p-valor: 0.0009)\n",
      "La columna YrSold no sigue una distribuciÃ³n normal (p-valor: 0.0000)\n",
      "La columna MoSold no sigue una distribuciÃ³n normal (p-valor: 0.0000)\n",
      "La columna GrLivArea no sigue una distribuciÃ³n normal (p-valor: 0.0009)\n",
      "La columna BuiltUntilSold no sigue una distribuciÃ³n normal (p-valor: 0.0000)\n",
      "La columna BuiltUntilRemod no sigue una distribuciÃ³n normal (p-valor: 0.0000)\n",
      "La columna GarageArea_power no sigue una distribuciÃ³n normal (p-valor: 0.0000)\n",
      "La columna TotalBsmtSF_quantile no sigue una distribuciÃ³n normal (p-valor: 0.0000)\n",
      "La columna LotFrontage_quantile no sigue una distribuciÃ³n normal (p-valor: 0.0000)\n",
      "La columna YearBuilt_robustscaler no sigue una distribuciÃ³n normal (p-valor: 0.0000)\n",
      "La columna YearRemodAdd_robustscaler no sigue una distribuciÃ³n normal (p-valor: 0.0000)\n",
      "La columna MasVnrArea_quantile no sigue una distribuciÃ³n normal (p-valor: 0.0000)\n",
      "La columna GarageYrBlt_robustscaler no sigue una distribuciÃ³n normal (p-valor: 0.0000)\n",
      "La columna 1stFlrSF_quantile no sigue una distribuciÃ³n normal (p-valor: 0.0017)\n",
      "La columna MoSold_quantile no sigue una distribuciÃ³n normal (p-valor: 0.0000)\n",
      "La columna GrLivArea_quantile no sigue una distribuciÃ³n normal (p-valor: 0.0018)\n",
      "La columna BuiltUntilSold_robustscaler no sigue una distribuciÃ³n normal (p-valor: 0.0000)\n",
      "La columna BuiltUntilRemod_robustscaler no sigue una distribuciÃ³n normal (p-valor: 0.0000)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, PowerTransformer, QuantileTransformer\n",
    "from scipy.stats import shapiro\n",
    "\n",
    "def select_transformation(data: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Selecciona la transformaciÃ³n apropiada para cada columna numÃ©rica del DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): DataFrame de entrada.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Diccionario con las transformaciones seleccionadas para cada columna.\n",
    "    \"\"\"\n",
    "    transformations = {}\n",
    "    numeric_columns = data.select_dtypes(include=['int64', 'float64']).columns\n",
    "    \n",
    "    for column in numeric_columns:\n",
    "        try:\n",
    "            column_data = data[column].dropna()\n",
    "            \n",
    "            if len(column_data) < 3:\n",
    "                transformations[column] = None\n",
    "                app_logger.info(f\"La columna {column} tiene menos de 3 valores no nulos. No se aplicarÃ¡ transformaciÃ³n.\")\n",
    "                continue\n",
    "            \n",
    "            _, p_value = shapiro(column_data)\n",
    "            skewness = column_data.skew()\n",
    "            unique_count = column_data.nunique()\n",
    "            \n",
    "            if p_value > 0.05:\n",
    "                transformations[column] = StandardScaler()\n",
    "                app_logger.info(f\"Se seleccionÃ³ StandardScaler para la columna {column}\")\n",
    "            elif abs(skewness) > 1:\n",
    "                transformations[column] = PowerTransformer(method='yeo-johnson')\n",
    "                app_logger.info(f\"Se seleccionÃ³ PowerTransformer para la columna {column}\")\n",
    "            elif unique_count < 10:\n",
    "                transformations[column] = None\n",
    "                app_logger.info(f\"La columna {column} tiene menos de 10 valores Ãºnicos. No se aplicarÃ¡ transformaciÃ³n.\")\n",
    "            elif abs(skewness) > 0.5:\n",
    "                transformations[column] = RobustScaler()\n",
    "                app_logger.info(f\"Se seleccionÃ³ RobustScaler para la columna {column}\")\n",
    "            else:\n",
    "                transformations[column] = QuantileTransformer(output_distribution='normal')\n",
    "                app_logger.info(f\"Se seleccionÃ³ QuantileTransformer para la columna {column}\")\n",
    "        except Exception as e:\n",
    "            errors_logger.error(f\"Error al procesar la columna {column}: {str(e)}\")\n",
    "            transformations[column] = None\n",
    "    \n",
    "    return transformations\n",
    "\n",
    "def apply_transformations(data: pd.DataFrame, transformations: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aplica las transformaciones seleccionadas a las columnas del DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): DataFrame de entrada.\n",
    "        transformations (dict): Diccionario con las transformaciones a aplicar.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame con las transformaciones aplicadas.\n",
    "    \"\"\"\n",
    "    transformed_data = data.copy()\n",
    "    \n",
    "    for column, transformer in transformations.items():\n",
    "        try:\n",
    "            if transformer is not None:\n",
    "                transformed_data[column] = transformer.fit_transform(data[[column]])\n",
    "                app_logger.info(f\"Se aplicÃ³ la transformaciÃ³n {type(transformer).__name__} a la columna {column}\")\n",
    "            else:\n",
    "                app_logger.info(f\"No se aplicÃ³ transformaciÃ³n a la columna {column}\")\n",
    "        except Exception as e:\n",
    "            errors_logger.error(f\"Error al aplicar la transformaciÃ³n a la columna {column}: {str(e)}\")\n",
    "            transformed_data[column] = data[column]\n",
    "    \n",
    "    return transformed_data\n",
    "\n",
    "try:\n",
    "    transformations = select_transformation(transformed_data)\n",
    "    transformed_data = apply_transformations(transformed_data, transformations)\n",
    "    app_logger.info(\"Se completÃ³ la selecciÃ³n y aplicaciÃ³n de transformaciones\")\n",
    "except Exception as e:\n",
    "    errors_logger.error(f\"Error general en el proceso de transformaciÃ³n: {str(e)}\")\n",
    "\n",
    "def select_transformation(data: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Selecciona la transformaciÃ³n apropiada para cada columna numÃ©rica del DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): DataFrame de entrada.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Diccionario con las transformaciones seleccionadas para cada columna.\n",
    "    \"\"\"\n",
    "    transformations = {}\n",
    "    numeric_columns = data.select_dtypes(include=['int64', 'float64']).columns\n",
    "    \n",
    "    for column in numeric_columns:\n",
    "        column_data = data[column].dropna()\n",
    "        \n",
    "        if len(column_data) < 3:\n",
    "            transformations[column] = None\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            _, p_value = shapiro(column_data)\n",
    "            skewness = column_data.skew()\n",
    "            unique_count = column_data.nunique()\n",
    "            \n",
    "            if p_value > 0.05:\n",
    "                transformations[column] = StandardScaler()\n",
    "            elif abs(skewness) > 1:\n",
    "                transformations[column] = PowerTransformer(method='yeo-johnson')\n",
    "            elif unique_count < 10:\n",
    "                transformations[column] = None\n",
    "            elif abs(skewness) > 0.5:\n",
    "                transformations[column] = RobustScaler()\n",
    "            else:\n",
    "                transformations[column] = QuantileTransformer(output_distribution='normal')\n",
    "        except Exception as e:\n",
    "            print(f\"Error al procesar la columna {column}: {str(e)}\")\n",
    "            transformations[column] = None\n",
    "    \n",
    "    return transformations\n",
    "\n",
    "def apply_transformations(data: pd.DataFrame, transformations: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aplica las transformaciones seleccionadas a las columnas del DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): DataFrame de entrada.\n",
    "        transformations (dict): Diccionario con las transformaciones a aplicar.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame con las transformaciones aplicadas.\n",
    "    \"\"\"\n",
    "    transformed_data = data.copy()\n",
    "    \n",
    "    for column, transformer in transformations.items():\n",
    "        if transformer is not None:\n",
    "            try:\n",
    "                transformed_column = transformer.fit_transform(data[[column]])\n",
    "                suffix = '_' + transformer.__class__.__name__.lower().replace('transformer', '')\n",
    "                new_column_name = f\"{column}{suffix}\"\n",
    "                transformed_data[new_column_name] = transformed_column\n",
    "            except Exception as e:\n",
    "                print(f\"Error al transformar la columna {column}: {str(e)}\")\n",
    "        else:\n",
    "            transformed_data[column] = data[column]\n",
    "    \n",
    "    return transformed_data\n",
    "\n",
    "def check_normality(data: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Comprueba la normalidad de las columnas numÃ©ricas despuÃ©s de las transformaciones.\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): DataFrame transformado.\n",
    "    \"\"\"\n",
    "    numeric_columns = data.select_dtypes(include=['int64', 'float64']).columns\n",
    "    \n",
    "    for column in numeric_columns:\n",
    "        column_data = data[column].dropna()\n",
    "        \n",
    "        if len(column_data) < 3:\n",
    "            print(f\"La columna {column} tiene menos de 3 valores no nulos.\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            _, p_value = shapiro(column_data)\n",
    "            status = \"sigue\" if p_value > 0.05 else \"no sigue\"\n",
    "            print(f\"La columna {column} {status} una distribuciÃ³n normal (p-valor: {p_value:.4f})\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error al comprobar la normalidad de la columna {column}: {str(e)}\")\n",
    "\n",
    "# Uso del cÃ³digo mejorado\n",
    "try:\n",
    "    transformations = select_transformation(transformed_data)\n",
    "    transformed_data = apply_transformations(transformed_data, transformations)\n",
    "    check_normality(transformed_data)\n",
    "except Exception as e:\n",
    "    print(f\"Error general: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import cross_validate, train_test_split, cross_val_predict\n",
    "from sklearn.metrics import make_scorer, mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "import time\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "scoring = {\n",
    "    'rmse': make_scorer(rmse, greater_is_better=False),\n",
    "    'mse': 'neg_mean_squared_error',\n",
    "    'mae': 'neg_mean_absolute_error',\n",
    "    'r2': 'r2'\n",
    "}\n",
    "\n",
    "def plot_residuals(y_true, y_pred, title, version):\n",
    "    app_logger.info(f'Creando plot de residuos - {title}')\n",
    "    print('Creando plot de residuos')\n",
    "    # GrÃ¡fico de residuos vs valores predichos\n",
    "    residuals = y_true - y_pred\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(x=y_pred, y=residuals)\n",
    "    plt.axhline(y=0, color='r', linestyle='--')\n",
    "    plt.xlabel('Valores predichos')\n",
    "    plt.ylabel('Residuos')\n",
    "    plt.title(f'GrÃ¡fico de Residuos - {title}')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Crear subdirectorio para la versiÃ³n\n",
    "    version_dir = f'../plots/v{version}'\n",
    "    os.makedirs(version_dir, exist_ok=True)\n",
    "    \n",
    "    path = f'{version_dir}/residuos_{title.replace(\" \", \"_\")}.png'\n",
    "    plt.savefig(path)\n",
    "    plt.close()\n",
    "    logger.log_visualization(f'GrÃ¡fico de Residuos - {title}', path)\n",
    "\n",
    "    # DistribuciÃ³n de los residuos\n",
    "    app_logger.info(f'Creando distribuciÃ³n de residuos - {title}')\n",
    "    print('Creando distribuciÃ³n de residuos')\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(residuals, kde=True)\n",
    "    plt.xlabel('Residuos')\n",
    "    plt.ylabel('Frecuencia')\n",
    "    plt.title(f'DistribuciÃ³n de Residuos - {title}')\n",
    "    plt.tight_layout()\n",
    "    path = f'{version_dir}/distribucion_residuos_{title.replace(\" \", \"_\")}.png'\n",
    "    plt.savefig(path)\n",
    "    plt.close()\n",
    "    logger.log_visualization(f'DistribuciÃ³n de Residuos - {title}', path)\n",
    "\n",
    "\n",
    "def plot_regression(y_true, y_pred, title, version):\n",
    "    app_logger.info(f'Creando plot de regresiÃ³n - {title}')\n",
    "    print('Creando plot de regresiÃ³n')\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(x=y_true, y=y_pred)\n",
    "    plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--', lw=2)\n",
    "    plt.xlabel('True values')\n",
    "    plt.ylabel('Predicted values')\n",
    "    plt.title(f'Regression Plot - {title}')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Crear subdirectorio para la versiÃ³n\n",
    "    version_dir = f'../plots/v{version}'\n",
    "    os.makedirs(version_dir, exist_ok=True)\n",
    "    \n",
    "    path = f'{version_dir}/regression_{title.replace(\" \", \"_\")}.png'\n",
    "    plt.savefig(path)\n",
    "    plt.close()\n",
    "    logger.log_visualization(f'Regression Plot - {title}', path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluar_modelo(nombre, modelo, X, y, transformado=False, version=\"1.0\"):\n",
    "    '''\n",
    "    \"\"\"\n",
    "    EvalÃºa un modelo de regresiÃ³n utilizando validaciÃ³n cruzada y mÃ©tricas de rendimiento.\n",
    "\n",
    "    Esta funciÃ³n realiza las siguientes tareas:\n",
    "    1. Ejecuta una validaciÃ³n cruzada de 5 folds en el modelo proporcionado.\n",
    "    2. Entrena el modelo en todo el conjunto de datos.\n",
    "    3. Calcula mÃ©tricas de rendimiento tanto para la validaciÃ³n cruzada como para el conjunto de entrenamiento.\n",
    "    4. Registra los resultados y genera visualizaciones.\n",
    "\n",
    "    ParÃ¡metros:\n",
    "    -----------\n",
    "    ``nombre`` : str\n",
    "        Nombre del modelo a evaluar.\n",
    "    ``modelo`` : objeto estimador de scikit-learn\n",
    "        Modelo de regresiÃ³n a evaluar.\n",
    "    ``X`` : array-like o DataFrame\n",
    "        CaracterÃ­sticas de entrada.\n",
    "    ``y`` : array-like o Series\n",
    "        Variable objetivo.\n",
    "    ``transformado`` : bool, opcional (default=False)\n",
    "        Indica si los datos han sido transformados previamente.\n",
    "\n",
    "    Retorna:\n",
    "    --------\n",
    "    dict\n",
    "        Un diccionario con las mÃ©tricas de rendimiento calculadas.\n",
    "\n",
    "    Efectos secundarios:\n",
    "    --------------------\n",
    "    - Registra los resultados utilizando el logger.\n",
    "    - Genera y guarda grÃ¡ficos de residuos y regresiÃ³n.\n",
    "\n",
    "    Notas:\n",
    "    ------\n",
    "    Las mÃ©tricas calculadas incluyen RMSE, MSE, MAE y R2, tanto para validaciÃ³n cruzada como para el conjunto de entrenamiento completo.\n",
    "    \"\"\"\n",
    "    '''\n",
    "    inicio = time.time()\n",
    "    print(f'Evaluando modelo: {nombre}')\n",
    "    app_logger.info(f'Evaluando modelo: {nombre} en la version - {version}')\n",
    "    try:\n",
    "        \n",
    "        # ValidaciÃ³n cruzada\n",
    "        resultados_cv = cross_validate(modelo, X, y, cv=5, scoring=scoring, n_jobs=-1, return_train_score=True)\n",
    "        \n",
    "        tiempo_ejecucion = time.time() - inicio\n",
    "        \n",
    "        # MÃ©tricas de validaciÃ³n cruzada\n",
    "        rmse_cv = -np.mean(resultados_cv['test_rmse'])\n",
    "        mse_cv = -np.mean(resultados_cv['test_mse'])\n",
    "        mae_cv = -np.mean(resultados_cv['test_mae'])\n",
    "        r2_cv = np.mean(resultados_cv['test_r2'])\n",
    "        \n",
    "        # MÃ©tricas en conjunto de entrenamiento\n",
    "        rmse_train = -np.mean(resultados_cv['train_rmse'])\n",
    "        mse_train = -np.mean(resultados_cv['train_mse'])\n",
    "        mae_train = -np.mean(resultados_cv['train_mae'])\n",
    "        r2_train = np.mean(resultados_cv['train_r2'])\n",
    "        \n",
    "        sufijo = \"transformado\" if transformado else \"sin transformar\"\n",
    "        estudio = f\"RegresiÃ³n simple {nombre} - {sufijo}\"\n",
    "        hiperparams = str(modelo.get_params())\n",
    "        \n",
    "        # Registrar resultados\n",
    "        logger.log_results(estudio, version, nombre, rmse_cv, mse_cv, mae_cv, r2_cv, hiperparams, tiempo_ejecucion)\n",
    "\n",
    "        # Registrar train\n",
    "        logger.log_results(estudio + \" - Train\", version, nombre, rmse_train, mse_train, mae_train, r2_train, hiperparams, tiempo_ejecucion)\n",
    "\n",
    "        #Realizar inferencia (predicciones) con cross_val_predict\n",
    "        \n",
    "        y_pred = cross_val_predict(modelo, X, y, cv=5, n_jobs=-1)\n",
    "        \n",
    "        \n",
    "        # Generar visualizaciones\n",
    "        plot_residuals(y, y_pred, f'{nombre} - {sufijo}', version)\n",
    "        plot_regression(y, y_pred, f'{nombre} - {sufijo}', version)\n",
    "        \n",
    "        return {\n",
    "            'nombre': nombre,\n",
    "            'rmse_cv': rmse_cv,\n",
    "            'mse_cv': mse_cv,\n",
    "            'mae_cv': mae_cv,\n",
    "            'r2_cv': r2_cv,\n",
    "            'rmse_train': rmse_train,\n",
    "            'mse_train': mse_train,\n",
    "            'mae_train': mae_train,\n",
    "            'r2_train': r2_train,\n",
    "            'tiempo': tiempo_ejecucion,\n",
    "            'hiperparams': hiperparams\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        errors_logger.error(f\"Error al evaluar el modelo {nombre}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def evaluar_modelos(X, y, X_transformado, y_transformado, version=\"1.0\"):\n",
    "    modelos = {\n",
    "        'LinearRegression': LinearRegression(n_jobs=-1),\n",
    "        'Ridge': Ridge(random_state=42),\n",
    "        'Lasso': Lasso(random_state=42),\n",
    "        'ElasticNet': ElasticNet(random_state=42)\n",
    "    }\n",
    "    \n",
    "    resultados = []\n",
    "    \n",
    "    for nombre, modelo in modelos.items():\n",
    "        resultado = evaluar_modelo(nombre, modelo, X, y, version=version)\n",
    "        if resultado:\n",
    "            resultados.append(resultado)\n",
    "        \n",
    "        resultado_transformado = evaluar_modelo(nombre, modelo, X_transformado, y_transformado, transformado=True, version=version)\n",
    "        if resultado_transformado:\n",
    "            resultados.append(resultado_transformado)\n",
    "    \n",
    "    return resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados = evaluar_modelos(X, y, transformed_data, y, version=\"1.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1460 entries, 0 to 1459\n",
      "Data columns (total 47 columns):\n",
      " #   Column                        Non-Null Count  Dtype   \n",
      "---  ------                        --------------  -----   \n",
      " 0   OverallQual                   1460 non-null   category\n",
      " 1   GarageArea                    1460 non-null   float64 \n",
      " 2   TotalBsmtSF                   1460 non-null   float64 \n",
      " 3   LotFrontage                   1460 non-null   float64 \n",
      " 4   BedroomAbvGr                  1460 non-null   category\n",
      " 5   YearBuilt                     1460 non-null   float64 \n",
      " 6   YearRemodAdd                  1460 non-null   float64 \n",
      " 7   OverallCond                   1460 non-null   category\n",
      " 8   MasVnrArea                    1460 non-null   float64 \n",
      " 9   TotRmsAbvGrd                  1460 non-null   category\n",
      " 10  GarageYrBlt                   1460 non-null   float64 \n",
      " 11  1stFlrSF                      1460 non-null   float64 \n",
      " 12  YrSold                        1460 non-null   int64   \n",
      " 13  MoSold                        1460 non-null   float64 \n",
      " 14  GrLivArea                     1460 non-null   float64 \n",
      " 15  BuiltUntilSold                1460 non-null   float64 \n",
      " 16  BuiltUntilRemod               1460 non-null   float64 \n",
      " 17  HasBeenRemodeled              1460 non-null   bool    \n",
      " 18  BsmtHalfBath_1                1460 non-null   bool    \n",
      " 19  BsmtHalfBath_2                1460 non-null   bool    \n",
      " 20  GarageCars_1                  1460 non-null   bool    \n",
      " 21  GarageCars_2                  1460 non-null   bool    \n",
      " 22  GarageCars_3                  1460 non-null   bool    \n",
      " 23  GarageCars_4                  1460 non-null   bool    \n",
      " 24  HalfBath_1                    1460 non-null   bool    \n",
      " 25  HalfBath_2                    1460 non-null   bool    \n",
      " 26  Fireplaces_1                  1460 non-null   bool    \n",
      " 27  Fireplaces_2                  1460 non-null   bool    \n",
      " 28  Fireplaces_3                  1460 non-null   bool    \n",
      " 29  FullBath_1                    1460 non-null   bool    \n",
      " 30  FullBath_2                    1460 non-null   bool    \n",
      " 31  FullBath_3                    1460 non-null   bool    \n",
      " 32  BsmtFullBath_1                1460 non-null   bool    \n",
      " 33  BsmtFullBath_2                1460 non-null   bool    \n",
      " 34  BsmtFullBath_3                1460 non-null   bool    \n",
      " 35  GarageArea_power              1460 non-null   float64 \n",
      " 36  TotalBsmtSF_quantile          1460 non-null   float64 \n",
      " 37  LotFrontage_quantile          1460 non-null   float64 \n",
      " 38  YearBuilt_robustscaler        1460 non-null   float64 \n",
      " 39  YearRemodAdd_robustscaler     1460 non-null   float64 \n",
      " 40  MasVnrArea_quantile           1460 non-null   float64 \n",
      " 41  GarageYrBlt_robustscaler      1460 non-null   float64 \n",
      " 42  1stFlrSF_quantile             1460 non-null   float64 \n",
      " 43  MoSold_quantile               1460 non-null   float64 \n",
      " 44  GrLivArea_quantile            1460 non-null   float64 \n",
      " 45  BuiltUntilSold_robustscaler   1460 non-null   float64 \n",
      " 46  BuiltUntilRemod_robustscaler  1460 non-null   float64 \n",
      "dtypes: bool(18), category(4), float64(24), int64(1)\n",
      "memory usage: 318.1 KB\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "transformed_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function flush_figures at 0x000001D75B465790> (for post_execute):\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\envs\\david_pcc\\lib\\site-packages\\matplotlib_inline\\backend_inline.py:126\u001b[0m, in \u001b[0;36mflush_figures\u001b[1;34m()\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m InlineBackend\u001b[38;5;241m.\u001b[39minstance()\u001b[38;5;241m.\u001b[39mclose_figures:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;66;03m# ignore the tracking, just draw and close all figures\u001b[39;00m\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 126\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    128\u001b[0m         \u001b[38;5;66;03m# safely show traceback if in IPython, else raise\u001b[39;00m\n\u001b[0;32m    129\u001b[0m         ip \u001b[38;5;241m=\u001b[39m get_ipython()\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\envs\\david_pcc\\lib\\site-packages\\matplotlib_inline\\backend_inline.py:90\u001b[0m, in \u001b[0;36mshow\u001b[1;34m(close, block)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m figure_manager \u001b[38;5;129;01min\u001b[39;00m Gcf\u001b[38;5;241m.\u001b[39mget_all_fig_managers():\n\u001b[1;32m---> 90\u001b[0m         \u001b[43mdisplay\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfigure_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanvas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fetch_figure_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfigure_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanvas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     95\u001b[0m     show\u001b[38;5;241m.\u001b[39m_to_draw \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\envs\\david_pcc\\lib\\site-packages\\IPython\\core\\display_functions.py:298\u001b[0m, in \u001b[0;36mdisplay\u001b[1;34m(include, exclude, metadata, transient, display_id, raw, clear, *objs, **kwargs)\u001b[0m\n\u001b[0;32m    296\u001b[0m     publish_display_data(data\u001b[38;5;241m=\u001b[39mobj, metadata\u001b[38;5;241m=\u001b[39mmetadata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 298\u001b[0m     format_dict, md_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m format_dict:\n\u001b[0;32m    300\u001b[0m         \u001b[38;5;66;03m# nothing to display (e.g. _ipython_display_ took over)\u001b[39;00m\n\u001b[0;32m    301\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\envs\\david_pcc\\lib\\site-packages\\IPython\\core\\formatters.py:179\u001b[0m, in \u001b[0;36mDisplayFormatter.format\u001b[1;34m(self, obj, include, exclude)\u001b[0m\n\u001b[0;32m    177\u001b[0m md \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mformatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m    181\u001b[0m     \u001b[38;5;66;03m# FIXME: log the exception\u001b[39;00m\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\envs\\david_pcc\\lib\\site-packages\\decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[0;32m    231\u001b[0m     args, kw \u001b[38;5;241m=\u001b[39m fix(args, kw, sig)\n\u001b[1;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcaller\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mextras\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\envs\\david_pcc\\lib\\site-packages\\IPython\\core\\formatters.py:223\u001b[0m, in \u001b[0;36mcatch_format_error\u001b[1;34m(method, self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"show traceback on failed format call\"\"\"\u001b[39;00m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 223\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:\n\u001b[0;32m    225\u001b[0m     \u001b[38;5;66;03m# don't warn on NotImplementedErrors\u001b[39;00m\n\u001b[0;32m    226\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_return(\u001b[38;5;28;01mNone\u001b[39;00m, args[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\envs\\david_pcc\\lib\\site-packages\\IPython\\core\\formatters.py:340\u001b[0m, in \u001b[0;36mBaseFormatter.__call__\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    338\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 340\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprinter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;66;03m# Finally look for special method names\u001b[39;00m\n\u001b[0;32m    342\u001b[0m method \u001b[38;5;241m=\u001b[39m get_real_method(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_method)\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\envs\\david_pcc\\lib\\site-packages\\IPython\\core\\pylabtools.py:152\u001b[0m, in \u001b[0;36mprint_figure\u001b[1;34m(fig, fmt, bbox_inches, base64, **kwargs)\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend_bases\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FigureCanvasBase\n\u001b[0;32m    150\u001b[0m     FigureCanvasBase(fig)\n\u001b[1;32m--> 152\u001b[0m \u001b[43mfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanvas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprint_figure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbytes_io\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    153\u001b[0m data \u001b[38;5;241m=\u001b[39m bytes_io\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fmt \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msvg\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\envs\\david_pcc\\lib\\site-packages\\matplotlib\\backend_bases.py:2342\u001b[0m, in \u001b[0;36mFigureCanvasBase.print_figure\u001b[1;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[0;32m   2336\u001b[0m     renderer \u001b[38;5;241m=\u001b[39m _get_renderer(\n\u001b[0;32m   2337\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure,\n\u001b[0;32m   2338\u001b[0m         functools\u001b[38;5;241m.\u001b[39mpartial(\n\u001b[0;32m   2339\u001b[0m             print_method, orientation\u001b[38;5;241m=\u001b[39morientation)\n\u001b[0;32m   2340\u001b[0m     )\n\u001b[0;32m   2341\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(renderer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_draw_disabled\u001b[39m\u001b[38;5;124m\"\u001b[39m, nullcontext)():\n\u001b[1;32m-> 2342\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2344\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bbox_inches:\n\u001b[0;32m   2345\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m bbox_inches \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtight\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\envs\\david_pcc\\lib\\site-packages\\matplotlib\\artist.py:95\u001b[0m, in \u001b[0;36m_finalize_rasterization.<locals>.draw_wrapper\u001b[1;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(draw)\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdraw_wrapper\u001b[39m(artist, renderer, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 95\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m renderer\u001b[38;5;241m.\u001b[39m_rasterizing:\n\u001b[0;32m     97\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstop_rasterizing()\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\envs\\david_pcc\\lib\\site-packages\\matplotlib\\artist.py:72\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[1;34m(artist, renderer)\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     70\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstart_filter()\n\u001b[1;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\envs\\david_pcc\\lib\\site-packages\\matplotlib\\figure.py:3175\u001b[0m, in \u001b[0;36mFigure.draw\u001b[1;34m(self, renderer)\u001b[0m\n\u001b[0;32m   3172\u001b[0m         \u001b[38;5;66;03m# ValueError can occur when resizing a window.\u001b[39;00m\n\u001b[0;32m   3174\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch\u001b[38;5;241m.\u001b[39mdraw(renderer)\n\u001b[1;32m-> 3175\u001b[0m \u001b[43mmimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_draw_list_compositing_images\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3176\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43martists\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuppressComposite\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3178\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sfig \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubfigs:\n\u001b[0;32m   3179\u001b[0m     sfig\u001b[38;5;241m.\u001b[39mdraw(renderer)\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\envs\\david_pcc\\lib\\site-packages\\matplotlib\\image.py:131\u001b[0m, in \u001b[0;36m_draw_list_compositing_images\u001b[1;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m not_composite \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_images:\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m artists:\n\u001b[1;32m--> 131\u001b[0m         \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;66;03m# Composite any adjacent images together\u001b[39;00m\n\u001b[0;32m    134\u001b[0m     image_group \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\envs\\david_pcc\\lib\\site-packages\\matplotlib\\artist.py:72\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[1;34m(artist, renderer)\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     70\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstart_filter()\n\u001b[1;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\envs\\david_pcc\\lib\\site-packages\\matplotlib\\axes\\_base.py:3064\u001b[0m, in \u001b[0;36m_AxesBase.draw\u001b[1;34m(self, renderer)\u001b[0m\n\u001b[0;32m   3061\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m artists_rasterized:\n\u001b[0;32m   3062\u001b[0m     _draw_rasterized(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure, artists_rasterized, renderer)\n\u001b[1;32m-> 3064\u001b[0m \u001b[43mmimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_draw_list_compositing_images\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3065\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43martists\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuppressComposite\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3067\u001b[0m renderer\u001b[38;5;241m.\u001b[39mclose_group(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maxes\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   3068\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\envs\\david_pcc\\lib\\site-packages\\matplotlib\\image.py:131\u001b[0m, in \u001b[0;36m_draw_list_compositing_images\u001b[1;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m not_composite \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_images:\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m artists:\n\u001b[1;32m--> 131\u001b[0m         \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;66;03m# Composite any adjacent images together\u001b[39;00m\n\u001b[0;32m    134\u001b[0m     image_group \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\envs\\david_pcc\\lib\\site-packages\\matplotlib\\artist.py:72\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[1;34m(artist, renderer)\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     70\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstart_filter()\n\u001b[1;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\envs\\david_pcc\\lib\\site-packages\\matplotlib\\patches.py:590\u001b[0m, in \u001b[0;36mPatch.draw\u001b[1;34m(self, renderer)\u001b[0m\n\u001b[0;32m    588\u001b[0m transform \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_transform()\n\u001b[0;32m    589\u001b[0m tpath \u001b[38;5;241m=\u001b[39m transform\u001b[38;5;241m.\u001b[39mtransform_path_non_affine(path)\n\u001b[1;32m--> 590\u001b[0m affine \u001b[38;5;241m=\u001b[39m \u001b[43mtransform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_affine\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_draw_paths_with_artist_properties(\n\u001b[0;32m    592\u001b[0m     renderer,\n\u001b[0;32m    593\u001b[0m     [(tpath, affine,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    596\u001b[0m       \u001b[38;5;66;03m# transparent, but do if it is None.\u001b[39;00m\n\u001b[0;32m    597\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_facecolor \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_facecolor[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)])\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\envs\\david_pcc\\lib\\site-packages\\matplotlib\\transforms.py:2450\u001b[0m, in \u001b[0;36mCompositeGenericTransform.get_affine\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2447\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_b\u001b[38;5;241m.\u001b[39mget_affine()\n\u001b[0;32m   2448\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Affine2D(np\u001b[38;5;241m.\u001b[39mdot(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_b\u001b[38;5;241m.\u001b[39mget_affine()\u001b[38;5;241m.\u001b[39mget_matrix(),\n\u001b[1;32m-> 2450\u001b[0m                            \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_a\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_affine\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mget_matrix()))\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\envs\\david_pcc\\lib\\site-packages\\matplotlib\\transforms.py:2449\u001b[0m, in \u001b[0;36mCompositeGenericTransform.get_affine\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2447\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_b\u001b[38;5;241m.\u001b[39mget_affine()\n\u001b[0;32m   2448\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mAffine2D\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_b\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_affine\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2450\u001b[0m \u001b[43m                           \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_a\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_affine\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\envs\\david_pcc\\lib\\site-packages\\matplotlib\\transforms.py:1903\u001b[0m, in \u001b[0;36mAffine2D.__init__\u001b[1;34m(self, matrix, **kwargs)\u001b[0m\n\u001b[0;32m   1900\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m matrix \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1901\u001b[0m     \u001b[38;5;66;03m# A bit faster than np.identity(3).\u001b[39;00m\n\u001b[0;32m   1902\u001b[0m     matrix \u001b[38;5;241m=\u001b[39m IdentityTransform\u001b[38;5;241m.\u001b[39m_mtx\n\u001b[1;32m-> 1903\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mtx \u001b[38;5;241m=\u001b[39m \u001b[43mmatrix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1904\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_invalid \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## GeneraciÃ³n de nuevas caracterÃ­sticas y evaluaciÃ³n de modelos\n",
    "# FunciÃ³n para aÃ±adir nuevas caracterÃ­sticas\n",
    "def aÃ±adir_nuevas_caracteristicas(df):\n",
    "    try:\n",
    "        nuevas_columnas = {}\n",
    "        \n",
    "        for col in df.select_dtypes(include=['int64', 'float64']).columns:\n",
    "            nuevas_columnas[f'{col}_squared'] = np.square(df[col])\n",
    "            \n",
    "            if (df[col] >= 0).all():\n",
    "                nuevas_columnas[f'{col}_log'] = np.log1p(df[col])\n",
    "                nuevas_columnas[f'{col}_sqrt'] = np.sqrt(df[col])\n",
    "            \n",
    "            # AÃ±adir el producto de columnas\n",
    "            for col2 in df.select_dtypes(include=['int64', 'float64']).columns:\n",
    "                if col != col2:\n",
    "                    nuevas_columnas[f'{col}_x_{col2}'] = df[col] * df[col2]\n",
    "                    \n",
    "        df_new = pd.concat([df, pd.DataFrame(nuevas_columnas)], axis=1)\n",
    "        app_logger.info(f\"Se han aÃ±adido {len(nuevas_columnas)} nuevas caracterÃ­sticas.\")\n",
    "        return df_new\n",
    "    except Exception as e:\n",
    "        app_logger.error(f\"Error al aÃ±adir nuevas caracterÃ­sticas: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "try:\n",
    "    # Aplicar las nuevas caracterÃ­sticas a ambos datasets\n",
    "    X_new = aÃ±adir_nuevas_caracteristicas(X.copy())\n",
    "    transformed_data_new = aÃ±adir_nuevas_caracteristicas(transformed_data.copy())\n",
    "\n",
    "    # Dividir los nuevos datasets\n",
    "    X_train_new, X_test_new, y_train_new, y_test_new = train_test_split(X_new, y, test_size=0.2, random_state=42)\n",
    "    X_train_transformed_new, X_test_transformed_new, y_train_transformed_new, y_test_transformed_new = train_test_split(transformed_data_new, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Evaluar los modelos con los nuevos datasets\n",
    "    resultados_nuevos = evaluar_modelos(X_train_new, y_train_new, X_train_transformed_new, y_train_transformed_new, version=\"2.0 - Polynomial Features\")\n",
    "\n",
    "except Exception as e:\n",
    "    app_logger.error(f\"Error en el proceso principal: {str(e)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Possible Features\n",
    "- Based on ``YearRemodAdd`` and ``YearBuilt`` as the dataset description stated, if they are equal that means that the house is not remodeled and if its different means that it has been remodeled, we can add a binary feature indicating this.\n",
    "- Reduce ``YearBuilt`` and ``YrSold`` to ``TimeToSell``.\n",
    "- Convert ``YearRemodAdd`` to ``TimeUntilRemod`` that means the time since it was built until it was remod, and ``RemodUntilSale`` that is the time since it was remod until it was sold.\n",
    "- Porch and Deck Areas: Create a total porch area feature and a binary indicator for houses with porches.\n",
    "- Proximity and Neighborhood Effects: Group neighborhoods into clusters based on median house prices to capture locality effects.\n",
    "- GeoCode neighborhoods.\n",
    "- Total Square Footage: Combine all square footage features (``1stFlrSF``, ``2ndFlrSF``, ``TotalBsmtSF``, etc.) into a single feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inconsistencies\n",
    "- In some cases ``GarageYrBlt`` (year that the garage was built) was previous to ``YearBuilt`` which is not logical. We can modify this cases and transform those values to the year that the house was built assumming this criterion. This variable is related mostly with the built year and the Remodelation year that we can discard it as it only adds complexity with no info to the dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finrl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
