{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from lazypredict.Supervised import LazyRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import xgboost as xgb\n",
    "\n",
    "# Cargar los datos\n",
    "data = pd.read_csv('../data/FS_final_train.csv', index_col=0)\n",
    "\n",
    "# Separar las características y la variable objetivo\n",
    "X = data.drop(columns=['log_SalePrice'])\n",
    "y = data['log_SalePrice']\n",
    "\n",
    "# Estandarizar las características\n",
    "scaler = StandardScaler()\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns, index=X.index)\n",
    "X_scaled.dropna(inplace=True)\n",
    "\n",
    "#Eliminar la fila eliminada en y tambien en X\n",
    "dropped_rows = y.index.difference(X_scaled.index)\n",
    "y_scaled = y.drop(index=dropped_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # De las correlaciones altas nos quedamos con las que tienen mas correlacion con la target\n",
    "# corr_matrix = X_scaled.corr()\n",
    "\n",
    "# #Eliminar las diagonales\n",
    "# corr_matrix = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "# corr_matrix = corr_matrix.stack().reset_index()\n",
    "# corr_matrix.columns = ['Feature 1', 'Feature 2', 'Correlation']\n",
    "\n",
    "# # De las parejas con abs(correlacion > 0.9) nos quedamos con la que tiene mas correlacion con la target\n",
    "# corr_matrix = corr_matrix[corr_matrix['Correlation'].abs() > 0.9]\n",
    "# corr_matrix = corr_matrix.sort_values(by='Correlation', ascending=False)\n",
    "\n",
    "# # de la pareja eliminar la que menos corr tenga con la target y, usar corrwith\n",
    "# features_to_drop = []\n",
    "# for index, row in corr_matrix.iterrows():\n",
    "#     feature1 = row['Feature 1']\n",
    "#     feature2 = row['Feature 2']\n",
    "#     if y.corr(X[feature1]) < y.corr(X[feature2]):\n",
    "#         features_to_drop.append(feature1)\n",
    "#     else:\n",
    "#         features_to_drop.append(feature2)\n",
    "\n",
    "# X_scaled = X_scaled.drop(columns=features_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After loading and preprocessing the data\n",
    "print(\"X shape:\", X_scaled.shape)\n",
    "print(\"y shape:\", y_scaled.shape)\n",
    "print(\"X info:\")\n",
    "print(X.info())\n",
    "print(\"\\nX describe:\")\n",
    "print(X.describe())\n",
    "print(\"\\ny describe:\")\n",
    "print(y.describe())\n",
    "\n",
    "# Check for NaN or infinite values\n",
    "print(\"\\nNaN in X:\", X_scaled.isna().sum().sum())\n",
    "print(\"NaN in y:\", y_scaled.isna().sum())\n",
    "print(\"Inf in X:\", np.isinf(X_scaled).sum().sum())\n",
    "print(\"Inf in y:\", np.isinf(y_scaled).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_rmse_scorer(y_true, y_pred):\n",
    "    try:\n",
    "        mse = mean_squared_error(y_true, y_pred)\n",
    "        if np.isnan(mse) or np.isinf(mse):\n",
    "            print(f\"MSE is {mse}\")\n",
    "            print(f\"y_true: min={np.min(y_true)}, max={np.max(y_true)}, mean={np.mean(y_true)}\")\n",
    "            print(f\"y_pred: min={np.min(y_pred)}, max={np.max(y_pred)}, mean={np.mean(y_pred)}\")\n",
    "        return np.sqrt(mse)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in RMSE calculation: {e}\")\n",
    "        print(f\"y_true shape: {y_true.shape}, y_pred shape: {y_pred.shape}\")\n",
    "        print(f\"y_true: {y_true[:5]}, y_pred: {y_pred[:5]}\")\n",
    "        return np.nan\n",
    "\n",
    "rmse_scorer = make_scorer(custom_rmse_scorer, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_oversample_data(X, y, percentage=0.2, noise_level=0.01):\n",
    "    #Random seed\n",
    "    random_seed = 42\n",
    "\n",
    "    np.random.seed(random_seed)\n",
    "    # Seleccionar aleatoriamente un porcentaje de los datos\n",
    "    sample_size = int(len(y) * percentage)\n",
    "    random_indices = np.random.choice(y.index, size=sample_size, replace=True)\n",
    "    \n",
    "    # Crear muestras con ruido\n",
    "    X_sampled = X.loc[random_indices]\n",
    "    y_sampled = y.loc[random_indices] + np.random.normal(0, noise_level, size=sample_size)\n",
    "    \n",
    "    # Combinar las muestras originales con las sobremuestreadas\n",
    "    X_resampled = pd.concat([X, X_sampled], axis=0)\n",
    "    y_resampled = pd.concat([y, y_sampled], axis=0)\n",
    "    \n",
    "    \n",
    "    return X_resampled, y_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import xgboost as xgb\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Configuración comn de Optuna\n",
    "def create_study(name, version=None):\n",
    "    return optuna.create_study(study_name=f'{name}_housing_{version}', directions = ['minimize', 'minimize'],\n",
    "                               sampler=optuna.samplers.TPESampler(), pruner=optuna.pruners.MedianPruner(n_startup_trials=10, n_min_trials=2),\n",
    "                               load_if_exists=True, storage=f'sqlite:///../models/{name}_housing_{version}.db')\n",
    "def objective_gb(trial):\n",
    "    # Parámetros para decidir si aplicar sobremuestreo y filtrado de correlación\n",
    "    do_oversampling = trial.suggest_categorical('do_oversampling', [True, False])\n",
    "\n",
    "    # Definición de parámetros para GradientBoostingRegressor\n",
    "    params = {\n",
    "        'loss': trial.suggest_categorical('loss', ['squared_error']),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 1.0),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
    "        'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n",
    "        'criterion': trial.suggest_categorical('criterion', ['friedman_mse', 'squared_error']),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 20),\n",
    "        'min_weight_fraction_leaf': trial.suggest_uniform('min_weight_fraction_leaf', 0.0, 0.5),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 40),\n",
    "        'min_impurity_decrease': trial.suggest_uniform('min_impurity_decrease', 0.0, 1.0),\n",
    "        'init': None,\n",
    "        'random_state': 42,\n",
    "        'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),\n",
    "        'alpha': trial.suggest_uniform('alpha', 0.0, 1.0),\n",
    "        'verbose': 0,\n",
    "        'max_leaf_nodes': trial.suggest_int('max_leaf_nodes', 2, 20),\n",
    "        'warm_start': False,\n",
    "        'validation_fraction': 0.2,\n",
    "        'n_iter_no_change': 100,\n",
    "        'tol': 1e-4,\n",
    "        'ccp_alpha': trial.suggest_uniform('ccp_alpha', 0.0, 0.1)\n",
    "    }\n",
    "    \n",
    "    model = GradientBoostingRegressor(**params)\n",
    "    \n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    val_rmses = []\n",
    "    train_rmses = []\n",
    "    \n",
    "    # Aplicar filtrado de correlación si se selecciona\n",
    "    \n",
    "    for train_index, val_index in kf.split(X_train):\n",
    "        X_train_val, X_val = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "        y_train_val, y_val = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "\n",
    "        # Aplicar sobremuestreo si se selecciona\n",
    "        if do_oversampling:\n",
    "            X_train_val, y_train_val = custom_oversample_data(X_train_val, y_train_val)\n",
    "        \n",
    "        model.fit(X_train_val, y_train_val)\n",
    "        val_pred = model.predict(X_val)\n",
    "        train_pred = model.predict(X_train_val)\n",
    "        \n",
    "        val_rmse = custom_rmse_scorer(y_val, val_pred)\n",
    "        train_rmse = custom_rmse_scorer(y_train_val, train_pred)\n",
    "        \n",
    "        val_rmses.append(val_rmse)\n",
    "        train_rmses.append(train_rmse)\n",
    "    \n",
    "    val_rmse = np.mean(val_rmses)\n",
    "    train_rmse = np.mean(train_rmses)\n",
    "    \n",
    "    print(f\"Val RMSE: {val_rmse}, Train RMSE: {train_rmse}\")\n",
    "    \n",
    "    rmse_diff = val_rmse - train_rmse\n",
    "    \n",
    "    return val_rmse, rmse_diff\n",
    "\n",
    "study_gb = create_study('gb_FS_Oversampling', version='2.3')\n",
    "study_gb.optimize(objective_gb, n_trials=100, show_progress_bar=True, gc_after_trial=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Direcciones de optimización: {study_gb.directions}')\n",
    "print(f'Nmero de ensayos: {study_gb.trials.__len__()}')\n",
    "# print(f'Mejor ensayo: {study_gb.best_trial}') solo se puede utilizar si la optimización es simple no multiple\n",
    "# print(f'Mejor valor: {study_gb.best_value}') solo se puede utilizar si la optimización es simple no multiple\n",
    "# print(f'Mejor hiperparámetros: {study_gb.best_params}') solo se puede utilizar si la optimización es simple no multiple\n",
    "study_gb_df = study_gb.trials_dataframe()\n",
    "study_gb_df.sort_values(by='values_0', ascending=True).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_best_params(study_gb, weights:np.ndarray) -> dict:\n",
    "    '''\n",
    "    Obtenemos los best params atraves de asociar un peso deseado a cada métrica que estamos optimizando\n",
    "    -------\n",
    "    Parámetros:\n",
    "    - study_xgb: optuna.Study, estudio de Optuna.\n",
    "    - weights: np.ndarray, pesos deseados para cada métrica.\n",
    "    -------\n",
    "    Devuelve:\n",
    "    - best_params: dict, hiperparámetros óptimos.\n",
    "    '''\n",
    "    study_df = study_gb.trials_dataframe()\n",
    "    study_df['weighted_average'] = np.average(study_df[[col for col in study_df.columns if 'values' in col]], weights=weights, axis=1)\n",
    "\n",
    "    # Encontrar el trial con el menor promedio ponderado\n",
    "    best_trial_index = study_df['weighted_average'].idxmin()\n",
    "    best_trial = study_df.loc[best_trial_index]\n",
    "\n",
    "    best_trial = [trial for trial in study_gb.best_trials if trial.number == best_trial_index]\n",
    "    best_params = best_trial[0].params\n",
    "    return best_params, best_trial[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params, best_values = obtain_best_params(study_gb, weights=np.array([0.75, 0.25]))\n",
    "print(f'Mejores hiperparámetros: {best_params}')\n",
    "print(f'Mejores valores: Mean rmse Validation: {best_values[0]}, Mean Diff (Val - Train) rmse: {best_values[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Entrenar el modelo con los mejores hiperparámetros encontrados\n",
    "# Eliminar do_oversampling\n",
    "do_oversampling = best_params.pop('do_oversampling')\n",
    "model = GradientBoostingRegressor(random_state=42, **best_params)\n",
    "\n",
    "if do_oversampling:\n",
    "    X_train_over, y_train_over = custom_oversample_data(X_train, y_train)\n",
    "    model.fit(X_train_over, y_train_over)\n",
    "else:\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "# Predecir sobre el conjunto de prueba\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluar todas las métricas de regresión\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'RMSE: {rmse}')\n",
    "print(f'MAE: {mae}')\n",
    "print(f'R^2 Score: {r2}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "david_pcc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
