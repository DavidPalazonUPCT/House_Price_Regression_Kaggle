{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from lazypredict.Supervised import LazyRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import xgboost as xgb\n",
    "\n",
    "# Cargar los datos\n",
    "data = pd.read_csv('../data/final_train.csv', index_col=0)\n",
    "\n",
    "# Separar las características y la variable objetivo\n",
    "X = data.drop(columns=['log_SalePrice'])\n",
    "y = data['log_SalePrice']\n",
    "\n",
    "def determinar_tipo_feature(df):\n",
    "    tipos = {}\n",
    "    df_mod = df.copy()\n",
    "    for col in df.columns:\n",
    "        unique_values = df[col].nunique()\n",
    "        if unique_values == 2:\n",
    "            tipos[col] = 'binaria'\n",
    "            df_mod[col] = df_mod[col].astype('category')\n",
    "        elif unique_values <= 20:\n",
    "            tipos[col] = 'categorica'\n",
    "            df_mod[col] = df_mod[col].astype('category')\n",
    "        else:\n",
    "            tipos[col] = 'continua'\n",
    "            df_mod[col] = df_mod[col].astype(np.float64)\n",
    "    return tipos, df_mod\n",
    "\n",
    "tipos_features, X = determinar_tipo_feature(X)\n",
    "tipos_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.drop(columns=['3SsnPorch'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estandarizar las características continuas\n",
    "# Estandarizar solo las características continuas\n",
    "scaler = StandardScaler()\n",
    "columnas_continuas = [col for col, tipo in tipos_features.items() if tipo == 'continua']\n",
    "X_scaled = X.copy()\n",
    "X_scaled[columnas_continuas] = scaler.fit_transform(X[columnas_continuas])\n",
    "\n",
    "#Eliminar la fila eliminada en y tambien en X\n",
    "X_scaled = X_scaled.dropna()\n",
    "dropped_rows = y.index.difference(X_scaled.index)\n",
    "y_scaled = y.drop(index=dropped_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # De las correlaciones altas nos quedamos con las que tienen mas correlacion con la target\n",
    "# corr_matrix = X_scaled.corr()\n",
    "\n",
    "# #Eliminar las diagonales\n",
    "# corr_matrix = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "# corr_matrix = corr_matrix.stack().reset_index()\n",
    "# corr_matrix.columns = ['Feature 1', 'Feature 2', 'Correlation']\n",
    "\n",
    "# # De las parejas con abs(correlacion > 0.9) nos quedamos con la que tiene mas correlacion con la target\n",
    "# corr_matrix = corr_matrix[corr_matrix['Correlation'].abs() > 0.9]\n",
    "# corr_matrix = corr_matrix.sort_values(by='Correlation', ascending=False)\n",
    "\n",
    "# # de la pareja eliminar la que menos corr tenga con la target y, usar corrwith\n",
    "# features_to_drop = []\n",
    "# for index, row in corr_matrix.iterrows():\n",
    "#     feature1 = row['Feature 1']\n",
    "#     feature2 = row['Feature 2']\n",
    "#     if y.corr(X[feature1]) < y.corr(X[feature2]):\n",
    "#         features_to_drop.append(feature1)\n",
    "#     else:\n",
    "#         features_to_drop.append(feature2)\n",
    "\n",
    "# X_scaled = X_scaled.drop(columns=features_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After loading and preprocessing the data\n",
    "print(\"X shape:\", X_scaled.shape)\n",
    "print(\"y shape:\", y_scaled.shape)\n",
    "print(\"X info:\")\n",
    "print(X.info())\n",
    "print(\"\\nX describe:\")\n",
    "print(X.describe())\n",
    "print(\"\\ny describe:\")\n",
    "print(y.describe())\n",
    "\n",
    "# Check for NaN or infinite values\n",
    "print(\"\\nNaN in X:\", X_scaled.isna().sum().sum())\n",
    "print(\"NaN in y:\", y_scaled.isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_rmse_scorer(y_true, y_pred):\n",
    "    try:\n",
    "        mse = mean_squared_error(y_true, y_pred)\n",
    "        if np.isnan(mse) or np.isinf(mse):\n",
    "            print(f\"MSE is {mse}\")\n",
    "            print(f\"y_true: min={np.min(y_true)}, max={np.max(y_true)}, mean={np.mean(y_true)}\")\n",
    "            print(f\"y_pred: min={np.min(y_pred)}, max={np.max(y_pred)}, mean={np.mean(y_pred)}\")\n",
    "        return np.sqrt(mse)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in RMSE calculation: {e}\")\n",
    "        print(f\"y_true shape: {y_true.shape}, y_pred shape: {y_pred.shape}\")\n",
    "        print(f\"y_true: {y_true[:5]}, y_pred: {y_pred[:5]}\")\n",
    "        return np.nan\n",
    "\n",
    "rmse_scorer = make_scorer(custom_rmse_scorer, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_oversample_data(X, y:pd.Series, percentage=0.2, noise_level=1):\n",
    "    #Random seed\n",
    "    random_seed = 42\n",
    "\n",
    "    np.random.seed(random_seed)\n",
    "    y = y.apply(lambda x: np.exp(x))\n",
    "\n",
    "    # Seleccionar de la mediana hacia arriba para hacer el oversampling\n",
    "    median_value = y.median()\n",
    "    y_median = y[y >= median_value]\n",
    "\n",
    "    # Seleccionar aleatoriamente un porcentaje de los datos\n",
    "    sample_size = int(len(y_median) * percentage)\n",
    "    random_indices = np.random.choice(y_median.index, size=sample_size, replace=True)\n",
    "        \n",
    "\n",
    "    # Crear muestras con ruido\n",
    "    X_sampled = X.loc[random_indices]\n",
    "    y_sampled = y_median.loc[random_indices] + np.random.normal(0, noise_level, size=sample_size)\n",
    "    \n",
    "    # Combinar las muestras originales con las sobremuestreadas\n",
    "    X_resampled = pd.concat([X, X_sampled], axis=0)\n",
    "    y_resampled = pd.concat([y.apply(lambda x: np.log(x)), y_sampled.apply(lambda x: np.log(x))], axis=0)\n",
    "    \n",
    "    \n",
    "    return X_resampled, y_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eliminar_outliers(x, umbral=1.5):\n",
    "    \"\"\"\n",
    "    Elimina outliers de un DataFrame basado en el rango intercuartil (IQR).\n",
    "    \n",
    "    Parámetros:\n",
    "    df (pd.DataFrame): DataFrame de entrada.\n",
    "    columna (str): Nombre de la columna en la que se eliminarán los outliers.\n",
    "    umbral (float): Factor multiplicativo para el IQR. Los valores fuera de [Q1 - umbral*IQR, Q3 + umbral*IQR] serán considerados outliers.\n",
    "    \n",
    "    Retorna:\n",
    "    pd.DataFrame: DataFrame sin outliers en la columna especificada.\n",
    "    \"\"\"\n",
    "    Q1 = x.quantile(0.25)\n",
    "    Q3 = x.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    filtro = (x >= Q1 - umbral * IQR) & (x <= Q3 + umbral * IQR)\n",
    "    return x[filtro]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)\n",
    "categorical_features = X_train.columns[X_train.dtypes == 'category']\n",
    "\n",
    "# Configuración comn de Optuna\n",
    "def create_study(name, version=None):\n",
    "    return optuna.create_study(study_name=f'{name}_housing_{version}', directions = ['minimize', 'minimize'],\n",
    "                               sampler=optuna.samplers.TPESampler(), pruner=optuna.pruners.MedianPruner(n_startup_trials=10, n_min_trials=2),\n",
    "                               load_if_exists=True, storage=f'sqlite:///../models/{name}_housing_{version}.db')\n",
    "\n",
    "def objective_hgb(trial):\n",
    "    # Definición de parámetros para HGB\n",
    "    do_oversampling = trial.suggest_categorical('do_oversampling', [True, False])\n",
    "    \n",
    "    params = {\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 1.0),\n",
    "        'max_iter': trial.suggest_int('max_iter', 50, 500),\n",
    "        'max_leaf_nodes': trial.suggest_int('max_leaf_nodes', 2, 50),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 10, 100),\n",
    "        'l2_regularization': trial.suggest_loguniform('l2_regularization', 1e-4, 1.0),\n",
    "        'max_bins': trial.suggest_int('max_bins', 100, 255),\n",
    "        'early_stopping': trial.suggest_categorical('early_stopping', [True, False]),\n",
    "        'validation_fraction': 0.1,\n",
    "        'n_iter_no_change': 50,\n",
    "    }\n",
    "    \n",
    "    base_models = [\n",
    "        ('hgb_1', HistGradientBoostingRegressor(random_state=42, categorical_features=categorical_features, **params)),\n",
    "        ('hgb_2', HistGradientBoostingRegressor(random_state=43, categorical_features=categorical_features, **params)),\n",
    "        ('hgb_3', HistGradientBoostingRegressor(random_state=44, categorical_features=categorical_features, **params))\n",
    "    ]\n",
    "\n",
    "    meta_model = LinearRegression(n_jobs=-1)\n",
    "\n",
    "    model = StackingRegressor(estimators=base_models, final_estimator=meta_model, n_jobs=-1)\n",
    "    \n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    val_rmses = []\n",
    "    train_rmses = []\n",
    "    \n",
    "    for train_index, val_index in kf.split(X_train):\n",
    "        X_train_val, X_val = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "        y_train_val, y_val = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "        \n",
    "        if do_oversampling:\n",
    "            X_train_val_over, y_train_val_over = custom_oversample_data(X_train_val, y_train_val)\n",
    "            y_train_val_over = eliminar_outliers(y_train_val_over)\n",
    "            dropped_rows = X_train_val_over.index.difference(y_train_val_over.index)\n",
    "            X_train_val_over = X_train_val_over.drop(index=dropped_rows)\n",
    "            \n",
    "            model.fit(X_train_val_over, y_train_val_over)\n",
    "        else:\n",
    "\n",
    "            y_train_val = eliminar_outliers(y_train_val)\n",
    "\n",
    "            dropped_rows = X_train_val.index.difference(y_train_val.index)\n",
    "            X_train_val = X_train_val.drop(index=dropped_rows)\n",
    "\n",
    "            model.fit(X_train_val, y_train_val)\n",
    "\n",
    "        val_pred = model.predict(X_val)\n",
    "        train_pred = model.predict(X_train_val)\n",
    "        \n",
    "        val_rmse = custom_rmse_scorer(y_val, val_pred)\n",
    "        train_rmse = custom_rmse_scorer(y_train_val, train_pred)\n",
    "        \n",
    "        val_rmses.append(val_rmse)\n",
    "        train_rmses.append(train_rmse)\n",
    "    \n",
    "    val_rmse = np.mean(val_rmses)\n",
    "    train_rmse = np.mean(train_rmses)\n",
    "    \n",
    "    print(f\"Val RMSE: {val_rmse}, Train RMSE: {train_rmse}\")\n",
    "    \n",
    "    rmse_diff = val_rmse - train_rmse\n",
    "    \n",
    "    return val_rmse, rmse_diff\n",
    "\n",
    "study_hgb = create_study('hgb_no_fs_Oversampling_stacking', version='1.0')\n",
    "study_hgb.optimize(objective_hgb, n_trials=100, show_progress_bar=True, gc_after_trial=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Direcciones de optimización: {study_hgb.directions}')\n",
    "print(f'Nmero de ensayos: {study_hgb.trials.__len__()}')\n",
    "# print(f'Mejor ensayo: {study_hgb.best_trial}') solo se puede utilizar si la optimización es simple no multiple\n",
    "# print(f'Mejor valor: {study_hgb.best_value}') solo se puede utilizar si la optimización es simple no multiple\n",
    "# print(f'Mejor hiperparámetros: {study_hgb.best_params}') solo se puede utilizar si la optimización es simple no multiple\n",
    "study_hgb_df = study_hgb.trials_dataframe()\n",
    "study_hgb_df.sort_values(by='values_0', ascending=True).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_best_params(study_hgb, weights:np.ndarray) -> dict:\n",
    "    '''\n",
    "    Obtenemos los best params atraves de asociar un peso deseado a cada métrica que estamos optimizando\n",
    "    -------\n",
    "    Parámetros:\n",
    "    - study_xgb: optuna.Study, estudio de Optuna.\n",
    "    - weights: np.ndarray, pesos deseados para cada métrica.\n",
    "    -------\n",
    "    Devuelve:\n",
    "    - best_params: dict, hiperparámetros óptimos.\n",
    "    '''\n",
    "    study_df = study_hgb.trials_dataframe()\n",
    "    study_df['weighted_average'] = np.average(study_df[[col for col in study_df.columns if 'values' in col]], weights=weights, axis=1)\n",
    "\n",
    "    # Encontrar el trial con el menor promedio ponderado\n",
    "    best_trial_index = study_df['weighted_average'].idxmin()\n",
    "    best_trial = study_df.loc[best_trial_index]\n",
    "\n",
    "    best_trial = [trial for trial in study_hgb.best_trials if trial.number == best_trial_index]\n",
    "    best_params = best_trial[0].params\n",
    "    return best_params, best_trial[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params, best_values = obtain_best_params(study_hgb, weights=np.array([0.75, 0.25]))\n",
    "print(f'Mejores hiperparámetros: {best_params}')\n",
    "print(f'Mejores valores: Mean rmse Validation: {best_values[0]}, Mean Diff (Val - Train) rmse: {best_values[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Entrenar el modelo con los mejores hiperparámetros encontrados\n",
    "do_oversampling = best_params.pop('do_oversampling')\n",
    "base_models = [\n",
    "    ('hgb_1', HistGradientBoostingRegressor(random_state=42, categorical_features=categorical_features, **best_params)),\n",
    "    ('hgb_2', HistGradientBoostingRegressor(random_state=43, categorical_features=categorical_features, **best_params)),\n",
    "    ('hgb_3', HistGradientBoostingRegressor(random_state=44, categorical_features=categorical_features, **best_params))\n",
    "]\n",
    "\n",
    "meta_model = LinearRegression(n_jobs=-1)\n",
    "model = StackingRegressor(estimators=base_models, final_estimator=meta_model, n_jobs=-1)\n",
    "\n",
    "X_train_over, y_train_over = custom_oversample_data(X_train, y_train)\n",
    "model.fit(X_train_over, y_train_over)\n",
    "\n",
    "\n",
    "# Predecir sobre el conjunto de prueba\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluar todas las métricas de regresión\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'RMSE: {rmse}')\n",
    "print(f'MAE: {mae}')\n",
    "print(f'R^2 Score: {r2}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "david_pcc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
