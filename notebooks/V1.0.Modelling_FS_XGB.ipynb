{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from lazypredict.Supervised import LazyRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Cargar los datos\n",
    "data = pd.read_csv('../data/FS_final_train.csv', index_col=0)\n",
    "\n",
    "# Separar las características y la variable objetivo\n",
    "X = data.drop(columns=['log_SalePrice'])\n",
    "y = data['log_SalePrice']\n",
    "\n",
    "# Estandarizar las características\n",
    "scaler = StandardScaler()\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns, index=X.index)\n",
    "X_scaled.dropna(inplace=True)\n",
    "\n",
    "#Eliminar la fila eliminada en y tambien en X\n",
    "dropped_rows = y.index.difference(X_scaled.index)\n",
    "y_scaled = y.drop(index=dropped_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapa de calor de correlación\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(X_scaled.corr(method='pearson'), annot=True, cmap='coolwarm', linewidths=0.5, mask=np.triu(np.ones_like(X_scaled.corr(method='pearson'), dtype=bool)), fmt='.2f', annot_kws={'size': 8}, cbar=False)\n",
    "plt.title('Mapa de calor de correlación')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # De las correlaciones altas nos quedamos con las que tienen mas correlacion con la target\n",
    "# corr_matrix = X_scaled.corr()\n",
    "\n",
    "# #Eliminar las diagonales\n",
    "# corr_matrix = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "# corr_matrix = corr_matrix.stack().reset_index()\n",
    "# corr_matrix.columns = ['Feature 1', 'Feature 2', 'Correlation']\n",
    "\n",
    "# # De las parejas con abs(correlacion > 0.9) nos quedamos con la que tiene mas correlacion con la target\n",
    "# corr_matrix = corr_matrix[corr_matrix['Correlation'].abs() > 0.9]\n",
    "# corr_matrix = corr_matrix.sort_values(by='Correlation', ascending=False)\n",
    "\n",
    "# # de la pareja eliminar la que menos corr tenga con la target y, usar corrwith\n",
    "# features_to_drop = []\n",
    "# for index, row in corr_matrix.iterrows():\n",
    "#     feature1 = row['Feature 1']\n",
    "#     feature2 = row['Feature 2']\n",
    "#     if y.corr(X[feature1]) < y.corr(X[feature2]):\n",
    "#         features_to_drop.append(feature1)\n",
    "#     else:\n",
    "#         features_to_drop.append(feature2)\n",
    "\n",
    "# X_scaled = X_scaled.drop(columns=features_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Empleando X_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import xgboost as xgb\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Configuración comn de Optuna\n",
    "def create_study(name, version=None):\n",
    "    return optuna.create_study(study_name=f'{name}_housing_{version}', directions = ['minimize', 'minimize'],\n",
    "                               sampler=optuna.samplers.TPESampler(), pruner=optuna.pruners.MedianPruner(n_startup_trials=10, n_min_trials=2),\n",
    "                               load_if_exists=True, storage=f'sqlite:///../models/{name}_housing_{version}.db')\n",
    "\n",
    "def objective_xgb(trial):\n",
    "    # Definición de parámetros para XGBoost\n",
    "    params_xgb = {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'device': 'cuda',\n",
    "        'booster': trial.suggest_categorical('booster', ['gbtree']),\n",
    "        'tree_method': 'hist',\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 40),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 1.0),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 5, 10),\n",
    "        'gamma': trial.suggest_float('gamma', 0.0, 1.0),\n",
    "        'subsample': trial.suggest_float('subsample', 0.3, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.3, 1.0),\n",
    "        'reg_alpha': trial.suggest_loguniform('reg_alpha', 0.01, 1.0),\n",
    "        'reg_lambda': trial.suggest_loguniform('reg_lambda', 0.01, 1.0),\n",
    "        'n_jobs': -1,\n",
    "        'max_delta_step': trial.suggest_int('max_delta_step', 0, 10),\n",
    "        'max_bin': trial.suggest_int('max_bin', 256, 1024),\n",
    "        'num_parallel_tree': trial.suggest_int('num_parallel_tree', 1, 10),\n",
    "        'grow_policy': trial.suggest_categorical('grow_policy', ['depthwise', 'lossguide']),\n",
    "        'eval_metric': 'rmse'\n",
    "    }\n",
    "\n",
    "\n",
    "    # Preparar el conjunto de datos en un formato DMatrix\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train, feature_names=X_train.columns.tolist())\n",
    "\n",
    "    # Validación cruzada con XGBoost\n",
    "    result_xgb = xgb.cv(\n",
    "        params_xgb,\n",
    "        dtrain,\n",
    "        num_boost_round=1000,\n",
    "        nfold=5,\n",
    "        stratified=False,\n",
    "        early_stopping_rounds=100,\n",
    "        seed=42,\n",
    "        verbose_eval=True,\n",
    "        as_pandas=True\n",
    "    )\n",
    "    \n",
    "    # Extraemos la mejor puntuación AUC-PR\n",
    "    mean_rmse = result_xgb['test-rmse-mean'].min()\n",
    "    diff_rmse = np.abs(result_xgb['test-rmse-mean'].min() - result_xgb['train-rmse-mean'].min())\n",
    "    return mean_rmse, diff_rmse\n",
    "\n",
    "study_xgb = create_study('xgb_FS', version='1.1')\n",
    "# study_xgb.optimize(objective_xgb, n_trials=100, show_progress_bar=True, gc_after_trial=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Direcciones de optimización: {study_xgb.directions}')\n",
    "print(f'Nmero de ensayos: {study_xgb.trials.__len__()}')\n",
    "# print(f'Mejor ensayo: {study_xgb.best_trial}') solo se puede utilizar si la optimización es simple no multiple\n",
    "# print(f'Mejor valor: {study_xgb.best_value}') solo se puede utilizar si la optimización es simple no multiple\n",
    "# print(f'Mejor hiperparámetros: {study_xgb.best_params}') solo se puede utilizar si la optimización es simple no multiple\n",
    "study_xgb_df = study_xgb.trials_dataframe()\n",
    "study_xgb_df.sort_values(by='values_0', ascending=True).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_best_params(study_xgb, weights:np.ndarray) -> dict:\n",
    "    '''\n",
    "    Obtenemos los best params atraves de asociar un peso deseado a cada métrica que estamos optimizando\n",
    "    -------\n",
    "    Parámetros:\n",
    "    - study_xgb: optuna.Study, estudio de Optuna.\n",
    "    - weights: np.ndarray, pesos deseados para cada métrica.\n",
    "    -------\n",
    "    Devuelve:\n",
    "    - best_params: dict, hiperparámetros óptimos.\n",
    "    '''\n",
    "    study_df = study_xgb.trials_dataframe()\n",
    "    study_df['weighted_average'] = np.average(study_df[[col for col in study_df.columns if 'values' in col]], weights=weights, axis=1)\n",
    "\n",
    "    # Encontrar el trial con el menor promedio ponderado\n",
    "    best_trial_index = study_df['weighted_average'].idxmin()\n",
    "    best_trial = study_df.loc[best_trial_index]\n",
    "\n",
    "    best_trial = [trial for trial in study_xgb.best_trials if trial.number == best_trial_index]\n",
    "    best_params = best_trial[0].params\n",
    "    return best_params, best_trial[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params, best_values = obtain_best_params(study_xgb, weights=np.array([0.75, 0.25]))\n",
    "print(f'Mejores hiperparámetros: {best_params}')\n",
    "print(f'Mejores valores: Mean rmse Validation: {best_values[0]}, Mean Diff (Val - Train) rmse: {best_values[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from xgboost import callback\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Función para crear DMatrix\n",
    "def create_dmatrix(X, y):\n",
    "    return xgb.DMatrix(X, label=y, feature_names=X.columns.tolist())\n",
    "\n",
    "# Dividir los datos de train en train y val\n",
    "X_train_val, X_val, y_train_val, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Crear DMatrix para los conjuntos de datos\n",
    "dtrain = create_dmatrix(X_train, y_train)\n",
    "dtrain_val = create_dmatrix(X_train_val, y_train_val)\n",
    "dval = create_dmatrix(X_val, y_val)\n",
    "dtest = create_dmatrix(X_test, y_test)\n",
    "\n",
    "# Parámetros del modelo\n",
    "params = {\n",
    "    **best_params,\n",
    "    'objective': 'reg:squarederror',\n",
    "    'eval_metric': ['rmse'],\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1,\n",
    "}\n",
    "\n",
    "# Configuración del entrenamiento\n",
    "num_boost_round = 1000\n",
    "early_stopping_rounds = 300\n",
    "evals = [(dtrain_val, 'train'), (dval, 'val'), (dtest, 'test')] #Es importante que el early stopping no sea sobre dtest puesto que estos datos no se conocen.\n",
    "\n",
    "# Definimos el callback para el early stopping sobre el conjunto de validación nunca el de test.\n",
    "EarlyStopping_callback = callback.EarlyStopping(metric_name='rmse', data_name='val', maximize=False, save_best=True, rounds=early_stopping_rounds, min_delta=0.001)\n",
    "\n",
    "# Entrenar el modelo\n",
    "evals_result = {}\n",
    "model = xgb.train(\n",
    "    params,\n",
    "    dtrain_val,\n",
    "    num_boost_round=num_boost_round,\n",
    "    evals=evals,\n",
    "    evals_result=evals_result,\n",
    "    early_stopping_rounds=early_stopping_rounds,\n",
    "    callbacks=[EarlyStopping_callback],\n",
    "    verbose_eval=100  # Mostrar métricas cada 100 rondas\n",
    ")\n",
    "\n",
    "n_rounds = len(evals_result['train']['rmse'])\n",
    "\n",
    "# Encontrar la mejor ronda\n",
    "best_round = model.best_iteration\n",
    "\n",
    "print(f\"\\nMejor ronda: {best_round}\")\n",
    "print(f\"Mejor rmse en train: {evals_result['train']['rmse'][best_round-1]:.6f}\")\n",
    "print(f\"Mejor rmse en validación: {evals_result['val']['rmse'][best_round-1]:.6f}\")\n",
    "print(f\"Mejor rmse en test: {evals_result['test']['rmse'][best_round-1]:.6f}\")\n",
    "print(f\"Diferencia entre train y test: {evals_result['test']['rmse'][best_round-1] - evals_result['train']['rmse'][best_round-1]:.6f}\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configurar el estilo de Seaborn\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"paper\", font_scale=1.2)\n",
    "\n",
    "# Crear la figura y los ejes\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Graficar las curvas de aprendizaje\n",
    "sns.lineplot(x=range(1, n_rounds+1), y=evals_result['train']['rmse'], label='Train', ax=ax)\n",
    "sns.lineplot(x=range(1, n_rounds+1), y=evals_result['val']['rmse'], label='Validación', ax=ax)\n",
    "sns.lineplot(x=range(1, n_rounds+1), y=evals_result['test']['rmse'], label='Test', ax=ax)\n",
    "\n",
    "# Añadir la línea vertical para la mejor ronda\n",
    "ax.axvline(x=best_round, color='r', linestyle='--', label='Best Iteration')\n",
    "\n",
    "# Añadir el texto para el gap\n",
    "ax.text(best_round, (evals_result['test']['rmse'][best_round-1] + evals_result['train']['rmse'][best_round-1]) / 2, \n",
    "        f\"Gap: {evals_result['test']['rmse'][best_round-1] - evals_result['train']['rmse'][best_round-1]:.6f}\", \n",
    "        color='black', ha='center', va='center', backgroundcolor='white')\n",
    "\n",
    "# Ajustar ylim entre 0.5 y 0.7\n",
    "ax.set_ylim(0, 0.5)\n",
    "\n",
    "# Configurar etiquetas y título\n",
    "ax.set_xlabel('Iterations')\n",
    "ax.set_ylabel('RMSE')\n",
    "ax.set_title('XGBoost Learning Curve')\n",
    "\n",
    "# Ajustar la leyenda\n",
    "ax.legend(loc='best')\n",
    "\n",
    "# Mostrar la gráfica\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Empleando X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import xgboost as xgb\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Configuración comn de Optuna\n",
    "def create_study(name, version=None):\n",
    "    return optuna.create_study(study_name=f'{name}_housing_{version}', directions = ['minimize', 'minimize'],\n",
    "                               sampler=optuna.samplers.TPESampler(), pruner=optuna.pruners.MedianPruner(n_startup_trials=10, n_min_trials=2),\n",
    "                               load_if_exists=True, storage=f'sqlite:///../models/{name}_housing_{version}.db')\n",
    "\n",
    "def objective_xgb(trial):\n",
    "    # Definición de parámetros para XGBoost\n",
    "    params_xgb = {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'device': 'gpu',\n",
    "        'booster': trial.suggest_categorical('booster', ['gbtree']),\n",
    "        'tree_method': 'hist',\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 40),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 1.0),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 5, 10),\n",
    "        'gamma': trial.suggest_float('gamma', 0.0, 1.0),\n",
    "        'subsample': trial.suggest_float('subsample', 0.3, 0.9),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.3, 0.9),\n",
    "        'reg_alpha': trial.suggest_loguniform('reg_alpha', 0.5, 1.0),\n",
    "        'reg_lambda': trial.suggest_loguniform('reg_lambda', 0.5, 1.0),\n",
    "        'n_jobs': -1,\n",
    "        'max_delta_step': trial.suggest_int('max_delta_step', 0, 10),\n",
    "        'max_bin': trial.suggest_int('max_bin', 256, 1024),\n",
    "        'num_parallel_tree': trial.suggest_int('num_parallel_tree', 1, 10),\n",
    "        'grow_policy': trial.suggest_categorical('grow_policy', ['depthwise', 'lossguide']),\n",
    "        'eval_metric': 'rmse'\n",
    "    }\n",
    "\n",
    "\n",
    "    # Preparar el conjunto de datos en un formato DMatrix\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train, feature_names=X_train.columns.tolist())\n",
    "\n",
    "    # Validación cruzada con XGBoost\n",
    "    result_xgb = xgb.cv(\n",
    "        params_xgb,\n",
    "        dtrain,\n",
    "        num_boost_round=1000,\n",
    "        nfold=5,\n",
    "        early_stopping_rounds=100,\n",
    "        seed=42,\n",
    "        verbose_eval=True,\n",
    "        as_pandas=True\n",
    "    )\n",
    "    \n",
    "    # Extraemos la mejor puntuación AUC-PR\n",
    "    mean_rmse = result_xgb['test-rmse-mean'].min()\n",
    "    diff_rmse = np.abs(result_xgb['test-rmse-mean'].min() - result_xgb['train-rmse-mean'].min())\n",
    "    return mean_rmse, diff_rmse\n",
    "\n",
    "study_xgb = create_study('xgb_NO_FS', version='1.0')\n",
    "study_xgb.optimize(objective_xgb, n_trials=100, show_progress_bar=True, gc_after_trial=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Direcciones de optimización: {study_xgb.directions}')\n",
    "print(f'Nmero de ensayos: {study_xgb.trials.__len__()}')\n",
    "# print(f'Mejor ensayo: {study_xgb.best_trial}') solo se puede utilizar si la optimización es simple no multiple\n",
    "# print(f'Mejor valor: {study_xgb.best_value}') solo se puede utilizar si la optimización es simple no multiple\n",
    "# print(f'Mejor hiperparámetros: {study_xgb.best_params}') solo se puede utilizar si la optimización es simple no multiple\n",
    "study_xgb_df = study_xgb.trials_dataframe()\n",
    "study_xgb_df.sort_values(by='values_0', ascending=True).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def obtain_best_params(study_xgb, weights:np.ndarray) -> dict:\n",
    "    '''\n",
    "    Obtenemos los best params atraves de asociar un peso deseado a cada métrica que estamos optimizando\n",
    "    -------\n",
    "    Parámetros:\n",
    "    - study_xgb: optuna.Study, estudio de Optuna.\n",
    "    - weights: np.ndarray, pesos deseados para cada métrica.\n",
    "    -------\n",
    "    Devuelve:\n",
    "    - best_params: dict, hiperparámetros óptimos.\n",
    "    '''\n",
    "    study_df = study_xgb.trials_dataframe()\n",
    "    study_df['weighted_average'] = np.average(study_df[[col for col in study_df.columns if 'values' in col]], weights=weights, axis=1)\n",
    "\n",
    "    # Encontrar el trial con el menor promedio ponderado\n",
    "    best_trial_index = study_df['weighted_average'].idxmin()\n",
    "    best_trial = study_df.loc[best_trial_index]\n",
    "\n",
    "    best_trial = [trial for trial in study_xgb.best_trials if trial.number == best_trial_index]\n",
    "    best_params = best_trial[0].params\n",
    "    return best_params, best_trial[0].values\n",
    "\n",
    "best_params, best_values = obtain_best_params(study_xgb, weights=np.array([0.75, 0.25]))\n",
    "print(f'Mejores hiperparámetros: {best_params}')\n",
    "print(f'Mejores valores: Mean rmse Validation: {best_values[0]}, Mean Diff (Val - Train) rmse: {best_values[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from xgboost import callback\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Función para crear DMatrix\n",
    "def create_dmatrix(X, y):\n",
    "    return xgb.DMatrix(X, label=y, feature_names=X.columns.tolist())\n",
    "\n",
    "# Dividir los datos de train en train y val\n",
    "X_train_val, X_val, y_train_val, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Crear DMatrix para los conjuntos de datos\n",
    "dtrain = create_dmatrix(X_train, y_train)\n",
    "dtrain_val = create_dmatrix(X_train_val, y_train_val)\n",
    "dval = create_dmatrix(X_val, y_val)\n",
    "dtest = create_dmatrix(X_test, y_test)\n",
    "\n",
    "# Parámetros del modelo\n",
    "params = {\n",
    "    **best_params,\n",
    "    'objective': 'reg:squarederror',\n",
    "    'eval_metric': ['rmse'],\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1,\n",
    "}\n",
    "\n",
    "# Configuración del entrenamiento\n",
    "num_boost_round = 1000\n",
    "early_stopping_rounds = 300\n",
    "evals = [(dtrain_val, 'train'), (dval, 'val'), (dtest, 'test')] #Es importante que el early stopping no sea sobre dtest puesto que estos datos no se conocen.\n",
    "\n",
    "# Definimos el callback para el early stopping sobre el conjunto de validación nunca el de test.\n",
    "EarlyStopping_callback = callback.EarlyStopping(metric_name='rmse', data_name='val', maximize=False, save_best=True, rounds=early_stopping_rounds, min_delta=0.001)\n",
    "\n",
    "# Entrenar el modelo\n",
    "evals_result = {}\n",
    "model = xgb.train(\n",
    "    params,\n",
    "    dtrain_val,\n",
    "    num_boost_round=num_boost_round,\n",
    "    evals=evals,\n",
    "    evals_result=evals_result,\n",
    "    early_stopping_rounds=early_stopping_rounds,\n",
    "    callbacks=[EarlyStopping_callback],\n",
    "    verbose_eval=100  # Mostrar métricas cada 100 rondas\n",
    ")\n",
    "\n",
    "n_rounds = len(evals_result['train']['rmse'])\n",
    "\n",
    "# Encontrar la mejor ronda\n",
    "best_round = model.best_iteration\n",
    "\n",
    "print(f\"\\nMejor ronda: {best_round}\")\n",
    "print(f\"Mejor rmse en train: {evals_result['train']['rmse'][best_round-1]:.6f}\")\n",
    "print(f\"Mejor rmse en validación: {evals_result['val']['rmse'][best_round-1]:.6f}\")\n",
    "print(f\"Mejor rmse en test: {evals_result['test']['rmse'][best_round-1]:.6f}\")\n",
    "print(f\"Diferencia entre train y test: {evals_result['test']['rmse'][best_round-1] - evals_result['train']['rmse'][best_round-1]:.6f}\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configurar el estilo de Seaborn\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"paper\", font_scale=1.2)\n",
    "\n",
    "# Crear la figura y los ejes\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Graficar las curvas de aprendizaje\n",
    "sns.lineplot(x=range(1, n_rounds+1), y=evals_result['train']['rmse'], label='Train', ax=ax)\n",
    "sns.lineplot(x=range(1, n_rounds+1), y=evals_result['val']['rmse'], label='Validación', ax=ax)\n",
    "sns.lineplot(x=range(1, n_rounds+1), y=evals_result['test']['rmse'], label='Test', ax=ax)\n",
    "\n",
    "# Añadir la línea vertical para la mejor ronda\n",
    "ax.axvline(x=best_round, color='r', linestyle='--', label='Best Iteration')\n",
    "\n",
    "# Añadir el texto para el gap\n",
    "ax.text(best_round, (evals_result['test']['rmse'][best_round-1] + evals_result['train']['rmse'][best_round-1]) / 2, \n",
    "        f\"Gap: {evals_result['test']['rmse'][best_round-1] - evals_result['train']['rmse'][best_round-1]:.6f}\", \n",
    "        color='black', ha='center', va='center', backgroundcolor='white')\n",
    "\n",
    "# Ajustar ylim entre 0.5 y 0.7\n",
    "ax.set_ylim(0, 0.5)\n",
    "\n",
    "# Configurar etiquetas y título\n",
    "ax.set_xlabel('Iterations')\n",
    "ax.set_ylabel('RMSE')\n",
    "ax.set_title('XGBoost Learning Curve')\n",
    "\n",
    "# Ajustar la leyenda\n",
    "ax.legend(loc='best')\n",
    "\n",
    "# Mostrar la gráfica\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizando diferentes objective error functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import xgboost as xgb\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Configuración comn de Optuna\n",
    "def create_study(name, version=None):\n",
    "    return optuna.create_study(study_name=f'{name}_housing_{version}', directions = ['minimize', 'minimize'],\n",
    "                               sampler=optuna.samplers.TPESampler(), pruner=optuna.pruners.MedianPruner(n_startup_trials=10, n_min_trials=2),\n",
    "                               load_if_exists=True, storage=f'sqlite:///../models/{name}_housing_{version}.db')\n",
    "\n",
    "def objective_xgb(trial):\n",
    "    # Definición de parámetros para XGBoost\n",
    "    params_xgb = {\n",
    "        'objective': trial.suggest_categorical('objective', ['reg:squarederror', 'reg:linear']),\n",
    "        'device': 'cuda',\n",
    "        'booster': trial.suggest_categorical('booster', ['gbtree']),\n",
    "        'tree_method': 'auto',\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 40),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 1.0),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 5, 10),\n",
    "        'gamma': trial.suggest_float('gamma', 0.0, 1.0),\n",
    "        'subsample': trial.suggest_float('subsample', 0.3, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.3, 1.0),\n",
    "        'reg_alpha': trial.suggest_loguniform('reg_alpha', 0.01, 1.0),\n",
    "        'reg_lambda': trial.suggest_loguniform('reg_lambda', 0.01, 1.0),\n",
    "        'n_jobs': -1,\n",
    "        'max_delta_step': trial.suggest_int('max_delta_step', 0, 10),\n",
    "        'num_parallel_tree': trial.suggest_int('num_parallel_tree', 1, 10),\n",
    "        'grow_policy': trial.suggest_categorical('grow_policy', ['depthwise', 'lossguide']),\n",
    "        'eval_metric': 'rmse'\n",
    "    }\n",
    "\n",
    "\n",
    "    # Preparar el conjunto de datos en un formato DMatrix\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train, feature_names=X_train.columns.tolist())\n",
    "\n",
    "    # Validación cruzada con XGBoost\n",
    "    result_xgb = xgb.cv(\n",
    "        params_xgb,\n",
    "        dtrain,\n",
    "        num_boost_round=400,\n",
    "        nfold=5,\n",
    "        stratified=False,\n",
    "        early_stopping_rounds=50,\n",
    "        seed=42,\n",
    "        verbose_eval=True,\n",
    "        as_pandas=True\n",
    "    )\n",
    "    \n",
    "    # Extraemos la mejor puntuación AUC-PR\n",
    "    mean_rmse = result_xgb['test-rmse-mean'].min()\n",
    "    diff_rmse = np.abs(result_xgb['test-rmse-mean'].min() - result_xgb['train-rmse-mean'].min())\n",
    "    return mean_rmse, diff_rmse\n",
    "\n",
    "study_xgb = create_study('xgb_FS', version='2.5')\n",
    "study_xgb.optimize(objective_xgb, n_trials=100, show_progress_bar=True, gc_after_trial=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Direcciones de optimización: {study_xgb.directions}')\n",
    "print(f'Nmero de ensayos: {study_xgb.trials.__len__()}')\n",
    "# print(f'Mejor ensayo: {study_xgb.best_trial}') solo se puede utilizar si la optimización es simple no multiple\n",
    "# print(f'Mejor valor: {study_xgb.best_value}') solo se puede utilizar si la optimización es simple no multiple\n",
    "# print(f'Mejor hiperparámetros: {study_xgb.best_params}') solo se puede utilizar si la optimización es simple no multiple\n",
    "study_xgb_df = study_xgb.trials_dataframe()\n",
    "study_xgb_df.sort_values(by='values_0', ascending=True).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_best_params(study_xgb, weights:np.ndarray) -> dict:\n",
    "    '''\n",
    "    Obtenemos los best params atraves de asociar un peso deseado a cada métrica que estamos optimizando\n",
    "    -------\n",
    "    Parámetros:\n",
    "    - study_xgb: optuna.Study, estudio de Optuna.\n",
    "    - weights: np.ndarray, pesos deseados para cada métrica.\n",
    "    -------\n",
    "    Devuelve:\n",
    "    - best_params: dict, hiperparámetros óptimos.\n",
    "    '''\n",
    "    study_df = study_xgb.trials_dataframe()\n",
    "    study_df['weighted_average'] = np.average(study_df[[col for col in study_df.columns if 'values' in col]], weights=weights, axis=1)\n",
    "\n",
    "    # Encontrar el trial con el menor promedio ponderado\n",
    "    best_trial_index = study_df['weighted_average'].idxmin()\n",
    "    best_trial = study_df.loc[best_trial_index]\n",
    "\n",
    "    best_trial = [trial for trial in study_xgb.best_trials if trial.number == best_trial_index]\n",
    "    best_params = best_trial[0].params\n",
    "    return best_params, best_trial[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params, best_values = obtain_best_params(study_xgb, weights=np.array([0.75, 0.25]))\n",
    "print(f'Mejores hiperparámetros: {best_params}')\n",
    "print(f'Mejores valores: Mean rmse Validation: {best_values[0]}, Mean Diff (Val - Train) rmse: {best_values[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from xgboost import callback\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Función para crear DMatrix\n",
    "def create_dmatrix(X, y):\n",
    "    return xgb.DMatrix(X, label=y, feature_names=X.columns.tolist())\n",
    "\n",
    "# Dividir los datos de train en train y val\n",
    "X_train_val, X_val, y_train_val, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Crear DMatrix para los conjuntos de datos\n",
    "dtrain = create_dmatrix(X_train, y_train)\n",
    "dtrain_val = create_dmatrix(X_train_val, y_train_val)\n",
    "dval = create_dmatrix(X_val, y_val)\n",
    "dtest = create_dmatrix(X_test, y_test)\n",
    "\n",
    "# Parámetros del modelo\n",
    "params = {\n",
    "    **best_params,\n",
    "    'objective': 'reg:squarederror',\n",
    "    'eval_metric': ['rmse'],\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1,\n",
    "}\n",
    "\n",
    "# Configuración del entrenamiento\n",
    "num_boost_round = 1000\n",
    "early_stopping_rounds = 300\n",
    "evals = [(dtrain_val, 'train'), (dval, 'val'), (dtest, 'test')] #Es importante que el early stopping no sea sobre dtest puesto que estos datos no se conocen.\n",
    "\n",
    "# Definimos el callback para el early stopping sobre el conjunto de validación nunca el de test.\n",
    "EarlyStopping_callback = callback.EarlyStopping(metric_name='rmse', data_name='val', maximize=False, save_best=True, rounds=early_stopping_rounds, min_delta=0.001)\n",
    "\n",
    "# Entrenar el modelo\n",
    "evals_result = {}\n",
    "model = xgb.train(\n",
    "    params,\n",
    "    dtrain_val,\n",
    "    num_boost_round=num_boost_round,\n",
    "    evals=evals,\n",
    "    evals_result=evals_result,\n",
    "    early_stopping_rounds=early_stopping_rounds,\n",
    "    callbacks=[EarlyStopping_callback],\n",
    "    verbose_eval=100  # Mostrar métricas cada 100 rondas\n",
    ")\n",
    "\n",
    "n_rounds = len(evals_result['train']['rmse'])\n",
    "\n",
    "# Encontrar la mejor ronda\n",
    "best_round = model.best_iteration\n",
    "\n",
    "print(f\"\\nMejor ronda: {best_round}\")\n",
    "print(f\"Mejor rmse en train: {evals_result['train']['rmse'][best_round-1]:.6f}\")\n",
    "print(f\"Mejor rmse en validación: {evals_result['val']['rmse'][best_round-1]:.6f}\")\n",
    "print(f\"Mejor rmse en test: {evals_result['test']['rmse'][best_round-1]:.6f}\")\n",
    "print(f\"Diferencia entre train y test: {evals_result['test']['rmse'][best_round-1] - evals_result['train']['rmse'][best_round-1]:.6f}\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configurar el estilo de Seaborn\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"paper\", font_scale=1.2)\n",
    "\n",
    "# Crear la figura y los ejes\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Graficar las curvas de aprendizaje\n",
    "sns.lineplot(x=range(1, n_rounds+1), y=evals_result['train']['rmse'], label='Train', ax=ax)\n",
    "sns.lineplot(x=range(1, n_rounds+1), y=evals_result['val']['rmse'], label='Validación', ax=ax)\n",
    "sns.lineplot(x=range(1, n_rounds+1), y=evals_result['test']['rmse'], label='Test', ax=ax)\n",
    "\n",
    "# Añadir la línea vertical para la mejor ronda\n",
    "ax.axvline(x=best_round, color='r', linestyle='--', label='Best Iteration')\n",
    "\n",
    "# Añadir el texto para el gap\n",
    "ax.text(best_round, (evals_result['test']['rmse'][best_round-1] + evals_result['train']['rmse'][best_round-1]) / 2, \n",
    "        f\"Gap: {evals_result['test']['rmse'][best_round-1] - evals_result['train']['rmse'][best_round-1]:.6f}\", \n",
    "        color='black', ha='center', va='center', backgroundcolor='white')\n",
    "\n",
    "# Ajustar ylim entre 0.5 y 0.7\n",
    "ax.set_ylim(0, 0.5)\n",
    "\n",
    "# Configurar etiquetas y título\n",
    "ax.set_xlabel('Iterations')\n",
    "ax.set_ylabel('RMSE')\n",
    "ax.set_title('XGBoost Learning Curve')\n",
    "\n",
    "# Ajustar la leyenda\n",
    "ax.legend(loc='best')\n",
    "\n",
    "# Mostrar la gráfica\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "david_pcc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
